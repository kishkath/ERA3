{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## libraries\n\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:46:51.413455Z","iopub.execute_input":"2024-12-12T15:46:51.414259Z","iopub.status.idle":"2024-12-12T15:46:53.051262Z","shell.execute_reply.started":"2024-12-12T15:46:51.414225Z","shell.execute_reply":"2024-12-12T15:46:53.050558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## data transforms\ntrain_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntest_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:47:17.391932Z","iopub.execute_input":"2024-12-12T15:47:17.392952Z","iopub.status.idle":"2024-12-12T15:47:17.398195Z","shell.execute_reply.started":"2024-12-12T15:47:17.392909Z","shell.execute_reply":"2024-12-12T15:47:17.397036Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## datasets\n\ntrain = datasets.MNIST(\"./data\", train=True, download=True, transform=train_transforms)\ntest = datasets.MNIST(\"./data\", train=False, download=True, transform=test_transforms)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:48:38.072358Z","iopub.execute_input":"2024-12-12T15:48:38.072714Z","iopub.status.idle":"2024-12-12T15:48:40.861016Z","shell.execute_reply.started":"2024-12-12T15:48:38.072668Z","shell.execute_reply":"2024-12-12T15:48:40.860130Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## dataloader args\n\nSEED = 1\n\ncuda = torch.cuda.is_available()\nprint(\"CUDA Available: \", cuda)\n\nif cuda:\n    torch.manual_seed(SEED)\n\ndataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True)\n\ntrain_loader = torch.utils.data.DataLoader(train, **dataloader_args)\ntest_loader = torch.utils.data.DataLoader(test, **dataloader_args)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:50:33.976540Z","iopub.execute_input":"2024-12-12T15:50:33.977415Z","iopub.status.idle":"2024-12-12T15:50:34.073461Z","shell.execute_reply.started":"2024-12-12T15:50:33.977382Z","shell.execute_reply":"2024-12-12T15:50:34.072458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## load data\nimport matplotlib.pyplot as plt\n\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n\nfig = plt.figure()\nnum_images = 70\n\nfor idx in range(1, num_images+1):\n    plt.subplot(7, 10, idx)\n    plt.axis('off')\n    plt.imshow(images[idx].numpy().squeeze(), cmap='gray_r')\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T15:52:13.401087Z","iopub.execute_input":"2024-12-12T15:52:13.401469Z","iopub.status.idle":"2024-12-12T15:52:16.083972Z","shell.execute_reply.started":"2024-12-12T15:52:13.401434Z","shell.execute_reply":"2024-12-12T15:52:16.082935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Neural Network model\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3)),\n            nn.ReLU()) # (28-3) + 1 = 26, # Jout = 1, # Rfout = 1 + (3-1)*1 = 3\n\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3,3)),\n            nn.ReLU() \n                    \n        ) # (26-3+1) = 24, # Jout = 1 # Rfout = 3 + (3-1)*1 = 5 \n\n\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(in_channels=20, out_channels=20, kernel_size=(3,3)),\n            nn.ReLU()\n        ) # (24-3+1) =22, # Jout = 1 # Rfout = 5 + (3-1)*1 = 7\n\n        self.maxpool1 = nn.MaxPool2d(2, 2)\n        # (22-2)/2 + 1 = 11, # jout = 2*1 = 2, Rfout = 7 + (2-1)*1 = 9\n\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(in_channels=20, out_channels=32, kernel_size=(3,3)),\n            nn.ReLU() \n        ) # (11-3+1) = 9, # Jout = 2, # Rfout = 9 + (3-1)*2 = 13\n\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3,3)),\n            nn.ReLU()\n        ) # (9-3+1) = 7, # jout = 2 # Rfout = 13 + (3-1)*2 = 17\n\n        self.conv6 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=20, kernel_size=(1,1)),\n            nn.ReLU()\n        ) # (7-1+1) = 7, # Jout = 2, # Rfout = 17 + (1-1)*2 = 17\n\n        self.conv7 = nn.Sequential(\n            nn.Conv2d(in_channels=20, out_channels=20, kernel_size=(3, 3)),\n            nn.ReLU()\n        ) # (7-3+1) = 5, # Jout = 2, # Rfout = 17 + (3-1)*2 = 21\n        \n        self.conv8 = nn.Sequential(\n            nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(5,5))\n        ) # (5-5+1) = 1, # Jout = 2, # Rfout = 21 + (5-1)*2 = 29\n\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.maxpool1(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        x = self.conv6(x)\n        x = self.conv7(x)\n        x = self.conv8(x)\n        x = x.view(-1, 10)\n        return F.log_softmax(x, dim=-1)\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nmodel = Net().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:08:33.226201Z","iopub.execute_input":"2024-12-12T16:08:33.227167Z","iopub.status.idle":"2024-12-12T16:08:33.240831Z","shell.execute_reply.started":"2024-12-12T16:08:33.227133Z","shell.execute_reply":"2024-12-12T16:08:33.239884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install torchsummary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:08:33.847957Z","iopub.execute_input":"2024-12-12T16:08:33.848349Z","iopub.status.idle":"2024-12-12T16:08:33.853480Z","shell.execute_reply.started":"2024-12-12T16:08:33.848312Z","shell.execute_reply":"2024-12-12T16:08:33.852111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchsummary import summary\n\nsummary(model, input_size=(1, 28, 28))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:08:34.445730Z","iopub.execute_input":"2024-12-12T16:08:34.446415Z","iopub.status.idle":"2024-12-12T16:08:34.456214Z","shell.execute_reply.started":"2024-12-12T16:08:34.446382Z","shell.execute_reply":"2024-12-12T16:08:34.455223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Train & Test Loops\n\nfrom tqdm import tqdm\n\ntrain_losses = []\ntest_losses = []\ntrain_acc = []\ntest_acc = []\n\ndef train(model, device, train_loader, optimizer, epoch):\n  model.train()\n  pbar = tqdm(train_loader)\n  correct = 0\n  processed = 0\n  for batch_idx, (data, target) in enumerate(pbar):\n    # get samples\n    data, target = data.to(device), target.to(device)\n\n    # Init\n    optimizer.zero_grad()\n    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. \n    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n\n    # Predict\n    y_pred = model(data)\n\n    # Calculate loss\n    loss = F.nll_loss(y_pred, target)\n    train_losses.append(loss)\n\n    # Backpropagation\n    loss.backward()\n    optimizer.step()\n\n    # Update pbar-tqdm\n    \n    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n    correct += pred.eq(target.view_as(pred)).sum().item()\n    processed += len(data)\n\n    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n    train_acc.append(100*correct/processed)\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    test_losses.append(test_loss)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n    \n    test_acc.append(100. * correct / len(test_loader.dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:08:38.480993Z","iopub.execute_input":"2024-12-12T16:08:38.481781Z","iopub.status.idle":"2024-12-12T16:08:38.491884Z","shell.execute_reply.started":"2024-12-12T16:08:38.481749Z","shell.execute_reply":"2024-12-12T16:08:38.491006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model =  Net().to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nEPOCHS = 20\nfor epoch in range(EPOCHS):\n    print(\"EPOCH:\", epoch)\n    train(model, device, train_loader, optimizer, epoch)\n    test(model, device, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:08:41.226369Z","iopub.execute_input":"2024-12-12T16:08:41.226732Z","iopub.status.idle":"2024-12-12T16:11:36.000497Z","shell.execute_reply.started":"2024-12-12T16:08:41.226695Z","shell.execute_reply":"2024-12-12T16:11:35.999195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"t = [t_items.item() for t_items in train_losses]\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfig, axs = plt.subplots(2,2,figsize=(15,10))\naxs[0, 0].plot(t)\naxs[0, 0].set_title(\"Training Loss\")\naxs[1, 0].plot(train_acc)\naxs[1, 0].set_title(\"Training Accuracy\")\naxs[0, 1].plot(test_losses)\naxs[0, 1].set_title(\"Test Loss\")\naxs[1, 1].plot(test_acc)\naxs[1, 1].set_title(\"Test Accuracy\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:07:47.040932Z","iopub.execute_input":"2024-12-12T16:07:47.041719Z","iopub.status.idle":"2024-12-12T16:07:47.805175Z","shell.execute_reply.started":"2024-12-12T16:07:47.041666Z","shell.execute_reply":"2024-12-12T16:07:47.804214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Batch Normalization with Regularization","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:21:59.836891Z","iopub.execute_input":"2024-12-12T16:21:59.837582Z","iopub.status.idle":"2024-12-12T16:21:59.842137Z","shell.execute_reply.started":"2024-12-12T16:21:59.837543Z","shell.execute_reply":"2024-12-12T16:21:59.841320Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Neural Network model with batchNormalization\n\nclass Net2(nn.Module):\n    def __init__(self):\n        super(Net2, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=(3, 3)),\n            nn.BatchNorm2d(10),\n            nn.ReLU()) # (28-3) + 1 = 26, # Jout = 1, # Rfout = 1 + (3-1)*1 = 3\n\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3,3)),\n            nn.BatchNorm2d(20),\n            nn.ReLU() \n                    \n        ) # (26-3+1) = 24, # Jout = 1 # Rfout = 3 + (3-1)*1 = 5 \n\n\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(in_channels=20, out_channels=20, kernel_size=(3,3)),\n            nn.BatchNorm2d(20),\n            nn.ReLU()\n        ) # (24-3+1) =22, # Jout = 1 # Rfout = 5 + (3-1)*1 = 7\n\n        self.maxpool1 = nn.MaxPool2d(2, 2)\n        # (22-2)/2 + 1 = 11, # jout = 2*1 = 2, Rfout = 7 + (2-1)*1 = 9\n\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(in_channels=20, out_channels=32, kernel_size=(3,3)),\n            nn.BatchNorm2d(32),\n            nn.ReLU() \n        ) # (11-3+1) = 9, # Jout = 2, # Rfout = 9 + (3-1)*2 = 13\n\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3,3)),\n            nn.BatchNorm2d(32),\n            nn.ReLU()\n        ) # (9-3+1) = 7, # jout = 2 # Rfout = 13 + (3-1)*2 = 17\n\n        self.conv6 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=20, kernel_size=(1,1)),\n            nn.BatchNorm2d(20),\n            nn.ReLU()\n        ) # (7-1+1) = 7, # Jout = 2, # Rfout = 17 + (1-1)*2 = 17\n\n        self.conv7 = nn.Sequential(\n            nn.Conv2d(in_channels=20, out_channels=20, kernel_size=(3, 3)),\n            nn.BatchNorm2d(20),\n            nn.ReLU()\n        ) # (7-3+1) = 5, # Jout = 2, # Rfout = 17 + (3-1)*2 = 21\n        \n        self.conv8 = nn.Sequential(\n            nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(5,5))\n        ) # (5-5+1) = 1, # Jout = 2, # Rfout = 21 + (5-1)*2 = 29\n\n\n        self.dropout = nn.Dropout(0.05)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.dropout(x)\n        x = self.conv3(x)\n        x = self.dropout(x)\n        x = self.maxpool1(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        x = self.dropout(x)\n        x = self.conv6(x)\n        x = self.dropout(x)\n        x = self.conv7(x)\n        x = self.conv8(x)\n        x = x.view(-1, 10)\n        return F.log_softmax(x, dim=-1)\n\nmodel2 = Net2().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:24:31.999742Z","iopub.execute_input":"2024-12-12T16:24:32.000173Z","iopub.status.idle":"2024-12-12T16:24:32.037416Z","shell.execute_reply.started":"2024-12-12T16:24:32.000138Z","shell.execute_reply":"2024-12-12T16:24:32.036389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary(model2, input_size=(1, 28, 28))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:24:42.307455Z","iopub.execute_input":"2024-12-12T16:24:42.307838Z","iopub.status.idle":"2024-12-12T16:24:42.488077Z","shell.execute_reply.started":"2024-12-12T16:24:42.307807Z","shell.execute_reply":"2024-12-12T16:24:42.486866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer2 = optim.SGD(model2.parameters(), lr=0.01, momentum=0.9)\nEPOCHS = 20 \nfor epoch in range(EPOCHS):\n    print(f\"[EPOCH]: \", epoch)\n    train(model2, device, train_loader, optimizer2, epoch)\n    test(model2, device, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T16:25:55.911816Z","iopub.execute_input":"2024-12-12T16:25:55.912445Z","iopub.status.idle":"2024-12-12T16:28:54.152486Z","shell.execute_reply.started":"2024-12-12T16:25:55.912412Z","shell.execute_reply":"2024-12-12T16:28:54.151458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Seemingly using Batch Normalization with Droout-Regularization has helped w.r.t to overfiting.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Target**:\n----------\n\n1. Setup a lighter model related to problem of MNIST with image size of (28x28) as a first step, add Batch Normalization & Dropout Regularization as a next step.\n\n2. Basic necessary transforms of ToTensor(), Normalize\n\n3. Stored via Dataloader\n\n4. less Parameters Neural Network\n\n5. Basic Training & Testing Loops\n\n6. Train for 20 Epochs\n\n\n**Result**:\n---------\nA. Parameters: 30K (30,178)\n\nB. Best Training Accuracy: 99.84 (19th EPOCH)\n\nC. Best Test Accuracy: 99.48 (19th EPOCH)\n\n\n**Analysis**:\n------------\n\nA. Model with bit of lesser number of parameters.\n\nB. Model when made with lesser parameters with no techniques produced model with varying logs, along with overfitting. But, when we started to use BatchNormalization along with dropout regularization, model performance improved with less overfitting which produced a good inference model.","metadata":{}}]}