{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# AIM: \n\n# >>> Perform MaxPooling at RF=5\n# >>> GAP\n# >>> Add rotation, we guess that 5-7 degrees should be sufficient. \n# >>> Add LR Scheduler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T19:43:12.203104Z","iopub.execute_input":"2024-12-13T19:43:12.203349Z","iopub.status.idle":"2024-12-13T19:43:12.223222Z","shell.execute_reply.started":"2024-12-13T19:43:12.203323Z","shell.execute_reply":"2024-12-13T19:43:12.222657Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"### Libraries\n\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T19:43:12.263091Z","iopub.execute_input":"2024-12-13T19:43:12.263672Z","iopub.status.idle":"2024-12-13T19:43:16.656448Z","shell.execute_reply.started":"2024-12-13T19:43:12.263645Z","shell.execute_reply":"2024-12-13T19:43:16.655520Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"## transforms with Augmentations\n\ntrain_transforms = transforms.Compose([\n    transforms.RandomRotation((-5.0, 5.0), fill=(1,)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntest_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T19:43:16.657930Z","iopub.execute_input":"2024-12-13T19:43:16.658288Z","iopub.status.idle":"2024-12-13T19:43:16.663214Z","shell.execute_reply.started":"2024-12-13T19:43:16.658261Z","shell.execute_reply":"2024-12-13T19:43:16.662400Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"## dataset split\n\ntrain = datasets.MNIST(\"./data\", train=True, download=True, transform=train_transforms)\ntest = datasets.MNIST(\"./data\", train=False, download=True, transform=test_transforms)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T19:43:16.664204Z","iopub.execute_input":"2024-12-13T19:43:16.664452Z","iopub.status.idle":"2024-12-13T19:43:18.856202Z","shell.execute_reply.started":"2024-12-13T19:43:16.664422Z","shell.execute_reply":"2024-12-13T19:43:18.855446Z"}},"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9912422/9912422 [00:00<00:00, 53513090.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28881/28881 [00:00<00:00, 1483596.98it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Failed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1648877/1648877 [00:00<00:00, 13604479.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4542/4542 [00:00<00:00, 2905372.70it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"SEED = 1\n\n# CUDA?\ncuda = torch.cuda.is_available()\nprint(\"CUDA Available?\", cuda)\n\n# For reproducibility\ntorch.manual_seed(SEED)\n\nif cuda:\n    torch.cuda.manual_seed(SEED)\n\n# dataloader arguments - something you'll fetch these from cmdprmt\ndataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n\n# train dataloader\ntrain_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n\n# test dataloader\ntest_loader = torch.utils.data.DataLoader(test, **dataloader_args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T19:43:18.858015Z","iopub.execute_input":"2024-12-13T19:43:18.858299Z","iopub.status.idle":"2024-12-13T19:43:18.947242Z","shell.execute_reply.started":"2024-12-13T19:43:18.858269Z","shell.execute_reply":"2024-12-13T19:43:18.946126Z"}},"outputs":[{"name":"stdout","text":"CUDA Available? True\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"## Neural Network\n\ndropout_value = 0.075\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        # INPUT-BLOCK\n        self.inputblock = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3,3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # n_out = 28 - 3 + 1 = 26, # Jout = 1 # Rfout = 1 + (3-1)*1 = 3\n\n        # BLOCK -- [1]\n        self.hiddenblock1 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Dropout(dropout_value)\n        ) # n_out = 26 - 3 + 1 = 24, # Jout = 1, # Rfout = 3 + (3-1)*1 = 5\n\n        self.transistionblock = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=14, kernel_size=(1,1))\n        ) # n_out = 24, # Jout = 1, Rfout = 5\n        \n\n        ## TRANSISTION BLOCK\n        self.maxPool1 = nn.MaxPool2d((2, 2)) \n        # n_out = (24-2)/2 + 1 = 12, # Jout = 1*2 = 2, # Rfout = 7 + (2-1)*1 = 8\n        \n        # BLOCK -- [2]\n        self.hiddenblock2 = nn.Sequential(\n            nn.Conv2d(in_channels=14, out_channels=14, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(14),\n            nn.Dropout(dropout_value)\n        ) # n_out = 12 - 3 + 1 = 10, # Jout = 1, # Rfout = 5 + (3-1)*1 = 7\n\n        # BLOCK -- [3]\n        self.hiddenblock3 = nn.Sequential(\n            nn.Conv2d(in_channels=14, out_channels=12, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(12),\n            nn.Dropout(dropout_value)\n        ) # n_out = (10-3+1) = 8, # Jout = 2, # Rfout = 8 + (3-1)*2 = 12\n\n\n        # BLOCK --[5]\n        self.hiddenblock5 = nn.Sequential(\n            nn.Conv2d(in_channels=12, out_channels=10, kernel_size=(3, 3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(10),\n            nn.Dropout(dropout_value)\n        ) # n_out = 8 - 3 + 1 = 6, # Jout = 2, # Rfout = 12 + (3-1)*2 = 16\n\n        self.hiddenblock6 = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3,3), padding=0, bias=False),\n            nn.ReLU(),\n            nn.BatchNorm2d(10),\n            nn.Dropout(dropout_value)\n        ) # n_out = 6 - 3 + 1 = 4, # Jout = 2, # Rfout = 16 + (3-1)*2 = 20\n\n\n        self.gapblock = nn.Sequential(\n            nn.AvgPool2d(kernel_size=(4, 4))) # n_out = 5 - 5 + 1=1, # Jout = 2, # Rfout = 24 + (5-1)*2 = 32\n\n        self.outputblock = nn.Sequential(\n            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(1,1), padding=0, bias=False)\n        )\n\n    def forward(self, x):\n        x = self.inputblock(x)\n        x = self.hiddenblock1(x)\n        x = self.transistionblock(x)\n        x = self.maxPool1(x)\n        x = self.hiddenblock2(x)\n        x = self.hiddenblock3(x)\n        x = self.hiddenblock5(x)\n        x = self.hiddenblock6(x)\n        x = self.gapblock(x)\n        # x = self.outputblock(x)\n\n        x = x.view(-1, 10)\n        return F.log_softmax(x, dim=-1)\n\ndevice = 'cuda'\nmodel = Net().to(device)\n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T19:43:18.948581Z","iopub.execute_input":"2024-12-13T19:43:18.948938Z","iopub.status.idle":"2024-12-13T19:43:19.165593Z","shell.execute_reply.started":"2024-12-13T19:43:18.948901Z","shell.execute_reply":"2024-12-13T19:43:19.164689Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T19:43:19.166749Z","iopub.execute_input":"2024-12-13T19:43:19.167023Z","iopub.status.idle":"2024-12-13T19:43:28.865339Z","shell.execute_reply.started":"2024-12-13T19:43:19.166996Z","shell.execute_reply":"2024-12-13T19:43:28.864500Z"}},"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from torchsummary import summary\n\nsummary(model, input_size=(1, 28, 28))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T19:43:28.866590Z","iopub.execute_input":"2024-12-13T19:43:28.866891Z","iopub.status.idle":"2024-12-13T19:43:29.549419Z","shell.execute_reply.started":"2024-12-13T19:43:28.866863Z","shell.execute_reply":"2024-12-13T19:43:29.548618Z"}},"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 16, 26, 26]             144\n              ReLU-2           [-1, 16, 26, 26]               0\n       BatchNorm2d-3           [-1, 16, 26, 26]              32\n           Dropout-4           [-1, 16, 26, 26]               0\n            Conv2d-5           [-1, 16, 24, 24]           2,304\n              ReLU-6           [-1, 16, 24, 24]               0\n       BatchNorm2d-7           [-1, 16, 24, 24]              32\n           Dropout-8           [-1, 16, 24, 24]               0\n            Conv2d-9           [-1, 14, 24, 24]             238\n        MaxPool2d-10           [-1, 14, 12, 12]               0\n           Conv2d-11           [-1, 14, 10, 10]           1,764\n             ReLU-12           [-1, 14, 10, 10]               0\n      BatchNorm2d-13           [-1, 14, 10, 10]              28\n          Dropout-14           [-1, 14, 10, 10]               0\n           Conv2d-15             [-1, 12, 8, 8]           1,512\n             ReLU-16             [-1, 12, 8, 8]               0\n      BatchNorm2d-17             [-1, 12, 8, 8]              24\n          Dropout-18             [-1, 12, 8, 8]               0\n           Conv2d-19             [-1, 10, 6, 6]           1,080\n             ReLU-20             [-1, 10, 6, 6]               0\n      BatchNorm2d-21             [-1, 10, 6, 6]              20\n          Dropout-22             [-1, 10, 6, 6]               0\n           Conv2d-23             [-1, 10, 4, 4]             900\n             ReLU-24             [-1, 10, 4, 4]               0\n      BatchNorm2d-25             [-1, 10, 4, 4]              20\n          Dropout-26             [-1, 10, 4, 4]               0\n        AvgPool2d-27             [-1, 10, 1, 1]               0\n================================================================\nTotal params: 8,098\nTrainable params: 8,098\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.77\nParams size (MB): 0.03\nEstimated Total Size (MB): 0.80\n----------------------------------------------------------------\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from tqdm import tqdm\n\ntrain_losses = []\ntest_losses = []\ntrain_acc = []\ntest_acc = []\n\ndef train(model, device, train_loader, optimizer, epoch):\n  model.train()\n  pbar = tqdm(train_loader)\n  correct = 0\n  processed = 0\n  for batch_idx, (data, target) in enumerate(pbar):\n    # get samples\n    data, target = data.to(device), target.to(device)\n\n    # Init\n    optimizer.zero_grad()\n    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. \n    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n\n    # Predict\n    y_pred = model(data)\n\n    # Calculate loss\n    loss = F.nll_loss(y_pred, target)\n    train_losses.append(loss)\n\n    # Backpropagation\n    loss.backward()\n    optimizer.step()\n\n    # Update pbar-tqdm\n    \n    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n    correct += pred.eq(target.view_as(pred)).sum().item()\n    processed += len(data)\n\n    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n    train_acc.append(100*correct/processed)\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    test_losses.append(test_loss)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n    \n    test_acc.append(100. * correct / len(test_loader.dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T19:43:29.550597Z","iopub.execute_input":"2024-12-13T19:43:29.550967Z","iopub.status.idle":"2024-12-13T19:43:29.560847Z","shell.execute_reply.started":"2024-12-13T19:43:29.550930Z","shell.execute_reply":"2024-12-13T19:43:29.560028Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from torch.optim.lr_scheduler import StepLR\n\nmodel =  Net().to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nscheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n\n\nEPOCHS = 15\nfor epoch in range(EPOCHS):\n    print(\"EPOCH:\", epoch)\n    train(model, device, train_loader, optimizer, epoch)\n    scheduler.step()\n    test(model, device, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T19:43:29.561944Z","iopub.execute_input":"2024-12-13T19:43:29.562196Z","iopub.status.idle":"2024-12-13T19:45:58.145473Z","shell.execute_reply.started":"2024-12-13T19:43:29.562172Z","shell.execute_reply":"2024-12-13T19:45:58.144397Z"}},"outputs":[{"name":"stdout","text":"EPOCH: 0\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.1453649252653122 Batch_id=468 Accuracy=93.30: 100%|██████████| 469/469 [00:08<00:00, 53.55it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0917, Accuracy: 9801/10000 (98.01%)\n\nEPOCH: 1\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.16935063898563385 Batch_id=468 Accuracy=97.86: 100%|██████████| 469/469 [00:08<00:00, 55.29it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0526, Accuracy: 9877/10000 (98.77%)\n\nEPOCH: 2\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.060486745089292526 Batch_id=468 Accuracy=98.16: 100%|██████████| 469/469 [00:09<00:00, 52.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0505, Accuracy: 9871/10000 (98.71%)\n\nEPOCH: 3\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.08674818277359009 Batch_id=468 Accuracy=98.44: 100%|██████████| 469/469 [00:08<00:00, 54.57it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0325, Accuracy: 9912/10000 (99.12%)\n\nEPOCH: 4\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.09464696049690247 Batch_id=468 Accuracy=98.54: 100%|██████████| 469/469 [00:08<00:00, 53.89it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0323, Accuracy: 9911/10000 (99.11%)\n\nEPOCH: 5\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.058694858103990555 Batch_id=468 Accuracy=98.74: 100%|██████████| 469/469 [00:09<00:00, 51.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0270, Accuracy: 9934/10000 (99.34%)\n\nEPOCH: 6\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.060633644461631775 Batch_id=468 Accuracy=98.89: 100%|██████████| 469/469 [00:08<00:00, 53.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0265, Accuracy: 9934/10000 (99.34%)\n\nEPOCH: 7\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.04381522908806801 Batch_id=468 Accuracy=98.90: 100%|██████████| 469/469 [00:08<00:00, 53.06it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0260, Accuracy: 9933/10000 (99.33%)\n\nEPOCH: 8\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.03912774845957756 Batch_id=468 Accuracy=98.94: 100%|██████████| 469/469 [00:09<00:00, 50.58it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0266, Accuracy: 9930/10000 (99.30%)\n\nEPOCH: 9\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.06959942728281021 Batch_id=468 Accuracy=98.85: 100%|██████████| 469/469 [00:08<00:00, 53.36it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0247, Accuracy: 9934/10000 (99.34%)\n\nEPOCH: 10\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.019604947417974472 Batch_id=468 Accuracy=99.00: 100%|██████████| 469/469 [00:08<00:00, 53.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0247, Accuracy: 9935/10000 (99.35%)\n\nEPOCH: 11\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.08042048662900925 Batch_id=468 Accuracy=98.93: 100%|██████████| 469/469 [00:09<00:00, 52.07it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0256, Accuracy: 9933/10000 (99.33%)\n\nEPOCH: 12\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.04699130728840828 Batch_id=468 Accuracy=98.96: 100%|██████████| 469/469 [00:08<00:00, 53.03it/s] \n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0247, Accuracy: 9930/10000 (99.30%)\n\nEPOCH: 13\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.023696856573224068 Batch_id=468 Accuracy=98.98: 100%|██████████| 469/469 [00:08<00:00, 52.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0248, Accuracy: 9934/10000 (99.34%)\n\nEPOCH: 14\n","output_type":"stream"},{"name":"stderr","text":"Loss=0.035197172313928604 Batch_id=468 Accuracy=98.91: 100%|██████████| 469/469 [00:08<00:00, 53.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nTest set: Average loss: 0.0252, Accuracy: 9932/10000 (99.32%)\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}