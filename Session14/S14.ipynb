{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_training_params(model_params, seq_length=1024, batch_size=256, warmup_fraction=0.01):\n",
    "    \"\"\"\n",
    "    Calculate optimal training parameters for a given model size based on Chinchilla scaling laws.\n",
    "    \n",
    "    Args:\n",
    "        model_params (int): Number of model parameters in billions (e.g., 1 for 1B).\n",
    "        seq_length (int): Sequence length used in training (default is 1024).\n",
    "        batch_size (int): Batch size for training (default is 256).\n",
    "        warmup_fraction (float): Fraction of total training steps used for warmup (default is 0.01).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing training parameters.\n",
    "    \"\"\"\n",
    "    # Constants\n",
    "    tokens_per_param = 20  # Chinchilla scaling: 20 tokens per parameter\n",
    "    optimal_tokens = model_params * tokens_per_param * 1e9  # Total tokens to train on\n",
    "    \n",
    "    # Calculate total training steps\n",
    "    steps_per_epoch = optimal_tokens / (batch_size * seq_length)\n",
    "    total_training_steps = math.ceil(steps_per_epoch)\n",
    "    \n",
    "    # Warmup steps\n",
    "    warmup_steps = math.ceil(warmup_fraction * total_training_steps)\n",
    "    \n",
    "    # Optimizer hyperparameters (based on practical defaults)\n",
    "    if model_params < 1:  # For smaller models (<1B)\n",
    "        lr = 2e-4\n",
    "        weight_decay = 0.1\n",
    "    elif model_params <= 10:  # For medium-sized models (1B-10B)\n",
    "        lr = 3e-4\n",
    "        weight_decay = 0.1\n",
    "    else:  # For larger models (>10B)\n",
    "        lr = 1e-4\n",
    "        weight_decay = 0.01\n",
    "    \n",
    "    # Expected final loss (heuristic based on model size)\n",
    "    expected_loss = 1.69 - 0.03 * math.log10(model_params) \n",
    "    # update the expected_loss with Chinchilla scaling law\n",
    "    # expected_loss = 1.69 + 406.4/(model_params**0.28) + 410/(model_params**0.38)\n",
    "    \n",
    "    return {\n",
    "        \"Optimal Training Tokens\": f\"{optimal_tokens:.2e} tokens\",\n",
    "        \"Batch Size\": batch_size,\n",
    "        \"Sequence Length\": seq_length,\n",
    "        \"Total Training Steps\": total_training_steps,\n",
    "        \"Warmup Steps\": warmup_steps,\n",
    "        \"Learning Rate (LR)\": lr,\n",
    "        \"Weight Decay\": weight_decay,\n",
    "        \"Expected Final Loss\": round(expected_loss, 4)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the LLM Training Parameter Calculator!\n",
      "\n",
      "=== Training Parameters ===\n",
      "Optimal Training Tokens: 2.90e+09 tokens\n",
      "Batch Size: 64\n",
      "Sequence Length: 1024\n",
      "Total Training Steps: 44251\n",
      "Warmup Steps: 443\n",
      "Learning Rate (LR): 0.0002\n",
      "Weight Decay: 0.1\n",
      "Expected Final Loss: 1.7152\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to the LLM Training Parameter Calculator!\")\n",
    "# model_params = float(input(\"Enter the model parameters (in billions, e.g., 1 for 1B): \"))\n",
    "# seq_length = int(input(\"Enter the sequence length (default: 1024): \") or 1024)\n",
    "# batch_size = int(input(\"Enter the batch size (default: 256): \") or 256)\n",
    "model_params = 0.145\n",
    "seq_length = 1024\n",
    "batch_size = 64\n",
    "\n",
    "training_params = calculate_training_params(model_params, seq_length, batch_size)\n",
    "print(\"\\n=== Training Parameters ===\")\n",
    "for key, value in training_params.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Cross-Entropy Loss:\n",
      "Discriminator output (real): tensor([[0.4859],\n",
      "        [0.5076],\n",
      "        [0.5850],\n",
      "        [0.4937]], grad_fn=<SigmoidBackward0>)\n",
      "Discriminator output (fake): tensor([[0.4885],\n",
      "        [0.5176],\n",
      "        [0.5045],\n",
      "        [0.4825]], grad_fn=<SigmoidBackward0>)\n",
      "Discriminator loss (real): 0.6604079604148865\n",
      "Discriminator loss (fake): 0.690050482749939\n",
      "Total Discriminator Loss: 1.3504583835601807\n",
      "Generator Loss: 0.6970165967941284\n",
      "\n",
      "Least Squares Loss:\n",
      "Discriminator output (real): tensor([[0.4859],\n",
      "        [0.5076],\n",
      "        [0.5850],\n",
      "        [0.4937]], grad_fn=<SigmoidBackward0>)\n",
      "Discriminator output (fake): tensor([[0.4885],\n",
      "        [0.5176],\n",
      "        [0.5045],\n",
      "        [0.4825]], grad_fn=<SigmoidBackward0>)\n",
      "Discriminator loss (real): 0.11690548062324524\n",
      "Discriminator loss (fake): 0.12422600388526917\n",
      "Total Discriminator Loss: 0.2411314845085144\n",
      "Generator Loss: 0.12596707046031952\n"
     ]
    }
   ],
   "source": [
    "# GAN Loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set up device for computation\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "\n",
    "# Define Binary Cross-Entropy Loss for Vanilla GANs\n",
    "bce_loss = nn.BCELoss()\n",
    "\n",
    "# Define a sample batch size and the size of latent space\n",
    "batch_size = 4\n",
    "latent_dim = 100\n",
    "\n",
    "# Fake and real labels for the discriminator\n",
    "real_labels = torch.ones(batch_size, 1).to(device)  # Real labels (1s)\n",
    "fake_labels = torch.zeros(batch_size, 1).to(device)  # Fake labels (0s)\n",
    "\n",
    "# Inputs for Discriminator Loss\n",
    "# Real data (from the dataset)\n",
    "real_data = torch.randn(batch_size, 3, 64, 64).to(device)  # Simulated real images (e.g., 3 channels, 64x64)\n",
    "\n",
    "# Fake data (generated by the Generator)\n",
    "generator = nn.Sequential(\n",
    "    nn.Linear(latent_dim, 3 * 64 * 64),  # Fully connected layer to project latent space to image space\n",
    "    nn.Tanh()  # Tanh activation to output images with values between -1 and 1\n",
    ").to(device)\n",
    "\n",
    "latent_vectors = torch.randn(batch_size, latent_dim).to(device)  # Random noise (latent space)\n",
    "fake_data = generator(latent_vectors).view(batch_size, 3, 64, 64)  # Reshape to image dimensions\n",
    "\n",
    "# Discriminator (a simple model for this demonstration)\n",
    "discriminator = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(3 * 64 * 64, 128),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(128, 1),\n",
    "    nn.Sigmoid()  # Outputs probabilities between 0 and 1\n",
    ").to(device)\n",
    "\n",
    "# Forward pass for real and fake data through the discriminator\n",
    "real_output = discriminator(real_data)  # Discriminator output for real data\n",
    "fake_output = discriminator(fake_data.detach())  # Discriminator output for fake data\n",
    "\n",
    "# Calculate Discriminator Loss using Binary Cross-Entropy\n",
    "# Loss for real data (discriminator should classify it as real)\n",
    "loss_real = bce_loss(real_output, real_labels)\n",
    "\n",
    "# Loss for fake data (discriminator should classify it as fake)\n",
    "loss_fake = bce_loss(fake_output, fake_labels)\n",
    "\n",
    "d_loss = loss_real + loss_fake  # Total discriminator loss\n",
    "\n",
    "# Generator Loss (goal: fool the discriminator into classifying fake data as real)\n",
    "g_loss = bce_loss(discriminator(fake_data), real_labels)  # Use real_labels to simulate \"fooling\"\n",
    "\n",
    "# Print outputs for Binary Cross-Entropy Loss\n",
    "print(\"Binary Cross-Entropy Loss:\")\n",
    "print(\"Discriminator output (real):\", real_output)\n",
    "print(\"Discriminator output (fake):\", fake_output)\n",
    "print(\"Discriminator loss (real):\", loss_real.item())\n",
    "print(\"Discriminator loss (fake):\", loss_fake.item())\n",
    "print(\"Total Discriminator Loss:\", d_loss.item())\n",
    "print(\"Generator Loss:\", g_loss.item())\n",
    "\n",
    "# Least Squares Loss for LSGAN\n",
    "# Define real and fake labels for Least Squares Loss\n",
    "real_labels_ls = torch.ones(batch_size, 1).to(device)  # Real labels (target = 1)\n",
    "fake_labels_ls = torch.zeros(batch_size, 1).to(device)  # Fake labels (target = 0)\n",
    "\n",
    "# Calculate Discriminator Loss using Least Squares Loss\n",
    "loss_real_ls = 0.5 * torch.mean((real_output - real_labels_ls) ** 2)  # Loss for real data\n",
    "loss_fake_ls = 0.5 * torch.mean((fake_output - fake_labels_ls) ** 2)  # Loss for fake data\n",
    "\n",
    "d_loss_ls = loss_real_ls + loss_fake_ls  # Total discriminator loss\n",
    "\n",
    "# Generator Loss (goal: fool the discriminator into classifying fake data as real)\n",
    "g_loss_ls = 0.5 * torch.mean((discriminator(fake_data) - real_labels_ls) ** 2)\n",
    "\n",
    "# Print outputs for Least Squares Loss\n",
    "print(\"\\nLeast Squares Loss:\")\n",
    "print(\"Discriminator output (real):\", real_output)\n",
    "print(\"Discriminator output (fake):\", fake_output)\n",
    "print(\"Discriminator loss (real):\", loss_real_ls.item())\n",
    "print(\"Discriminator loss (fake):\", loss_fake_ls.item())\n",
    "print(\"Total Discriminator Loss:\", d_loss_ls.item())\n",
    "print(\"Generator Loss:\", g_loss_ls.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL (Kullback-Leibler) divergence loss is widely used in various contexts of deep neural networks (DNNs) and large language models (LLMs). Below are some key applications:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Knowledge Distillation**\n",
    "- **Purpose**: To transfer knowledge from a larger model (teacher) to a smaller model (student).\n",
    "- **Usage**: The student model is trained to minimize the KL divergence between the teacher's soft predictions (probability distribution over classes) and its own predictions.\n",
    "- **Equation**:\n",
    "  $$\n",
    "  \\text{KL}(P || Q) = \\\\sum P(x) \\\\log\\\\frac{P(x)}{Q(x)}\n",
    "  $$\n",
    "  where \\(P(x)\\) is the teacher's output distribution (softened with temperature), and \\(Q(x)\\) is the student's output.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4667\n",
      "Epoch 2, Loss: 0.4288\n",
      "Epoch 3, Loss: 0.3929\n",
      "Epoch 4, Loss: 0.3590\n",
      "Epoch 5, Loss: 0.3273\n",
      "Epoch 6, Loss: 0.2976\n",
      "Epoch 7, Loss: 0.2701\n",
      "Epoch 8, Loss: 0.2445\n",
      "Epoch 9, Loss: 0.2210\n",
      "Epoch 10, Loss: 0.1995\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple teacher model\n",
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 5)  # Input size 10, output size 5\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.softmax(self.fc(x), dim=1)  # Softmax for probability distribution\n",
    "\n",
    "# Define a simple student model\n",
    "class StudentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 5)  # Smaller capacity model with the same output size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.softmax(self.fc(x), dim=1)  # Softmax for probability distribution\n",
    "\n",
    "# Instantiate teacher and student models\n",
    "teacher = TeacherModel()\n",
    "student = StudentModel()\n",
    "\n",
    "# Move models to the appropriate device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "teacher.to(device)\n",
    "student.to(device)\n",
    "\n",
    "# Define a KL divergence loss\n",
    "kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "# Create a dummy input tensor\n",
    "input_data = torch.randn(16, 10).to(device)  # Batch size 16, input size 10\n",
    "\n",
    "# Teacher model generates predictions\n",
    "with torch.no_grad():  # Teacher is pre-trained, so no gradient updates are needed\n",
    "    teacher_output = teacher(input_data)\n",
    "\n",
    "# Define optimizer for the student model\n",
    "optimizer = optim.Adam(student.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop for the student model\n",
    "for epoch in range(10):  # Simple 10-epoch training\n",
    "    student_output = student(input_data)  # Student's predictions\n",
    "\n",
    "    # Compute KL divergence loss between teacher and student outputs\n",
    "    loss = kl_loss(torch.log(student_output), teacher_output)\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss for each epoch\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Variational Autoencoders (VAEs)**\n",
    "- **Purpose**: To learn a latent space representation of data.\n",
    "- **Usage**: In VAEs, KL divergence regularizes the latent space by ensuring that the approximate posterior distribution \\(q(z|x)\\) is close to the prior distribution \\(p(z)\\), typically a standard normal distribution \\(N(0, 1)\\).\n",
    "- **Loss Term**:\n",
    "  $$\n",
    "  \\\\text{KL}(q(z|x) || p(z)) = \\\\int q(z|x) \\\\log \\\\frac{q(z|x)}{p(z)} dz\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Reconstruction Loss: 196.6911, KL Loss: 3.5263, Total Loss: 200.2175\n",
      "Epoch 2, Reconstruction Loss: 195.2261, KL Loss: 3.0601, Total Loss: 198.2862\n",
      "Epoch 3, Reconstruction Loss: 194.2710, KL Loss: 2.6456, Total Loss: 196.9165\n",
      "Epoch 4, Reconstruction Loss: 192.3088, KL Loss: 2.2894, Total Loss: 194.5983\n",
      "Epoch 5, Reconstruction Loss: 191.4650, KL Loss: 1.9903, Total Loss: 193.4553\n",
      "Epoch 6, Reconstruction Loss: 190.8412, KL Loss: 1.7417, Total Loss: 192.5830\n",
      "Epoch 7, Reconstruction Loss: 190.4447, KL Loss: 1.5358, Total Loss: 191.9805\n",
      "Epoch 8, Reconstruction Loss: 187.5156, KL Loss: 1.3771, Total Loss: 188.8927\n",
      "Epoch 9, Reconstruction Loss: 187.4942, KL Loss: 1.2449, Total Loss: 188.7390\n",
      "Epoch 10, Reconstruction Loss: 187.6893, KL Loss: 1.1573, Total Loss: 188.8466\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the encoder network\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 6)  # Input size 10, hidden size 6\n",
    "        self.fc_mu = nn.Linear(6, 3)  # Latent mean\n",
    "        self.fc_logvar = nn.Linear(6, 3)  # Latent log variance\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        return mu, logvar\n",
    "\n",
    "# Define the decoder network\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 6)  # Latent size 3, hidden size 6\n",
    "        self.fc2 = nn.Linear(6, 10)  # Hidden size 6, output size 10\n",
    "\n",
    "    def forward(self, z):\n",
    "        hidden = torch.relu(self.fc1(z))\n",
    "        reconstructed = torch.sigmoid(self.fc2(hidden))\n",
    "        return reconstructed\n",
    "\n",
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        # Reparameterization trick\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, mu, logvar\n",
    "\n",
    "# Instantiate the VAE model\n",
    "vae = VAE()\n",
    "\n",
    "# Move model to the appropriate device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "vae.to(device)\n",
    "\n",
    "# Define optimizer and reconstruction loss\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.01)\n",
    "reconstruction_loss_fn = nn.MSELoss(reduction='sum')  # Reconstruction loss\n",
    "\n",
    "# Create a dummy input tensor\n",
    "input_data = torch.randn(16, 10).to(device)  # Batch size 16, input size 10\n",
    "\n",
    "# Training loop for the VAE\n",
    "for epoch in range(10):  # Simple 10-epoch training\n",
    "    reconstructed, mu, logvar = vae(input_data)  # Forward pass\n",
    "\n",
    "    # Reconstruction loss\n",
    "    reconstruction_loss = reconstruction_loss_fn(reconstructed, input_data)\n",
    "\n",
    "    # KL divergence loss\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the losses for each epoch\n",
    "    print(f\"Epoch {epoch + 1}, Reconstruction Loss: {reconstruction_loss.item():.4f}, KL Loss: {kl_loss.item():.4f}, Total Loss: {total_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Reinforcement Learning (Policy Optimization)**\n",
    "- **Purpose**: To ensure stability in policy updates during training.\n",
    "- **Usage**:\n",
    "  - In **Trust Region Policy Optimization (TRPO)** or **Proximal Policy Optimization (PPO)**, KL divergence is used to limit the step size when updating the policy to prevent drastic changes.\n",
    "  - KL divergence measures how different the new policy $$\\\\pi_{\\\\theta}(a|s)$$ is from the old policy $$\\\\pi_{\\\\text{old}}(a|s)$$.\n",
    "- **Loss Term**:\n",
    "  $$\n",
    "  \\\\text{KL}(\\\\pi_{\\\\text{old}} || \\\\pi_{\\\\theta})\n",
    "  $$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, KL Loss: 0.0087, Entropy: 0.6812, Total Loss: 0.0019\n",
      "Epoch 2, KL Loss: 0.0053, Entropy: 0.6870, Total Loss: -0.0015\n",
      "Epoch 3, KL Loss: 0.0041, Entropy: 0.6887, Total Loss: -0.0028\n",
      "Epoch 4, KL Loss: 0.0032, Entropy: 0.6881, Total Loss: -0.0037\n",
      "Epoch 5, KL Loss: 0.0023, Entropy: 0.6864, Total Loss: -0.0045\n",
      "Epoch 6, KL Loss: 0.0017, Entropy: 0.6839, Total Loss: -0.0052\n",
      "Epoch 7, KL Loss: 0.0015, Entropy: 0.6810, Total Loss: -0.0053\n",
      "Epoch 8, KL Loss: 0.0017, Entropy: 0.6786, Total Loss: -0.0051\n",
      "Epoch 9, KL Loss: 0.0017, Entropy: 0.6773, Total Loss: -0.0050\n",
      "Epoch 10, KL Loss: 0.0016, Entropy: 0.6774, Total Loss: -0.0052\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Simple policy network for reinforcement learning\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.softmax(self.fc2(torch.relu(self.fc1(x))), dim=-1)\n",
    "\n",
    "# Define a simple environment (e.g., 1D state space with discrete actions)\n",
    "state_dim = 4  # State size\n",
    "action_dim = 2  # Number of actions\n",
    "policy_old = PolicyNetwork(state_dim, action_dim)\n",
    "policy_new = PolicyNetwork(state_dim, action_dim)\n",
    "\n",
    "# Move models to the appropriate device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "policy_old.to(device)\n",
    "policy_new.to(device)\n",
    "\n",
    "# Define an optimizer for the new policy\n",
    "optimizer = optim.Adam(policy_new.parameters(), lr=0.01)\n",
    "\n",
    "# Dummy data for demonstration\n",
    "batch_size = 16\n",
    "states = torch.randn(batch_size, state_dim).to(device)  # Random states\n",
    "old_probs = policy_old(states).detach()  # Probabilities from the old policy\n",
    "\n",
    "# Training loop for policy updates with KL divergence\n",
    "for epoch in range(10):  # Simple 10-epoch training\n",
    "    # Get the new probabilities from the updated policy\n",
    "    new_probs = policy_new(states)\n",
    "\n",
    "    # Compute KL divergence between old and new policy probabilities\n",
    "    kl_loss = torch.sum(old_probs * torch.log(old_probs / new_probs), dim=1).mean()\n",
    "\n",
    "    # Add an optional entropy regularization term (to encourage exploration)\n",
    "    entropy = -torch.sum(new_probs * torch.log(new_probs + 1e-10), dim=1).mean()\n",
    "    total_loss = kl_loss - 0.01 * entropy  # Weight entropy with a small coefficient\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the losses for each epoch\n",
    "    print(f\"Epoch {epoch + 1}, KL Loss: {kl_loss.item():.4f}, Entropy: {entropy.item():.4f}, Total Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Text KL Loss: nan, Image KL Loss: nan, Total Loss: nan\n",
      "Epoch 2, Text KL Loss: nan, Image KL Loss: nan, Total Loss: nan\n",
      "Epoch 3, Text KL Loss: nan, Image KL Loss: nan, Total Loss: nan\n",
      "Epoch 4, Text KL Loss: nan, Image KL Loss: nan, Total Loss: nan\n",
      "Epoch 5, Text KL Loss: nan, Image KL Loss: nan, Total Loss: nan\n",
      "Epoch 6, Text KL Loss: nan, Image KL Loss: nan, Total Loss: nan\n",
      "Epoch 7, Text KL Loss: nan, Image KL Loss: nan, Total Loss: nan\n",
      "Epoch 8, Text KL Loss: nan, Image KL Loss: nan, Total Loss: nan\n",
      "Epoch 9, Text KL Loss: nan, Image KL Loss: nan, Total Loss: nan\n",
      "Epoch 10, Text KL Loss: nan, Image KL Loss: nan, Total Loss: nan\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define simple text and image encoders for contrastive learning\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.normalize(self.fc(x), p=2, dim=-1)  # L2 normalize\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.normalize(self.fc(x), p=2, dim=-1)  # L2 normalize\n",
    "\n",
    "# Define dimensions\n",
    "text_input_dim = 128  # Text embedding input size\n",
    "image_input_dim = 256  # Image embedding input size\n",
    "embed_dim = 64  # Common embedding dimension\n",
    "\n",
    "# Instantiate encoders\n",
    "text_encoder = TextEncoder(text_input_dim, embed_dim)\n",
    "image_encoder = ImageEncoder(image_input_dim, embed_dim)\n",
    "\n",
    "# Move models to the appropriate device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "text_encoder.to(device)\n",
    "image_encoder.to(device)\n",
    "\n",
    "# Define an optimizer for both encoders\n",
    "optimizer = optim.Adam(list(text_encoder.parameters()) + list(image_encoder.parameters()), lr=0.001)\n",
    "\n",
    "# Dummy data for demonstration\n",
    "batch_size = 16\n",
    "text_embeddings = torch.randn(batch_size, text_input_dim).to(device)  # Simulated text embeddings\n",
    "image_embeddings = torch.randn(batch_size, image_input_dim).to(device)  # Simulated image embeddings\n",
    "\n",
    "# Training loop for contrastive learning\n",
    "for epoch in range(10):  # Simple 10-epoch training\n",
    "    # Encode text and image embeddings\n",
    "    text_features = text_encoder(text_embeddings)\n",
    "    image_features = image_encoder(image_embeddings)\n",
    "\n",
    "    # Compute similarity matrices\n",
    "    text_to_image_sim = torch.matmul(text_features, image_features.T)  # Text-to-image similarity\n",
    "    image_to_text_sim = torch.matmul(image_features, text_features.T)  # Image-to-text similarity\n",
    "\n",
    "    # Compute probability distributions\n",
    "    text_probs = nn.functional.softmax(text_to_image_sim, dim=1)  # Row-wise softmax\n",
    "    image_probs = nn.functional.softmax(image_to_text_sim, dim=1)  # Row-wise softmax\n",
    "\n",
    "    # Ground truth: diagonal should be the highest (self-similarity)\n",
    "    target_probs = torch.eye(batch_size).to(device)  # Identity matrix as ground truth\n",
    "\n",
    "    # Compute KL divergence loss\n",
    "    text_kl_loss = torch.sum(target_probs * torch.log(target_probs / text_probs), dim=1).mean()\n",
    "    image_kl_loss = torch.sum(target_probs * torch.log(target_probs / image_probs), dim=1).mean()\n",
    "    \n",
    "    # Total loss (average of text-to-image and image-to-text losses)\n",
    "    total_loss = (text_kl_loss + image_kl_loss) / 2\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the losses for each epoch\n",
    "    print(f\"Epoch {epoch + 1}, Text KL Loss: {text_kl_loss.item():.4f}, Image KL Loss: {image_kl_loss.item():.4f}, Total Loss: {total_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focal Loss\n",
    "\n",
    "### **Focal Loss: Usage**\n",
    "\n",
    "Focal Loss is primarily designed to address the issue of class imbalance in classification tasks. It assigns more importance to hard-to-classify examples and reduces the weight for easily classified examples. Here's where it is commonly used:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Object Detection**\n",
    "- Widely used in **single-stage object detectors** like RetinaNet.\n",
    "- Helps the model focus on hard-to-detect objects (e.g., small, occluded, or overlapping objects) by reducing the loss contribution from well-classified examples (e.g., the background class in object detection).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Classification Loss: 0.2774, BBox Loss: 0.6539, Total Loss: 0.9313\n",
      "Epoch 2, Classification Loss: 0.2704, BBox Loss: 0.7370, Total Loss: 1.0074\n",
      "Epoch 3, Classification Loss: 0.1298, BBox Loss: 0.5171, Total Loss: 0.6468\n",
      "Epoch 4, Classification Loss: 0.0565, BBox Loss: 0.3225, Total Loss: 0.3790\n",
      "Epoch 5, Classification Loss: 0.0170, BBox Loss: 0.2213, Total Loss: 0.2383\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple object detection model (classification + bounding box regression)\n",
    "class ObjectDetectionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ObjectDetectionModel, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Linear(16 * 16 * 16, num_classes)  # Classification head\n",
    "        self.bbox_regressor = nn.Linear(16 * 16 * 16, 4)  # Bounding box regression head\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)  # Flatten features\n",
    "        class_logits = self.classifier(features)\n",
    "        bbox_deltas = self.bbox_regressor(features)\n",
    "        return class_logits, bbox_deltas\n",
    "\n",
    "# Define the Focal Loss for classification\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        \"\"\"\n",
    "        alpha: Balancing factor for class imbalance (e.g., positive vs negative classes).\n",
    "        gamma: Focusing parameter that emphasizes hard-to-classify examples.\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha # balances the importance of different classes, \n",
    "        # particularly in presense of class imbalance\n",
    "        # α = 0.25 for positive class, and α = 0.75 for negative class\n",
    "        # (for binary classification)., it reduces the contribution of the \n",
    "        # dominant class to the loss and increases focus on minority\n",
    "        self.gamma = gamma # controls the focus on hard-to-classify images\n",
    "        # for well classified examples (p_t closing on 1), (1 - p_t)^gamma\n",
    "        # becomes small, reducing their contribution to total loss\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Convert logits to probabilities using softmax\n",
    "        probs = torch.softmax(logits, dim=-1)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        # One-hot encode the target labels\n",
    "        targets_one_hot = nn.functional.one_hot(targets, num_classes=logits.size(-1)).float()  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        # Extract probabilities for the true class (p_t)\n",
    "        pt = torch.sum(probs * targets_one_hot, dim=-1)  # Shape: (batch_size,)\n",
    "\n",
    "        # Compute the focal weight (alpha * (1 - p_t)^gamma)\n",
    "        focal_weight = self.alpha * (1 - pt) ** self.gamma\n",
    "\n",
    "        # Compute the focal loss (-focal_weight * log(p_t))\n",
    "        loss = -focal_weight * torch.log(pt + 1e-8)  # Adding a small constant to avoid log(0)\n",
    "\n",
    "        return loss.mean()  # Return the average loss\n",
    "\n",
    "# Instantiate the model and loss functions\n",
    "num_classes = 5\n",
    "model = ObjectDetectionModel(num_classes=num_classes).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "classification_loss_fn = FocalLoss()\n",
    "bbox_loss_fn = nn.SmoothL1Loss()  # Smooth L1 loss for bounding box regression\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dummy data for demonstration\n",
    "batch_size = 8\n",
    "images = torch.randn(batch_size, 3, 32, 32).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Random images\n",
    "\n",
    "# Random class labels for the batch\n",
    "# Target classes are integers between 0 and num_classes-1\n",
    "# Example: [2, 0, 1, 3, ...] for a batch size of 8\n",
    "target_classes = torch.randint(0, num_classes, (batch_size,)).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Random bounding box coordinates for the batch\n",
    "# Example: [[x1, y1, x2, y2], ...] for each image\n",
    "# These are usually normalized to [0, 1] in real datasets\n",
    "target_bboxes = torch.randn(batch_size, 4).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Training loop for object detection\n",
    "for epoch in range(5):  # Simple 5-epoch training\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    class_logits, bbox_deltas = model(images)\n",
    "\n",
    "    # Compute classification loss using Focal Loss\n",
    "    # The loss emphasizes hard-to-classify examples using the focal weight\n",
    "    cls_loss = classification_loss_fn(class_logits, target_classes)\n",
    "\n",
    "    # Compute bounding box regression loss using Smooth L1 Loss\n",
    "    # This loss penalizes large deviations between predicted and true bounding box coordinates\n",
    "    bbox_loss = bbox_loss_fn(bbox_deltas, target_bboxes)\n",
    "\n",
    "    # Total loss is the sum of classification and regression losses\n",
    "    total_loss = cls_loss + bbox_loss\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    total_loss.backward()  # Compute gradients\n",
    "    optimizer.step()  # Update model parameters\n",
    "\n",
    "    # Print the losses for each epoch\n",
    "    print(f\"Epoch {epoch + 1}, Classification Loss: {cls_loss.item():.4f}, BBox Loss: {bbox_loss.item():.4f}, Total Loss: {total_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Focal Loss Components\n",
    "\n",
    "Focal Loss is a modification of the standard cross-entropy loss to address class imbalance in classification tasks by down-weighting the loss for well-classified examples and focusing on hard-to-classify ones. Here's how it works:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. `γ` (Gamma)**\n",
    "- **Purpose**: Controls the focus on hard-to-classify examples.\n",
    "- **Mechanism**:\n",
    "  - For well-classified examples (high probability for the true class, $$ p_t $$ near 1), the factor $$ (1 - p_t)^\\gamma $$ becomes small, reducing their contribution to the total loss.\n",
    "  - For hard-to-classify examples (low probability for the true class, $$ p_t $$ near 0), the factor $$ (1 - p_t)^\\gamma $$ remains large, increasing their contribution.\n",
    "- **Effect**: Higher $$ \\gamma $$ increases the focus on harder examples. Common values are $$ \\gamma = 2 $$ or $$ \\gamma = 3 $$.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. `α` (Alpha)**\n",
    "- **Purpose**: Balances the importance of different classes, particularly in the presence of class imbalance.\n",
    "- **Mechanism**:\n",
    "  - If $$ \\alpha $$ is set to 0.25 for the positive class and 0.75 for the negative class (for binary classification), it reduces the contribution of the dominant class (negative) to the loss and increases the focus on the minority class (positive).\n",
    "- **Effect**: Helps balance the influence of underrepresented classes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Focal Loss Formula**\n",
    "For a single example:\n",
    "$$\n",
    "\\text{FL}(p_t) = -\\alpha (1 - p_t)^\\gamma \\log(p_t)\n",
    "$$\n",
    "Where:\n",
    "- $$ p_t $$: Probability assigned to the true class.\n",
    "- $$ \\alpha $$: Balancing factor for class weights.\n",
    "- $$ \\gamma $$: Modulating factor for focusing on hard examples.\n",
    "\n",
    "---\n",
    "\n",
    "### **How it Works in the Code**\n",
    "1. **Logits to Probabilities**:\n",
    "   - `probs = torch.softmax(logits, dim=-1)` computes the probability distribution over classes.\n",
    "\n",
    "2. **True Class Probabilities**:\n",
    "   - `pt = torch.sum(probs * targets_one_hot, dim=-1)` extracts the predicted probabilities for the true class.\n",
    "\n",
    "3. **Focal Weight**:\n",
    "   - `focal_weight = self.alpha * (1 - pt) ** self.gamma` applies the modulating factor, emphasizing harder examples.\n",
    "\n",
    "4. **Final Loss**:\n",
    "   - `loss = -focal_weight * torch.log(pt + 1e-8)` computes the weighted log loss, where small $$ p_t $$ values contribute more due to the $$ (1 - p_t)^\\gamma $$ term.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "- **`γ` (Gamma)**: Focuses on hard examples by down-weighting easy examples.\n",
    "- **`α` (Alpha)**: Balances the contribution of different classes, addressing class imbalance.\n",
    "- Combined, they make Focal Loss effective in scenarios like object detection with highly imbalanced classes (e.g., background vs. objects). \n",
    "\n",
    "Let me know if you'd like further clarification or an expanded explanation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **2. Imbalanced Datasets**\n",
    "- Used in classification tasks with **severe class imbalance**, such as:\n",
    "  - **Medical imaging**: Identifying rare diseases.\n",
    "  - **Fraud detection**: Classifying rare fraudulent transactions.\n",
    "  - **Anomaly detection**: Detecting rare events or behaviors.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Multi-label Classification**\n",
    "- Applied in scenarios where multiple labels can be assigned to an input, and there is an imbalance in the distribution of labels (e.g., detecting attributes in an image).\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Semantic Segmentation**\n",
    "- Focuses on improving the classification of small or minority regions in segmentation tasks (e.g., segmenting rare object parts).\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Natural Language Processing (NLP)**\n",
    "- Useful in NLP tasks where certain classes are underrepresented, such as:\n",
    "  - Named Entity Recognition (NER) for rare entities.\n",
    "  - Sentiment analysis with imbalanced positive and negative reviews.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Multi-class Imbalance**\n",
    "- Tasks like image classification with a **long-tailed distribution** of classes benefit from Focal Loss, as it reduces the dominance of frequent classes.\n",
    "\n",
    "---\n",
    "\n",
    "Focal Loss is particularly effective when you want to focus on difficult-to-classify examples and mitigate the effects of imbalanced datasets, making it a versatile choice in various machine learning domains. Let me know if you'd like examples or a detailed mathematical explanation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Classification Loss: 1.6428, IoU Loss: 1.0000, Total Loss: 2.6428\n",
      "Epoch 2, Classification Loss: 1.0660, IoU Loss: 1.0000, Total Loss: 2.0660\n",
      "Epoch 3, Classification Loss: 0.6803, IoU Loss: 1.0000, Total Loss: 1.6803\n",
      "Epoch 4, Classification Loss: 0.3360, IoU Loss: 1.0000, Total Loss: 1.3360\n",
      "Epoch 5, Classification Loss: 0.1691, IoU Loss: 1.0000, Total Loss: 1.1691\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple object detection model (classification + bounding box regression)\n",
    "class ObjectDetectionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ObjectDetectionModel, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Linear(16 * 16 * 16, num_classes)  # Classification head\n",
    "        self.bbox_regressor = nn.Linear(16 * 16 * 16, 4)  # Bounding box regression head\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)  # Flatten features\n",
    "        class_logits = self.classifier(features)\n",
    "        bbox_deltas = self.bbox_regressor(features)\n",
    "        return class_logits, bbox_deltas\n",
    "\n",
    "# Define IoU loss for bounding box regression\n",
    "class IoULoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IoULoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred_boxes, target_boxes):\n",
    "        \"\"\"\n",
    "        pred_boxes: Predicted bounding boxes (batch_size, 4) [x1, y1, x2, y2]\n",
    "        target_boxes: Ground truth bounding boxes (batch_size, 4) [x1, y1, x2, y2]\n",
    "        \"\"\"\n",
    "        # Compute intersection\n",
    "        x1 = torch.max(pred_boxes[:, 0], target_boxes[:, 0])\n",
    "        y1 = torch.max(pred_boxes[:, 1], target_boxes[:, 1])\n",
    "        x2 = torch.min(pred_boxes[:, 2], target_boxes[:, 2])\n",
    "        y2 = torch.min(pred_boxes[:, 3], target_boxes[:, 3])\n",
    "\n",
    "        intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n",
    "\n",
    "        # Compute areas of predicted and target boxes\n",
    "        pred_area = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])\n",
    "        target_area = (target_boxes[:, 2] - target_boxes[:, 0]) * (target_boxes[:, 3] - target_boxes[:, 1])\n",
    "\n",
    "        # Compute union\n",
    "        union = pred_area + target_area - intersection\n",
    "\n",
    "        # Compute IoU\n",
    "        iou = intersection / (union + 1e-6)  # Add a small constant to avoid division by zero\n",
    "\n",
    "        # IoU loss\n",
    "        loss = 1 - iou.mean()  # Minimize 1 - IoU\n",
    "        return loss\n",
    "\n",
    "# Instantiate the model and loss functions\n",
    "num_classes = 5\n",
    "model = ObjectDetectionModel(num_classes=num_classes).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "classification_loss_fn = nn.CrossEntropyLoss()  # Cross-entropy for classification\n",
    "bbox_loss_fn = IoULoss()  # IoU loss for bounding box regression\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dummy data for demonstration\n",
    "batch_size = 8\n",
    "images = torch.randn(batch_size, 3, 32, 32).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Random images\n",
    "\n",
    "# Random class labels for the batch\n",
    "target_classes = torch.randint(0, num_classes, (batch_size,)).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Random class labels\n",
    "\n",
    "# Random bounding box coordinates for the batch\n",
    "target_bboxes = torch.randn(batch_size, 4).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Random ground truth boxes\n",
    "pred_bboxes = torch.randn(batch_size, 4).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Predicted boxes\n",
    "\n",
    "# Training loop for object detection\n",
    "for epoch in range(5):  # Simple 5-epoch training\n",
    "    model.train()\n",
    "\n",
    "    # Forward pass through the model\n",
    "    class_logits, bbox_deltas = model(images)\n",
    "\n",
    "    # Compute classification loss\n",
    "    cls_loss = classification_loss_fn(class_logits, target_classes)\n",
    "\n",
    "    # Compute IoU loss for bounding boxes\n",
    "    bbox_loss = bbox_loss_fn(bbox_deltas, target_bboxes)\n",
    "\n",
    "    # Total loss is the sum of classification and IoU losses\n",
    "    total_loss = cls_loss + bbox_loss\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    total_loss.backward()  # Compute gradients\n",
    "    optimizer.step()  # Update model parameters\n",
    "\n",
    "    # Print the losses for each epoch\n",
    "    print(f\"Epoch {epoch + 1}, Classification Loss: {cls_loss.item():.4f}, IoU Loss: {bbox_loss.item():.4f}, Total Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DICE Loss\n",
    "\n",
    "### **Dice Loss: Overview**\n",
    "\n",
    "**Dice Loss** is a loss function designed for **binary and multi-class segmentation tasks**. It measures the overlap between the predicted segmentation and the ground truth, focusing on minimizing differences in areas of the objects being segmented.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Definition**\n",
    "The **Dice Coefficient** (or Dice Similarity Coefficient, DSC) measures the similarity between two sets (predicted and ground truth masks):\n",
    "$$\n",
    "\\text{Dice Coefficient} = \\frac{2 |A \\cap B|}{|A| + |B|}\n",
    "$$\n",
    "Where:\n",
    "- \\(A\\) = Predicted mask.\n",
    "- \\(B\\) = Ground truth mask.\n",
    "\n",
    "The **Dice Loss** is defined as:\n",
    "$$\n",
    "\\text{Dice Loss} = 1 - \\text{Dice Coefficient}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications**\n",
    "1. **Image Segmentation**:\n",
    "   - Used in tasks such as medical imaging (e.g., tumor segmentation in MRI scans).\n",
    "   - Ensures better overlap between predicted and actual segmentations.\n",
    "\n",
    "2. **Imbalanced Data**:\n",
    "   - Particularly effective when dealing with highly imbalanced classes (e.g., small objects in large images).\n",
    "   - Penalizes false positives and false negatives equally.\n",
    "\n",
    "3. **Binary and Multi-Class Segmentation**:\n",
    "   - Can be extended for multi-class segmentation tasks by averaging Dice Loss across all classes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages**\n",
    "1. Focuses on overlap, which is more relevant for segmentation tasks than simple pixel-wise losses (like cross-entropy).\n",
    "2. Handles class imbalance better, as it normalizes by the total size of the masks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice Loss: 0.6024\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define Dice Loss\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        \"\"\"\n",
    "        preds: Predicted segmentation map (batch_size, num_classes, height, width)\n",
    "        targets: Ground truth segmentation map (batch_size, num_classes, height, width)\n",
    "        \"\"\"\n",
    "        smooth = 1.0  # To avoid division by zero\n",
    "        preds = preds.softmax(dim=1)  # Apply softmax to get probabilities\n",
    "\n",
    "        # Flatten predictions and targets for Dice computation\n",
    "        preds_flat = preds.view(preds.size(0), preds.size(1), -1)  # (batch_size, num_classes, -1)\n",
    "        targets_flat = targets.view(targets.size(0), targets.size(1), -1)  # (batch_size, num_classes, -1)\n",
    "\n",
    "        # Compute intersection and union\n",
    "        intersection = (preds_flat * targets_flat).sum(dim=2)  # Intersection per class\n",
    "        union = preds_flat.sum(dim=2) + targets_flat.sum(dim=2)  # Union per class\n",
    "\n",
    "        # Dice coefficient\n",
    "        dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "\n",
    "        # Average over batch and classes\n",
    "        dice_loss = 1 - dice.mean()\n",
    "        return dice_loss\n",
    "\n",
    "# Dummy data for demonstration\n",
    "batch_size = 4\n",
    "num_classes = 3\n",
    "height, width = 64, 64\n",
    "\n",
    "# Predictions (logits) from a model\n",
    "preds = torch.randn(batch_size, num_classes, height, width).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Ground truth segmentation maps (one-hot encoded)\n",
    "targets = torch.randint(0, 2, (batch_size, num_classes, height, width)).float().to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Instantiate Dice Loss\n",
    "dice_loss_fn = DiceLoss()\n",
    "\n",
    "# Compute Dice Loss\n",
    "loss = dice_loss_fn(preds, targets)\n",
    "print(f\"Dice Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Perceptual Loss: Where It Is Used**\n",
    "\n",
    "Perceptual loss measures the perceptual similarity between images rather than pixel-wise differences, focusing on how humans perceive visual differences. It relies on pre-trained deep neural networks, such as VGG, to compare features from intermediate layers. Here are its primary use cases:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Image Super-Resolution**\n",
    "- Perceptual loss is used to train models that generate high-resolution images from low-resolution inputs.\n",
    "- It ensures the output image is visually similar to the ground truth in terms of texture and structure, beyond pixel-level accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Image-to-Image Translation**\n",
    "- Applied in tasks like style transfer, domain adaptation, and image colorization.\n",
    "- Perceptual loss ensures that the transformed image retains the content structure of the input while matching the desired target style.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Generative Adversarial Networks (GANs)**\n",
    "- In tasks like image synthesis and inpainting, perceptual loss helps improve the quality of the generated image by aligning it with human perception.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Neural Style Transfer**\n",
    "- Used to minimize differences in content and style between the input and target images.\n",
    "- Content loss focuses on structural similarity, while style loss ensures stylistic consistency.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Video Frame Prediction**\n",
    "- Ensures temporal consistency in videos by preserving perceptual quality across consecutive frames.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. 3D Reconstruction**\n",
    "- Helps in aligning 3D models or reconstructed objects with 2D image views, improving texture and geometry quality.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Image Denoising and Deblurring**\n",
    "- Used to improve perceptual quality by focusing on restoring image details as seen by humans.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Face Reconstruction and Alignment**\n",
    "- Ensures facial landmarks and features are preserved during tasks like face swapping, alignment, or super-resolution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptual Loss: 0.0199\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple perceptual loss using a pre-trained VGG-like network\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.feature_extractor = feature_extractor  # Pre-trained feature extractor\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False  # Freeze the feature extractor\n",
    "\n",
    "    def forward(self, input_image, target_image):\n",
    "        \"\"\"\n",
    "        input_image: Generated image (batch_size, channels, height, width)\n",
    "        target_image: Ground truth image (batch_size, channels, height, width)\n",
    "        \"\"\"\n",
    "        input_features = self.feature_extractor(input_image)\n",
    "        target_features = self.feature_extractor(target_image)\n",
    "        loss = nn.functional.mse_loss(input_features, target_features)  # MSE on feature maps\n",
    "        return loss\n",
    "\n",
    "# Define a simple feature extractor (e.g., part of a VGG-like network)\n",
    "class SimpleFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleFeatureExtractor, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "# Instantiate the feature extractor and perceptual loss\n",
    "feature_extractor = SimpleFeatureExtractor().to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "perceptual_loss_fn = PerceptualLoss(feature_extractor)\n",
    "\n",
    "# Dummy data for demonstration\n",
    "batch_size = 4\n",
    "input_images = torch.randn(batch_size, 3, 64, 64).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Generated images\n",
    "target_images = torch.randn(batch_size, 3, 64, 64).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Ground truth images\n",
    "\n",
    "# Compute perceptual loss\n",
    "loss = perceptual_loss_fn(input_images, target_images)\n",
    "print(f\"Perceptual Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC Loss\n",
    "\n",
    "### **CTC Loss: Overview**\n",
    "\n",
    "**Connectionist Temporal Classification (CTC) Loss** is designed for sequence prediction tasks where:\n",
    "1. The alignment between input and output sequences is unknown.\n",
    "2. The input and output sequence lengths may differ.\n",
    "\n",
    "It is commonly used in applications where outputs have varying lengths, such as speech recognition, handwriting recognition, and OCR (optical character recognition).\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Features**\n",
    "1. **Alignment-Free**: \n",
    "   - CTC automatically handles the alignment of input and output sequences.\n",
    "   - For example, in speech recognition, the model predicts a probability distribution over characters for each time step of the audio, and CTC determines the best alignment.\n",
    "\n",
    "2. **Blank Tokens**: \n",
    "   - CTC introduces a special \"blank\" token to handle varying sequence lengths.\n",
    "   - Blank tokens are used to fill gaps between output tokens when no explicit output is required.\n",
    "\n",
    "3. **Collapse Repeated Tokens**: \n",
    "   - CTC maps repeated predictions (e.g., \"hhheeelllooo\") and blanks to the target sequence (\"hello\").\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications**\n",
    "1. **Speech Recognition**:\n",
    "   - Used in end-to-end speech models like DeepSpeech.\n",
    "   - Converts acoustic feature sequences into text without requiring pre-aligned transcriptions.\n",
    "   \n",
    "2. **Handwriting Recognition**:\n",
    "   - Maps handwritten strokes to characters or words.\n",
    "\n",
    "3. **Optical Character Recognition (OCR)**:\n",
    "   - Used in systems where text is extracted from images without strict alignment between input pixels and output characters.\n",
    "\n",
    "4. **Sign Language Recognition**:\n",
    "   - Predicts the sequence of words or letters corresponding to gestures in sign language.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works**\n",
    "1. The model outputs a probability distribution over the set of possible characters (including the blank token) for each time step.\n",
    "2. CTC computes the total probability of all valid alignments that map to the target sequence.\n",
    "3. The loss minimizes the negative log probability of the correct output sequence given the input sequence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTC Loss: 5.3860\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple Transformer-based model for sequence prediction\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_classes, num_heads=2, num_layers=2):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(model_dim, num_classes + 1)  # +1 for the blank token\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Project input to model dimension\n",
    "        x = self.transformer(x)  # Pass through transformer layers\n",
    "        x = self.fc(x)  # Project to class probabilities (including blank)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = 13  # Example input dimension (e.g., MFCCs for speech)\n",
    "model_dim = 64  # Model dimension\n",
    "num_classes = 10  # Number of output classes\n",
    "\n",
    "model = SimpleTransformer(input_dim, model_dim, num_classes).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Define the CTC Loss\n",
    "ctc_loss_fn = nn.CTCLoss(blank=num_classes, reduction='mean', zero_infinity=True)\n",
    "\n",
    "# Dummy data for demonstration\n",
    "batch_size = 4\n",
    "sequence_length = 15  # Length of input sequences\n",
    "output_length = 5  # Length of target sequences\n",
    "\n",
    "# Inputs (e.g., MFCC features for speech)\n",
    "inputs = torch.randn(sequence_length, batch_size, input_dim).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Targets (e.g., token indices)\n",
    "targets = torch.randint(0, num_classes, (batch_size * output_length,)).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Input and target lengths\n",
    "input_lengths = torch.full((batch_size,), sequence_length, dtype=torch.long).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "target_lengths = torch.full((batch_size,), output_length, dtype=torch.long).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Forward pass\n",
    "logits = model(inputs)  # Shape: (sequence_length, batch_size, num_classes + 1)\n",
    "log_probs = nn.functional.log_softmax(logits, dim=-1)  # Log probabilities for CTC\n",
    "\n",
    "# Compute CTC Loss\n",
    "loss = ctc_loss_fn(log_probs, targets, input_lengths, target_lengths)\n",
    "print(f\"CTC Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet loss\n",
    "\n",
    "### **Triplet Loss: Overview**\n",
    "\n",
    "**Triplet Loss** is a type of loss function used for **metric learning**. It ensures that embeddings of similar items are closer together while embeddings of dissimilar items are farther apart in the feature space.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works**\n",
    "Triplet loss operates on **triplets of data**:\n",
    "1. **Anchor**: The reference data point.\n",
    "2. **Positive**: A data point similar to the anchor.\n",
    "3. **Negative**: A data point dissimilar to the anchor.\n",
    "\n",
    "The loss is designed to minimize the distance between the **anchor** and **positive** embeddings while maximizing the distance between the **anchor** and **negative** embeddings.\n",
    "\n",
    "The mathematical formulation:\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{triplet}} = \\max \\left( d(a, p) - d(a, n) + \\text{margin}, 0 \\\\right)\n",
    "\\]\n",
    "Where:\n",
    "- \\( a, p, n \\): Anchor, Positive, and Negative embeddings.\n",
    "- \\( d(x, y) \\): Distance metric (usually L2 or cosine distance).\n",
    "- **Margin**: A predefined constant that defines the minimum required separation between \\( d(a, p) \\) and \\( d(a, n) \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications**\n",
    "1. **Face Recognition**:\n",
    "   - Used in models like FaceNet to ensure embeddings of the same person are closer than embeddings of different people.\n",
    "2. **Image Retrieval**:\n",
    "   - Ensures that visually similar images are closer in the embedding space.\n",
    "3. **Speaker Verification**:\n",
    "   - Verifies if two audio samples belong to the same speaker.\n",
    "4. **Signature Verification**:\n",
    "   - Determines whether two signatures belong to the same person.\n",
    "5. **Product Recommendation**:\n",
    "   - Ensures embeddings of similar products are closer together in recommendation systems.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple embedding model for triplet loss\n",
    "class SimpleEmbeddingModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super(SimpleEmbeddingModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, embedding_dim)  # Project to embedding space\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.normalize(self.fc(x), p=2, dim=-1)  # Normalize embeddings\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = 10  # Input feature dimension\n",
    "embedding_dim = 5  # Embedding space dimension\n",
    "model = SimpleEmbeddingModel(input_dim, embedding_dim).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Define Triplet Loss\n",
    "triplet_loss_fn = nn.TripletMarginLoss(margin=1.0, p=2)  # L2 distance with margin 1.0\n",
    "\n",
    "# Dummy data for demonstration\n",
    "batch_size = 4\n",
    "anchor = torch.randn(batch_size, input_dim).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Anchor samples\n",
    "positive = torch.randn(batch_size, input_dim).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Positive samples\n",
    "negative = torch.randn(batch_size, input_dim).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Negative samples\n",
    "\n",
    "# Forward pass to compute embeddings\n",
    "anchor_embed = model(anchor)\n",
    "positive_embed = model(positive)\n",
    "negative_embed = model(negative)\n",
    "\n",
    "# Compute Triplet Loss\n",
    "loss = triplet_loss_fn(anchor_embed, positive_embed, negative_embed)\n",
    "print(f\"Triplet Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **1. Docker Basics**\n",
    "- `docker --version` – Check Docker version.\n",
    "- `docker --help` – Get help for Docker commands.\n",
    "- `docker <subcommand> --help` – Get help for a specific subcommand.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Working with Containers**\n",
    "- `docker run <image>` – Run a container.\n",
    "- `docker ps` – List running containers.\n",
    "- `docker ps -a` – List all containers (including stopped ones).\n",
    "- `docker start <container_id>` – Start a stopped container.\n",
    "- `docker stop <container_id>` – Stop a running container.\n",
    "- `docker restart <container_id>` – Restart a container.\n",
    "- `docker rm <container_id>` – Remove a container.\n",
    "- `docker logs <container_id>` – View logs of a container.\n",
    "- `docker exec -it <container_id> /bin/bash` – Access a container’s shell.\n",
    "- `docker kill <container_id>` – Force-stop a container.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Working with Images**\n",
    "- `docker images` – List all available images.\n",
    "- `docker pull <image>` – Download an image from a registry.\n",
    "- `docker build -t <image_name>:<tag> <path>` – Build an image from a `Dockerfile`.\n",
    "- `docker tag <source_image> <target_image>` – Tag an image with a new name.\n",
    "- `docker rmi <image_id>` – Remove an image.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Docker Compose**\n",
    "- `docker-compose up` – Start all services in the `docker-compose.yml`.\n",
    "- `docker-compose up --build` – Rebuild images and start services.\n",
    "- `docker-compose down` – Stop and remove services and containers.\n",
    "- `docker-compose ps` – List services managed by Docker Compose.\n",
    "- `docker-compose logs` – View logs for services.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Docker Networks**\n",
    "- `docker network ls` – List all networks.\n",
    "- `docker network create <network_name>` – Create a new network.\n",
    "- `docker network inspect <network_name>` – View details of a network.\n",
    "- `docker network rm <network_name>` – Remove a network.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Docker Volumes**\n",
    "- `docker volume ls` – List all volumes.\n",
    "- `docker volume create <volume_name>` – Create a new volume.\n",
    "- `docker volume inspect <volume_name>` – Inspect a volume.\n",
    "- `docker volume rm <volume_name>` – Remove a volume.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. System Cleanup**\n",
    "- `docker system prune` – Remove unused containers, images, networks, and volumes.\n",
    "- `docker container prune` – Remove stopped containers.\n",
    "- `docker image prune` – Remove unused images.\n",
    "- `docker volume prune` – Remove unused volumes.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Inspecting and Debugging**\n",
    "- `docker inspect <container_id>` – View details of a container.\n",
    "- `docker stats` – Monitor resource usage of running containers.\n",
    "- `docker top <container_id>` – Display running processes inside a container.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
