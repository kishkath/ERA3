{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10696419,"sourceType":"datasetVersion","datasetId":6628204}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n# from dataclasses import dataclass\n# import math\n# import tiktoken\n\n# model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n# \n# # -----------------------------------------------------------------------------\n# # Causal Self-Attention Module\n# # -----------------------------------------------------------------------------\n\n# class CasualSelfAttention(nn.Module):\n#     def __init__(self, config):\n#         super().__init__()\n#         # Linear layer to project input embeddings into Queries (Q), Keys (K), and Values (V).\n#         # Output size is 3 times the embedding size because we need Q, K, and V.\n#         self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed)\n\n#         # Linear layer to project the attention output back to the original embedding size.\n#         self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n\n#         # Number of attention heads and embedding size.\n#         self.n_head = config.n_head\n#         self.n_embed = config.n_embed\n\n#         # Create a lower triangular matrix to mask future tokens (for causal/self-attention).\n#         # This ensures the model can't \"peek\" at future tokens during training.\n#         self.register_buffer(\n#             \"bias\",\n#             torch.tril(torch.ones(config.block_size, config.block_size))\n#             .view(1, 1, config.block_size, config.block_size)\n#         )\n\n#     def forward(self, x):\n#         B, T, C = x.size()  # Batch size, sequence length, embedding dimension.\n\n#         # Project the input x to get combined queries, keys, and values.\n#         qkv = self.c_attn(x)\n\n#         # Split the combined projections into separate Q, K, and V tensors.\n#         q, k, v = qkv.split(self.n_embed, dim=2)\n\n#         # Reshape Q, K, V to handle multiple attention heads.\n#         # New shape: (Batch, heads, sequence length, head dimension)\n#         q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n#         k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n#         v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n\n#         # Compute scaled dot-product attention scores.\n#         att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n\n#         # Apply the causal mask to prevent attention to future tokens.\n#         att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n\n#         # Apply softmax to convert attention scores to probabilities.\n#         att = F.softmax(att, dim=-1)\n\n#         # Apply attention weights to the values (V).\n#         y = att @ v\n\n#         # Reshape back to the original input format.\n#         y = y.transpose(1, 2).contiguous().view(B, T, C)\n\n#         # Final linear projection to match the input embedding size.\n#         y = self.c_proj(y)\n#         return y\n\n# # -----------------------------------------------------------------------------\n# # Multi-Layer Perceptron (MLP)\n# # -----------------------------------------------------------------------------\n\n# class MLP(nn.Module):\n#     def __init__(self, config):\n#         super().__init__()\n#         # First linear layer expands the embedding dimension by 4x for richer learning.\n#         self.c_fc = nn.Linear(config.n_embed, 4 * config.n_embed)\n\n#         # GELU activation introduces non-linearity to help the model learn complex patterns.\n#         self.gelu = nn.GELU(approximate=\"tanh\")\n#         # Second linear layer projects the output back to the original embedding size.\n#         self.c_proj = nn.Linear(4 * config.n_embed, config.n_embed)\n#         # Correcting initialization: applying constant scaling of weights.\n#         nn.init.constant_(self.c_proj.weight, 1)\n\n#     def forward(self, x):\n#         x = self.c_fc(x)\n#         x = self.gelu(x)\n#         x = self.c_proj(x)\n#         return x\n\n# # -----------------------------------------------------------------------------\n# # Transformer Block\n# # -----------------------------------------------------------------------------\n\n# class Block(nn.Module):\n#     def __init__(self, config):\n#         super().__init__()\n#         # Layer normalization to stabilize training and improve convergence.\n#         self.ln_1 = nn.LayerNorm(config.n_embed)\n#         # Causal self-attention to capture relationships between tokens.\n#         self.attn = CasualSelfAttention(config)\n#         # Another layer normalization before feeding into the MLP.\n#         self.ln_2 = nn.LayerNorm(config.n_embed)\n#         # Feed-forward neural network (MLP) to process representations from the attention output.\n#         self.mlp = MLP(config)\n\n#     def forward(self, x):\n#         # First sub-layer: apply LayerNorm, then attention, then add a residual connection.\n#         x = x + self.attn(self.ln_1(x))\n#         # Second sub-layer: apply LayerNorm, then MLP, then add a residual connection.\n#         x = x + self.mlp(self.ln_2(x))\n#         return x\n\n# # -----------------------------------------------------------------------------\n# # Configuration Class for SMOLL Model\n# # -----------------------------------------------------------------------------\n\n# @dataclass\n# class SMOLConfig:\n#     block_size: int = 1024  # Maximum tokens (context length) the model can see at once.\n#     vocab_size: int = 50257  # Vocabulary size.\n#     n_layer: int = 12       # Number of transformer blocks (layers).\n#     n_head: int = 12        # Number of attention heads.\n#     n_embed: int = 768      # Dimensionality of token embeddings.\n\n# # -----------------------------------------------------------------------------\n# # SMOLL Model Definition\n# # -----------------------------------------------------------------------------\n\n# class SMOLL(nn.Module):\n#     def __init__(self, config):\n#         super().__init__()\n#         self.config = config\n\n#         # Build transformer components in a ModuleDict.\n#         self.transformer = nn.ModuleDict({\n#             # Token embedding: maps token indices to embedding vectors.\n#             'wte': nn.Embedding(config.vocab_size, config.n_embed),\n#             # Positional embedding: provides a unique embedding for each position.\n#             'wpe': nn.Embedding(config.block_size, config.n_embed),\n#             # A list of transformer blocks.\n#             'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n#             # Final layer normalization.\n#             'ln_f': nn.LayerNorm(config.n_embed),\n#         })\n\n#         # Output layer (language model head) to project transformer output to vocabulary logits.\n#         self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)\n\n#     def forward(self, idx, targets=None):\n#         B, T = idx.size()\n#         # Ensure the sequence length does not exceed the model's block size.\n#         assert T <= self.config.block_size, (\n#             f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n#         )\n\n#         # Create a tensor of position indices.\n#         pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n#         # Obtain positional embeddings for each position.\n#         pos_emb = self.transformer.wpe(pos)\n#         # Obtain token embeddings for the input tokens.\n#         tok_emb = self.transformer.wte(idx)\n#         # Combine token and positional embeddings.\n#         x = tok_emb + pos_emb\n\n#         # Pass through each transformer block.\n#         for block in self.transformer.h:\n#             x = block(x)\n\n#         # Apply the final layer normalization.\n#         x = self.transformer.ln_f(x)\n#         # Compute logits by projecting the transformer output to the vocabulary size.\n#         logits = self.lm_head(x)\n\n#         loss = None\n#         if targets is not None:\n#             # Flatten logits and targets for computing cross-entropy loss.\n#             loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n#         return logits, loss\n\n#     @classmethod\n#     def from_pretrained(cls, model_type):\n#         \"\"\"\n#         Create a SMOLL model instance with weights loaded from a pretrained smollvm2 model.\n\n#         Args:\n#             model_type (str): Must be 'smollvm2-135'.\n\n#         Returns:\n#             model (SMOLL): A SMOLL instance with pretrained weights.\n#         \"\"\"\n#         # Validate that the requested model type is supported.\n#         assert model_type in {'smollvm2-135'}, \"Unsupported model type. Only 'smollvm2-135' is supported.\"\n\n#         # Import the generic pretrained model class from Hugging Face Transformers.\n#         from transformers import AutoModelForCausalLM\n#         print(\"Loading weights from pretrained smollvm2 model: %s\" % model_type)\n\n#         # Define model hyperparameters based on the smollvm2-135 variant.\n#         # (Adjust these values as necessary to match the actual model's configuration.)\n#         config_args = {\n#             'smollvm2-135': dict(n_layer=12, n_head=12, n_embed=768),  # Example: 135M parameter variant.\n#         }[model_type]\n#         # Set constant parameters for the smollvm2 model.\n#         config_args['vocab_size'] = 50257\n#         config_args['block_size'] = 1024\n\n#         # Create a configuration object and initialize a new SMOLL model.\n#         config = SMOLConfig(**config_args)\n#         model = cls(config)\n\n#         # Retrieve the state dictionary (all parameters) of the new model.\n#         sd = model.state_dict()\n#         # Exclude keys that correspond to buffers (e.g., attention bias) that are not actual parameters.\n#         sd_keys = [k for k in sd.keys() if not k.endswith('.attn.bias')]\n\n#         # Load the Hugging Face smollvm2 model with pretrained weights.\n        \n#         model_hf = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=token)\n#         # model_hf = AutoModelForCausalLM.from_pretrained(model_type)\n#         sd_hf = model_hf.state_dict()\n\n#         # Filter out buffer keys from the Hugging Face state dictionary.\n#         sd_keys_hf = [k for k in sd_hf.keys() if not k.endswith('.attn.masked_bias')]\n#         sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n\n#         # List of parameter names that require transposition.\n#         # This is necessary because the original smollvm2 weights may use a Conv1D implementation,\n#         # whereas our implementation uses a standard Linear layer.\n#         transposed = [\n#             'attn.c_attn.weight',\n#             'attn.c_proj.weight',\n#             'mlp.c_fc.weight',\n#             'mlp.c_proj.weight'\n#         ]\n\n#         # Ensure the number of parameters matches between our model and the Hugging Face model.\n#         assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n\n#         # Copy the weights from the Hugging Face model into our model's state dictionary.\n#         for k in sd_keys_hf:\n#             if any(k.endswith(w) for w in transposed):\n#                 # For these weights, verify that transposing the Hugging Face weight gives the correct shape.\n#                 assert sd_hf[k].shape[::-1] == sd[k].shape, f\"Shape mismatch for key {k}\"\n#                 with torch.no_grad():\n#                     sd[k].copy_(sd_hf[k].t())\n#             else:\n#                 assert sd_hf[k].shape == sd[k].shape, f\"Shape mismatch for key {k}\"\n#                 with torch.no_grad():\n#                     sd[k].copy_(sd_hf[k])\n#         return model\n\n# import torch\n# import torch.nn.functional as F\n# from transformers import AutoTokenizer, AutoModelForCausalLM\n\n# # -----------------------------------------------------------------------------\n# # Device Setup\n# # -----------------------------------------------------------------------------\n\n# device = 'cpu'\n# if torch.cuda.is_available():\n#     device = 'cuda'\n# elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n#     device = \"mps\"\n# print(f\"Using device: {device}\")\n\n# # -----------------------------------------------------------------------------\n# # Data Preparation\n# # -----------------------------------------------------------------------------\n\n# # Hyperparameters for training data.\n# B, T = 4, 32  # Batch size and sequence length for the training batch.\n\n# # Define the model name (make sure this model exists on Hugging Face).\n\n# from transformers import AutoTokenizer, AutoModelForCausalLM\n\n# tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n\n# # Load the tokenizer from the model repository.\n# # tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# # Read a sample text file (ensure 'input.txt' exists in your working directory).\n# with open('/kaggle/input/input-text/input.txt', 'r', encoding='utf-8') as f:\n#     text = f.read()\n\n# # Optionally trim the text (here we use the first 1000 characters).\n# text = text[:1000]\n\n# # Tokenize the text.\n# # (Set add_special_tokens=False if you want raw token ids without extra tokens.)\n# tokens = tokenizer.encode(text, add_special_tokens=False)\n\n# # Ensure there are enough tokens to form a training batch (B * T + 1 tokens).\n# if len(tokens) < B * T + 1:\n#     raise ValueError(\"Not enough tokens in input.txt for one batch.\")\n\n# # Create a tensor from tokens and move it to the selected device.\n# buf = torch.tensor(tokens[:B * T + 1], dtype=torch.long, device=device)\n# # x: input tokens, y: target tokens (shifted one position).\n# x = buf[:-1].view(B, T)\n# y = buf[1:].view(B, T)\n\n# # -----------------------------------------------------------------------------\n# # Model Initialization and Training\n# # -----------------------------------------------------------------------------\n\n# # Load the pre-trained smollvm2 model as a causal language model.\n# model = AutoModelForCausalLM.from_pretrained(model_name,use_auth_token=token)\n# model.to(device)\n\n# # Define an optimizer (e.g., Adam).\n# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# # A simple training loop (for demonstration purposes).\n# model.train()\n# num_steps = 500  # Adjust the number of training steps as needed.\n# for step in range(num_steps):\n#     optimizer.zero_grad()\n#     # When providing labels, the model returns a loss.\n#     outputs = model(input_ids=x, labels=y)\n#     loss = outputs.loss\n#     logits = outputs.logits  # (not used further in this simple loop)\n\n#     loss.backward()\n#     optimizer.step()\n\n#     if step % 10 == 0:\n#         print(f\"Step {step}, Loss: {loss.item()}\")\n\n# # -----------------------------------------------------------------------------\n# # Inference: Text Generation\n# # -----------------------------------------------------------------------------\n\n# def generate(model, prompt, max_length=30, temperature=1.0):\n#     \"\"\"\n#     Generate a sequence of tokens from the model given a prompt.\n\n#     Args:\n#         model (AutoModelForCausalLM): The trained smollvm2 model.\n#         prompt (str): The text prompt to start generation.\n#         max_length (int): The number of new tokens to generate.\n#         temperature (float): Temperature parameter for sampling diversity.\n\n#     Returns:\n#         List[int]: List of token IDs representing the generated sequence.\n#     \"\"\"\n#     model.eval()\n#     # Encode the prompt into token IDs and add a batch dimension.\n#     input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n\n#     with torch.no_grad():\n#         for _ in range(max_length):\n#             outputs = model(input_ids=input_ids)\n#             logits = outputs.logits\n\n#             # Focus on the logits of the last token and apply temperature scaling.\n#             next_token_logits = logits[:, -1, :] / temperature\n\n#             # Convert logits to probabilities.\n#             probs = F.softmax(next_token_logits, dim=-1)\n\n#             # Sample the next token (alternatively, you could use greedy decoding with argmax).\n#             next_token = torch.multinomial(probs, num_samples=1)\n\n#             # Append the sampled token to the input_ids.\n#             input_ids = torch.cat([input_ids, next_token], dim=1)\n\n#     # Remove the batch dimension and return the list of token IDs.\n#     return input_ids[0].tolist()\n\n# # Generate several sequences from a given prompt.\n# num_return_sequences = 5\n# max_gen_length = 120\n# prompt = \"Once upon a time\"\n\n# generated_sequences = []\n# for i in range(num_return_sequences):\n#     seq_ids = generate(model, prompt, max_length=max_gen_length, temperature=1.0)\n#     # Decode the token IDs back to text.\n#     generated_text = tokenizer.decode(seq_ids, skip_special_tokens=True)\n#     generated_sequences.append(generated_text)\n\n# print(\"\\nGenerated sequences:\")\n# for idx, seq in enumerate(generated_sequences, start=1):\n#     print(f\"\\nSequence {idx}:\\n{seq}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmodel_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T05:03:51.266358Z","iopub.execute_input":"2025-02-13T05:03:51.266547Z","iopub.status.idle":"2025-02-13T05:03:51.270103Z","shell.execute_reply.started":"2025-02-13T05:03:51.266528Z","shell.execute_reply":"2025-02-13T05:03:51.269197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n# from torch.utils.data import Dataset, DataLoader\n# from dataclasses import dataclass\n# import math\n# import tiktoken\n# from transformers import AutoTokenizer, AutoModelForCausalLM\n\n# # -----------------------------------------------------------------------------\n# # TECHNIQUE: Set matmul precision to 'high'\n# # -----------------------------------------------------------------------------\n# torch.set_float32_matmul_precision('high')  # TECHNIQUE: Ensuring high-precision matmul operations\n\n# # -----------------------------------------------------------------------------\n# # Causal Self-Attention Module with Flash Attention\n# # -----------------------------------------------------------------------------\n# class CasualSelfAttention(nn.Module):\n#     def __init__(self, config):\n#         super().__init__()\n#         self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed)  # TECHNIQUE: Q, K, V projection\n#         self.c_proj = nn.Linear(config.n_embed, config.n_embed)       # TECHNIQUE: Projection after attention\n#         self.n_head = config.n_head\n#         self.n_embed = config.n_embed\n#         self.register_buffer(\n#             \"bias\",\n#             torch.tril(torch.ones(config.block_size, config.block_size))\n#             .view(1, 1, config.block_size, config.block_size)\n#         )  # TECHNIQUE: Causal mask buffer\n#     def forward(self, x):\n#         B, T, C = x.size()\n#         qkv = self.c_attn(x)\n#         q, k, v = qkv.split(self.n_embed, dim=2)\n#         # TECHNIQUE: Reshape for multi-head attention\n#         q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n#         k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n#         v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n#         # TECHNIQUE: Use Flash Attention via scaled_dot_product_attention\n#         y = torch.nn.functional.scaled_dot_product_attention(\n#             q, k, v, attn_mask=None, dropout_p=0.0, is_causal=True\n#         )\n#         # TECHNIQUE: Recombine attention heads\n#         y = y.transpose(1, 2).contiguous().view(B, T, C)\n#         y = self.c_proj(y)\n#         return y\n\n# # -----------------------------------------------------------------------------\n# # Multi-Layer Perceptron (MLP)\n# # -----------------------------------------------------------------------------\n# class MLP(nn.Module):\n#     def __init__(self, config):\n#         super().__init__()\n#         self.c_fc = nn.Linear(config.n_embed, 4 * config.n_embed)  # TECHNIQUE: Expand dimensions 4x\n#         self.gelu = nn.GELU(approximate=\"tanh\")                     # TECHNIQUE: GELU activation\n#         self.c_proj = nn.Linear(4 * config.n_embed, config.n_embed)  # TECHNIQUE: Project back to embedding size\n#         nn.init.constant_(self.c_proj.weight, 1)                    # TECHNIQUE: Constant initialization\n#     def forward(self, x):\n#         x = self.c_fc(x)\n#         x = self.gelu(x)\n#         x = self.c_proj(x)\n#         return x\n\n# # -----------------------------------------------------------------------------\n# # Transformer Block with Residual Scaling\n# # -----------------------------------------------------------------------------\n# class Block(nn.Module):\n#     def __init__(self, config):\n#         super().__init__()\n#         self.ln_1 = nn.LayerNorm(config.n_embed)\n#         self.attn = CasualSelfAttention(config)\n#         self.ln_2 = nn.LayerNorm(config.n_embed)\n#         self.mlp = MLP(config)\n#         self.res_scale = 1.0 / math.sqrt(2)  # TECHNIQUE: Scale residual outputs by 1/√2\n#     def forward(self, x):\n#         # TECHNIQUE: Apply residual scaling to both attention and MLP outputs\n#         x = x + self.res_scale * self.attn(self.ln_1(x))\n#         x = x + self.res_scale * self.mlp(self.ln_2(x))\n#         return x\n\n# # -----------------------------------------------------------------------------\n# # Configuration Class for SMOLL Model with Power-of-2 Check\n# # -----------------------------------------------------------------------------\n# @dataclass\n# class SMOLConfig:\n#     block_size: int = 1024   # Maximum context length.\n#     vocab_size: int = 49152  # Updated TECHNIQUE: Set vocabulary size as per checkpoint.\n#     n_layer: int = 12        # Number of transformer layers.\n#     n_head: int = 9          # Updated TECHNIQUE: Set number of heads so that head_dim = 576/9 = 64.\n#     n_embed: int = 576       # Updated TECHNIQUE: Set embedding dimension as per checkpoint.\n#     def __post_init__(self):\n#         head_dim = self.n_embed // self.n_head\n#         if head_dim & (head_dim - 1) != 0:\n#             raise ValueError(\"Head dimension (n_embed/n_head) is not a power of 2!\")  # TECHNIQUE: Enforce per-head dimension as a power of 2\n\n# # -----------------------------------------------------------------------------\n# # SMOLL Model Definition with Weight Sharing\n# # -----------------------------------------------------------------------------\n# class SMOLL(nn.Module):\n#     def __init__(self, config):\n#         super().__init__()\n#         self.config = config\n#         self.transformer = nn.ModuleDict({\n#             'wte': nn.Embedding(config.vocab_size, config.n_embed),  # TECHNIQUE: Token embeddings\n#             'wpe': nn.Embedding(config.block_size, config.n_embed),  # TECHNIQUE: Positional embeddings\n#             'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n#             'ln_f': nn.LayerNorm(config.n_embed),\n#         })\n#         self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)  # TECHNIQUE: Output projection layer\n#         self.lm_head.weight = self.transformer['wte'].weight  # TECHNIQUE: Tie LM head weights with token embeddings\n#     def forward(self, idx, targets=None):\n#         B, T = idx.size()\n#         assert T <= self.config.block_size, (\n#             f\"Sequence length {T} exceeds block size {self.config.block_size}\"\n#         )\n#         pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n#         pos_emb = self.transformer['wpe'](pos)\n#         tok_emb = self.transformer['wte'](idx)\n#         x = tok_emb + pos_emb\n#         for block in self.transformer['h']:\n#             x = block(x)\n#         x = self.transformer['ln_f'](x)\n#         logits = self.lm_head(x)\n#         loss = None\n#         if targets is not None:\n#             loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n#         return logits, loss\n#     @classmethod\n#     def from_pretrained(cls, model_type):\n#         \"\"\"\n#         Loads a SMOLL model instance using a pretrained HF model.\n#         This version loads the state_dict with strict=False.\n#         \"\"\"\n#         assert model_type in {'smollvm2-135'}, \"Unsupported model type. Only 'smollvm2-135' is supported.\"\n#         from transformers import AutoModelForCausalLM\n#         print(\"Loading weights from pretrained smollvm2 model: %s\" % model_type)\n#         # Updated configuration to match checkpoint dimensions.\n#         config_args = {\n#             'smollvm2-135': dict(n_layer=12, n_head=9, n_embed=576),\n#         }[model_type]\n#         config_args['vocab_size'] = 49152\n#         config_args['block_size'] = 1024\n#         config = SMOLConfig(**config_args)\n#         model = cls(config)\n#         hf_model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=token)\n#         # TECHNIQUE: Load HF weights with strict=False to allow mismatched/missing keys.\n#         model.load_state_dict(hf_model.state_dict(), strict=False)\n#         return model\n\n# # -----------------------------------------------------------------------------\n# # Device Setup\n# # -----------------------------------------------------------------------------\n# device = 'cpu'\n# if torch.cuda.is_available():\n#     device = 'cuda'\n# elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n#     device = \"mps\"\n# print(f\"Using device: {device}\")\n\n# # -----------------------------------------------------------------------------\n# # Data Preparation and Dataloader Creation\n# # -----------------------------------------------------------------------------\n# B, T = 4, 32  # Batch size and sequence length.\n# tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)  # TECHNIQUE: Initialize tokenizer\n# with open('/kaggle/input/input-text/input.txt', 'r', encoding='utf-8') as f:\n#     text = f.read()\n# text = text[:1000]  # TECHNIQUE: Optionally trim the text for demonstration\n# tokens = tokenizer.encode(text, add_special_tokens=False)\n# if len(tokens) < T + 1:\n#     raise ValueError(\"Not enough tokens in input.txt for one training sample.\")\n\n# # -----------------------------------------------------------------------------\n# # TECHNIQUE: Full Dataloader and Profiling Setup\n# # -----------------------------------------------------------------------------\n# class TextDataset(Dataset):\n#     def __init__(self, tokens, block_size):\n#         self.tokens = tokens\n#         self.block_size = block_size\n#     def __len__(self):\n#         return len(self.tokens) - self.block_size\n#     def __getitem__(self, idx):\n#         x = torch.tensor(self.tokens[idx:idx+self.block_size], dtype=torch.long)\n#         y = torch.tensor(self.tokens[idx+1:idx+self.block_size+1], dtype=torch.long)\n#         return x, y\n\n# dataset = TextDataset(tokens, T)\n# dataloader = DataLoader(dataset, batch_size=B, shuffle=True)  # TECHNIQUE: Create DataLoader for batch loading\n\n# # -----------------------------------------------------------------------------\n# # Model Initialization and Training Loop with Autocast and Profiling\n# # -----------------------------------------------------------------------------\n# model = SMOLL.from_pretrained(\"smollvm2-135\")\n# model.to(device)\n# # Note: torch.compile has been removed to avoid backend issues.\n\n# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# # TECHNIQUE: Setup PyTorch profiler to capture performance metrics and trace information\n# with torch.profiler.profile(\n#     schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n#     on_trace_ready=torch.profiler.tensorboard_trace_handler('./log'),\n#     record_shapes=True,\n#     profile_memory=True,\n#     with_stack=True\n# ) as prof:\n#     model.train()\n#     step = 0\n#     num_steps = 8000  # Total training iterations\n#     for epoch in range(100):  # Loop over epochs\n#         for x_batch, y_batch in dataloader:\n#             x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n#             optimizer.zero_grad()\n#             # TECHNIQUE: Autocast for mixed-precision training (works on CUDA; on CPU it is a no-op)\n#             with torch.autocast(device_type='cuda' if device=='cuda' else 'cpu', dtype=torch.float16):\n#                 logits, loss = model(x_batch, targets=y_batch)\n#             loss.backward()\n#             optimizer.step()\n#             step += 1\n#             prof.step()  # TECHNIQUE: Step the profiler after each iteration\n#             if step % 10 == 0:\n#                 print(f\"Step {step}, Loss: {loss.item()}\")\n#             if step >= num_steps:\n#                 break\n#         if step >= num_steps:\n#             break\n\n# # -----------------------------------------------------------------------------\n# # Inference: Text Generation\n# # -----------------------------------------------------------------------------\n# def generate(model, prompt, max_length=100, temperature=1.0):\n#     model.eval()\n#     input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n#     with torch.no_grad():\n#         for _ in range(max_length):\n#             logits, _ = model(input_ids)\n#             next_token_logits = logits[:, -1, :] / temperature\n#             probs = F.softmax(next_token_logits, dim=-1)\n#             next_token = torch.multinomial(probs, num_samples=1)\n#             input_ids = torch.cat([input_ids, next_token], dim=1)\n#     return input_ids[0].tolist()\n\n# num_return_sequences = 5\n# max_gen_length = 100\n# prompt = \"Once upon a time\"\n# generated_sequences = []\n# for i in range(num_return_sequences):\n#     seq_ids = generate(model, prompt, max_length=max_gen_length, temperature=1.0)\n#     generated_text = tokenizer.decode(seq_ids, skip_special_tokens=True)\n#     generated_sequences.append(generated_text)\n\n# print(\"\\nGenerated sequences:\")\n# for idx, seq in enumerate(generated_sequences, start=1):\n#     print(f\"\\nSequence {idx}:\\n{seq}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T05:03:51.462865Z","iopub.execute_input":"2025-02-13T05:03:51.463290Z","iopub.status.idle":"2025-02-13T05:03:51.473932Z","shell.execute_reply.started":"2025-02-13T05:03:51.463256Z","shell.execute_reply":"2025-02-13T05:03:51.473047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom dataclasses import dataclass\nimport math\nimport tiktoken\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Make sure to define these globals before running the code:\n# model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n# token = \"your_hf_access_token_here\"\n\n# -----------------------------------------------------------------------------\n# TECHNIQUE: Set matmul precision to 'high'\n# -----------------------------------------------------------------------------\ntorch.set_float32_matmul_precision('high')  # TECHNIQUE: Ensuring high-precision matmul operations\n\n# -----------------------------------------------------------------------------\n# Causal Self-Attention Module with Flash Attention\n# -----------------------------------------------------------------------------\nclass CasualSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed)  # TECHNIQUE: Q, K, V projection\n        self.c_proj = nn.Linear(config.n_embed, config.n_embed)       # TECHNIQUE: Projection after attention\n        self.n_head = config.n_head\n        self.n_embed = config.n_embed\n        self.register_buffer(\n            \"bias\",\n            torch.tril(torch.ones(config.block_size, config.block_size))\n            .view(1, 1, config.block_size, config.block_size)\n        )  # TECHNIQUE: Causal mask buffer\n    def forward(self, x):\n        B, T, C = x.size()\n        qkv = self.c_attn(x)\n        q, k, v = qkv.split(self.n_embed, dim=2)\n        # TECHNIQUE: Reshape for multi-head attention\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        # TECHNIQUE: Use Flash Attention via scaled_dot_product_attention\n        y = torch.nn.functional.scaled_dot_product_attention(\n            q, k, v, attn_mask=None, dropout_p=0.0, is_causal=True\n        )\n        # TECHNIQUE: Recombine attention heads\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        y = self.c_proj(y)\n        return y\n\n# -----------------------------------------------------------------------------\n# Multi-Layer Perceptron (MLP)\n# -----------------------------------------------------------------------------\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embed, 4 * config.n_embed)  # TECHNIQUE: Expand dimensions 4x\n        self.gelu = nn.GELU(approximate=\"tanh\")                     # TECHNIQUE: GELU activation\n        self.c_proj = nn.Linear(4 * config.n_embed, config.n_embed)  # TECHNIQUE: Project back to embedding size\n        nn.init.constant_(self.c_proj.weight, 1)                    # TECHNIQUE: Constant initialization\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        return x\n\n# -----------------------------------------------------------------------------\n# Transformer Block with Residual Scaling\n# -----------------------------------------------------------------------------\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embed)\n        self.attn = CasualSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embed)\n        self.mlp = MLP(config)\n        self.res_scale = 1.0 / math.sqrt(2)  # TECHNIQUE: Scale residual outputs by 1/√2\n    def forward(self, x):\n        # TECHNIQUE: Apply residual scaling to both attention and MLP outputs\n        x = x + self.res_scale * self.attn(self.ln_1(x))\n        x = x + self.res_scale * self.mlp(self.ln_2(x))\n        return x\n\n# -----------------------------------------------------------------------------\n# Configuration Class for SMOLL Model with Power-of-2 Check\n# -----------------------------------------------------------------------------\n@dataclass\nclass SMOLConfig:\n    block_size: int = 1024   # Maximum context length.\n    vocab_size: int = 49152  # Updated TECHNIQUE: Set vocabulary size as per checkpoint.\n    n_layer: int = 12        # Number of transformer layers.\n    n_head: int = 9          # Updated TECHNIQUE: Set number of heads so that head_dim = 576/9 = 64.\n    n_embed: int = 576       # Updated TECHNIQUE: Set embedding dimension as per checkpoint.\n    def __post_init__(self):\n        head_dim = self.n_embed // self.n_head\n        if head_dim & (head_dim - 1) != 0:\n            raise ValueError(\"Head dimension (n_embed/n_head) is not a power of 2!\")  # TECHNIQUE: Enforce per-head dimension as a power of 2\n\n# -----------------------------------------------------------------------------\n# SMOLL Model Definition with Weight Sharing\n# -----------------------------------------------------------------------------\nclass SMOLL(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.transformer = nn.ModuleDict({\n            'wte': nn.Embedding(config.vocab_size, config.n_embed),  # TECHNIQUE: Token embeddings\n            'wpe': nn.Embedding(config.block_size, config.n_embed),  # TECHNIQUE: Positional embeddings\n            'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            'ln_f': nn.LayerNorm(config.n_embed),\n        })\n        self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)  # TECHNIQUE: Output projection layer\n        self.lm_head.weight = self.transformer['wte'].weight  # TECHNIQUE: Tie LM head weights with token embeddings\n    def forward(self, idx, targets=None):\n        B, T = idx.size()\n        assert T <= self.config.block_size, (\n            f\"Sequence length {T} exceeds block size {self.config.block_size}\"\n        )\n        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n        pos_emb = self.transformer['wpe'](pos)\n        tok_emb = self.transformer['wte'](idx)\n        x = tok_emb + pos_emb\n        for block in self.transformer['h']:\n            x = block(x)\n        x = self.transformer['ln_f'](x)\n        logits = self.lm_head(x)\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n        return logits, loss\n    @classmethod\n    def from_pretrained(cls, model_type):\n        \"\"\"\n        Loads a SMOLL model instance using a pretrained HF model.\n        This version loads the state_dict with strict=False.\n        \"\"\"\n        assert model_type in {'smollvm2-135'}, \"Unsupported model type. Only 'smollvm2-135' is supported.\"\n        from transformers import AutoModelForCausalLM\n        print(\"Loading weights from pretrained smollvm2 model: %s\" % model_type)\n        # Updated configuration to match checkpoint dimensions.\n        config_args = {\n            'smollvm2-135': dict(n_layer=12, n_head=9, n_embed=576),\n        }[model_type]\n        config_args['vocab_size'] = 49152\n        config_args['block_size'] = 1024\n        config = SMOLConfig(**config_args)\n        model = cls(config)\n        hf_model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=token)\n        # TECHNIQUE: Load HF weights with strict=False to allow mismatched/missing keys.\n        model.load_state_dict(hf_model.state_dict(), strict=False)\n        return model\n\n# -----------------------------------------------------------------------------\n# Device Setup\n# -----------------------------------------------------------------------------\ndevice = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'\nelif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n    device = \"mps\"\nprint(f\"Using device: {device}\")\n\n# -----------------------------------------------------------------------------\n# Data Preparation and Dataloader Creation\n# -----------------------------------------------------------------------------\nB, T = 8, 256  # Batch size and sequence length.\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)  # TECHNIQUE: Initialize tokenizer\nwith open('/kaggle/input/input-text/input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\ntext = text[:1000]  # TECHNIQUE: Optionally trim the text for demonstration\ntokens = tokenizer.encode(text, add_special_tokens=False)\nif len(tokens) < T + 1:\n    raise ValueError(\"Not enough tokens in input.txt for one training sample.\")\n\n# -----------------------------------------------------------------------------\n# TECHNIQUE: Full Dataloader and Profiling Setup\n# -----------------------------------------------------------------------------\nclass TextDataset(Dataset):\n    def __init__(self, tokens, block_size):\n        self.tokens = tokens\n        self.block_size = block_size\n    def __len__(self):\n        return len(self.tokens) - self.block_size\n    def __getitem__(self, idx):\n        x = torch.tensor(self.tokens[idx:idx+self.block_size], dtype=torch.long)\n        y = torch.tensor(self.tokens[idx+1:idx+self.block_size+1], dtype=torch.long)\n        return x, y\n\ndataset = TextDataset(tokens, T)\ndataloader = DataLoader(dataset, batch_size=B, shuffle=True)  # TECHNIQUE: Create DataLoader for batch loading\n\n# -----------------------------------------------------------------------------\n# Model Initialization and Training Loop with Autocast and Profiling\n# -----------------------------------------------------------------------------\nmodel = SMOLL.from_pretrained(\"smollvm2-135\")\nmodel.to(device)\n# Note: torch.compile has been removed to avoid backend issues.\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# ---------------------\n# First Training Stage: 5000 Steps with Inference Every 500 Steps\n# ---------------------\nnum_steps = 8400\nstep = 0\nwith torch.profiler.profile(\n    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n    on_trace_ready=torch.profiler.tensorboard_trace_handler('./log'),\n    record_shapes=True,\n    profile_memory=True,\n    with_stack=True\n) as prof:\n    model.train()\n    for epoch in range(50):  # Epoch loop; will break once 5000 steps are reached.\n        for x_batch, y_batch in dataloader:\n            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n            optimizer.zero_grad()\n            # TECHNIQUE: Autocast for mixed-precision training (works on CUDA; on CPU it is a no-op)\n            with torch.autocast(device_type='cuda' if device=='cuda' else 'cpu', dtype=torch.float16):\n                logits, loss = model(x_batch, targets=y_batch)\n            loss.backward()\n            optimizer.step()\n            step += 1\n            prof.step()  # TECHNIQUE: Step the profiler after each iteration\n            # Every 500 steps, generate text from a fixed prompt and print it.\n            # Log every 500 steps\n            if step % 500 == 0:\n                print(f\"*** Completed step {step} ***\")\n                sample_ids = generate(model, \"Once upon a time\", max_length=200, temperature=1)\n                print(f\"Step {step}, Sample: {tokenizer.decode(sample_ids, skip_special_tokens=True)}\")\n            if step >= num_steps:\n                break\n        if step >= num_steps:\n            break\n\n# Save a checkpoint after 5000 steps.\ntorch.save(model.state_dict(), \"/kaggle/working/smoll_checkpoint.pt\")\nprint(\"Checkpoint saved after 5000 training steps.\")\n\n# ---------------------\n# Second Training Stage: Load Checkpoint and Train for 50 More Steps\n# ---------------------\nmodel.load_state_dict(torch.load(\"/kaggle/working/smoll_checkpoint.pt\"))\nmodel.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Reinitialize optimizer if desired\n\nnum_steps_extra = 50\nstep_extra = 0\nmodel.train()\nfor epoch in range(100):  # This loop will break after 50 steps.\n    for x_batch, y_batch in dataloader:\n        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n        optimizer.zero_grad()\n        with torch.autocast(device_type='cuda' if device=='cuda' else 'cpu', dtype=torch.float16):\n            logits, loss = model(x_batch, targets=y_batch)\n        loss.backward()\n        optimizer.step()\n        step_extra += 1\n        if step_extra % 10 == 0:\n            print(f\"Extra Training Step {step_extra}, Loss: {loss.item()}\")\n        if step_extra >= num_steps_extra:\n            break\n    if step_extra >= num_steps_extra:\n        break\n\nprint(\"Additional 50 training steps completed.\")\n\n# -----------------------------------------------------------------------------\n# Final Inference: Text Generation\n# -----------------------------------------------------------------------------\ndef generate(model, prompt, max_length=200, temperature=1.0):\n    model.eval()\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        for _ in range(max_length):\n            logits, _ = model(input_ids)\n            next_token_logits = logits[:, -1, :] / temperature\n            probs = F.softmax(next_token_logits, dim=-1)\n            next_token = torch.multinomial(probs, num_samples=1)\n            input_ids = torch.cat([input_ids, next_token], dim=1)\n    return input_ids[0].tolist()\n\nfinal_sample_ids = generate(model, \"Once upon a time\", max_length=200, temperature=1.0)\nprint(\"Final generated text:\", tokenizer.decode(final_sample_ids, skip_special_tokens=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T05:04:16.521396Z","iopub.execute_input":"2025-02-13T05:04:16.521719Z","iopub.status.idle":"2025-02-13T05:05:27.164819Z","shell.execute_reply.started":"2025-02-13T05:04:16.521691Z","shell.execute_reply":"2025-02-13T05:05:27.164062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom dataclasses import dataclass\nimport math\nimport tiktoken\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer\n\n# ============================================================================\n# GLOBAL VARIABLES (define these before running)\n# ============================================================================\n\n# ============================================================================\n# TECHNIQUE: Set matmul precision to 'high'\n# ============================================================================\ntorch.set_float32_matmul_precision('high')  # TECHNIQUE: Ensuring high-precision matmul operations\n\n# ============================================================================\n# Model Components (same as before)\n# ============================================================================\n\n# -----------------------------------------------------------------------------\n# Causal Self-Attention Module with Flash Attention\n# -----------------------------------------------------------------------------\nclass CasualSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed)  # TECHNIQUE: Q, K, V projection\n        self.c_proj = nn.Linear(config.n_embed, config.n_embed)       # TECHNIQUE: Projection after attention\n        self.n_head = config.n_head\n        self.n_embed = config.n_embed\n        self.register_buffer(\n            \"bias\",\n            torch.tril(torch.ones(config.block_size, config.block_size))\n            .view(1, 1, config.block_size, config.block_size)\n        )  # TECHNIQUE: Causal mask buffer\n    def forward(self, x):\n        B, T, C = x.size()\n        qkv = self.c_attn(x)\n        q, k, v = qkv.split(self.n_embed, dim=2)\n        # TECHNIQUE: Reshape for multi-head attention\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        # TECHNIQUE: Use Flash Attention via scaled_dot_product_attention\n        y = torch.nn.functional.scaled_dot_product_attention(\n            q, k, v, attn_mask=None, dropout_p=0.0, is_causal=True\n        )\n        # TECHNIQUE: Recombine attention heads\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        y = self.c_proj(y)\n        return y\n\n# -----------------------------------------------------------------------------\n# Multi-Layer Perceptron (MLP)\n# -----------------------------------------------------------------------------\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embed, 4 * config.n_embed)  # TECHNIQUE: Expand dimensions 4x\n        self.gelu = nn.GELU(approximate=\"tanh\")                     # TECHNIQUE: GELU activation\n        self.c_proj = nn.Linear(4 * config.n_embed, config.n_embed)  # TECHNIQUE: Project back to embedding size\n        nn.init.constant_(self.c_proj.weight, 1)                    # TECHNIQUE: Constant initialization\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        return x\n\n# -----------------------------------------------------------------------------\n# Transformer Block with Residual Scaling\n# -----------------------------------------------------------------------------\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embed)\n        self.attn = CasualSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embed)\n        self.mlp = MLP(config)\n        self.res_scale = 1.0 / math.sqrt(2)  # TECHNIQUE: Scale residual outputs by 1/√2\n    def forward(self, x):\n        # TECHNIQUE: Apply residual scaling to both attention and MLP outputs\n        x = x + self.res_scale * self.attn(self.ln_1(x))\n        x = x + self.res_scale * self.mlp(self.ln_2(x))\n        return x\n\n# -----------------------------------------------------------------------------\n# Configuration Class for SMOLL Model with Power-of-2 Check\n# -----------------------------------------------------------------------------\n@dataclass\nclass SMOLConfig:\n    block_size: int = 1024   # Maximum context length.\n    vocab_size: int = 49152  # Updated TECHNIQUE: Set vocabulary size as per checkpoint.\n    n_layer: int = 12        # Number of transformer layers.\n    n_head: int = 9          # Updated TECHNIQUE: Set number of heads so that head_dim = 576/9 = 64.\n    n_embed: int = 576       # Updated TECHNIQUE: Set embedding dimension as per checkpoint.\n    def __post_init__(self):\n        head_dim = self.n_embed // self.n_head\n        if head_dim & (head_dim - 1) != 0:\n            raise ValueError(\"Head dimension (n_embed/n_head) is not a power of 2!\")  # TECHNIQUE: Enforce per-head dimension as a power of 2\n\n# -----------------------------------------------------------------------------\n# SMOLL Model Definition with Weight Sharing\n# -----------------------------------------------------------------------------\nclass SMOLL(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.transformer = nn.ModuleDict({\n            'wte': nn.Embedding(config.vocab_size, config.n_embed),  # TECHNIQUE: Token embeddings\n            'wpe': nn.Embedding(config.block_size, config.n_embed),  # TECHNIQUE: Positional embeddings\n            'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            'ln_f': nn.LayerNorm(config.n_embed),\n        })\n        self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)  # TECHNIQUE: Output projection layer\n        self.lm_head.weight = self.transformer['wte'].weight  # TECHNIQUE: Tie LM head weights with token embeddings\n    def forward(self, idx, targets=None):\n        B, T = idx.size()\n        assert T <= self.config.block_size, (\n            f\"Sequence length {T} exceeds block size {self.config.block_size}\"\n        )\n        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n        pos_emb = self.transformer['wpe'](pos)\n        tok_emb = self.transformer['wte'](idx)\n        x = tok_emb + pos_emb\n        for block in self.transformer['h']:\n            x = block(x)\n        x = self.transformer['ln_f'](x)\n        logits = self.lm_head(x)\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n        return logits, loss\n    @classmethod\n    def from_pretrained(cls, model_type):\n        \"\"\"\n        Loads a SMOLL model instance using a pretrained HF model.\n        This version loads the state_dict with strict=False.\n        \"\"\"\n        assert model_type in {'smollvm2-135'}, \"Unsupported model type. Only 'smollvm2-135' is supported.\"\n        from transformers import AutoModelForCausalLM\n        print(\"Loading weights from pretrained smollvm2 model: %s\" % model_type)\n        # Updated configuration to match checkpoint dimensions.\n        config_args = {\n            'smollvm2-135': dict(n_layer=12, n_head=9, n_embed=576),\n        }[model_type]\n        config_args['vocab_size'] = 49152\n        config_args['block_size'] = 1024\n        config = SMOLConfig(**config_args)\n        model = cls(config)\n        hf_model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=token)\n        # TECHNIQUE: Load HF weights with strict=False to allow mismatched/missing keys.\n        model.load_state_dict(hf_model.state_dict(), strict=False)\n        return model\n\n# ============================================================================\n# PyTorch Lightning Module Definition\n# ============================================================================\nclass SMOLLLightningModule(pl.LightningModule):\n    def __init__(self, model_type: str = \"smollvm2-135\"):\n        \"\"\"\n        The model_type parameter now has a default value so that load_from_checkpoint works without errors.\n        \"\"\"\n        super().__init__()\n        self.model_type = model_type\n        self.model = SMOLL.from_pretrained(model_type)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n    def forward(self, x, targets=None):\n        return self.model(x, targets)\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits, loss = self.model(x, targets=y)\n        self.log(\"train_loss\", loss, on_step=True, prog_bar=True)\n        # Log sample every 500 steps\n        if self.global_step % 500 == 0:\n            sample_ids = self.generate(\"Once upon a time\", max_length=100, temperature=1.0)\n            sample_text = self.tokenizer.decode(sample_ids, skip_special_tokens=True)\n            print(f\"\\n\\nStep {self.global_step}, \\nSample: {sample_text}\")\n        return loss\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.model.parameters(), lr=1e-3)\n    def generate(self, prompt, max_length=100, temperature=1.0):\n        self.model.eval()\n        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n        with torch.no_grad():\n            for _ in range(max_length):\n                logits, _ = self.model(input_ids)\n                next_token_logits = logits[:, -1, :] / temperature\n                probs = F.softmax(next_token_logits, dim=-1)\n                next_token = torch.multinomial(probs, num_samples=1)\n                input_ids = torch.cat([input_ids, next_token], dim=1)\n        return input_ids[0].tolist()\n\n# ============================================================================\n# Data Preparation and DataLoader\n# ============================================================================\nclass TextDataset(Dataset):\n    def __init__(self, tokens, block_size):\n        self.tokens = tokens\n        self.block_size = block_size\n    def __len__(self):\n        return len(self.tokens) - self.block_size\n    def __getitem__(self, idx):\n        x = torch.tensor(self.tokens[idx:idx+self.block_size], dtype=torch.long)\n        y = torch.tensor(self.tokens[idx+1:idx+self.block_size+1], dtype=torch.long)\n        return x, y\n\nB, T = 8, 128  # Batch size and sequence length.\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)  # TECHNIQUE: Initialize tokenizer\nwith open('/kaggle/input/input-text/input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\ntext = text[:1000]  # TECHNIQUE: Optionally trim the text for demonstration\ntokens = tokenizer.encode(text, add_special_tokens=False)\nif len(tokens) < T + 1:\n    raise ValueError(\"Not enough tokens in input.txt for one training sample.\")\ndataset = TextDataset(tokens, T)\ndataloader = DataLoader(dataset, batch_size=B, shuffle=True)  # TECHNIQUE: Create DataLoader for batch loading\n\n# ============================================================================\n# Training Stage 1: Train for 5000 Steps with Logging Every 500 Steps\n# ============================================================================\nlit_model = SMOLLLightningModule()  # Uses default model_type \"smollvm2-135\"\ntrainer = Trainer(\n    max_steps=5000,\n    log_every_n_steps=50,\n    accelerator=\"auto\",\n    devices=1\n)\ntrainer.fit(lit_model, train_dataloaders=dataloader)\ntrainer.save_checkpoint(\"pl_smoll_checkpoint.ckpt\")\nprint(\"Checkpoint saved after 5000 training steps.\")\n\n# ============================================================================\n# Training Stage 2: Load Checkpoint and Train for 50 More Steps\n# ============================================================================\nlit_model_extra = SMOLLLightningModule.load_from_checkpoint(\"pl_smoll_checkpoint.ckpt\")\ntrainer_extra = Trainer(\n    max_steps=50,\n    log_every_n_steps=10,\n    accelerator=\"auto\",\n    devices=1\n)\ntrainer_extra.fit(lit_model_extra, train_dataloaders=dataloader)\nprint(\"Additional 50 training steps completed.\")\n\n# ============================================================================\n# Final Inference: Text Generation\n# ============================================================================\nfinal_sample_ids = lit_model_extra.generate(\"Once upon a time\", max_length=100, temperature=1.0)\nfinal_text = tokenizer.decode(final_sample_ids, skip_special_tokens=True)\nprint(\"Final generated text:\", final_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T05:08:51.792566Z","iopub.execute_input":"2025-02-13T05:08:51.793260Z","execution_failed":"2025-02-13T05:38:23.175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/output.zip /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T10:15:15.294406Z","iopub.execute_input":"2025-02-13T10:15:15.294689Z","iopub.status.idle":"2025-02-13T10:22:02.389618Z","shell.execute_reply.started":"2025-02-13T10:15:15.294670Z","shell.execute_reply":"2025-02-13T10:22:02.388823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'/kaggle/working/output.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T10:31:20.493958Z","iopub.execute_input":"2025-02-13T10:31:20.494320Z","iopub.status.idle":"2025-02-13T10:31:20.502595Z","shell.execute_reply.started":"2025-02-13T10:31:20.494291Z","shell.execute_reply":"2025-02-13T10:31:20.501720Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}