{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10822221,"sourceType":"datasetVersion","datasetId":6719475},{"sourceId":40994862,"sourceType":"kernelVersion"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\ntoken = \"\"\n\n\n# Saving tokenizer files for hugging face app\nimport shutil\nimport os\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Paths for saving tokenizer and model\nsave_dir = \"/kaggle/working/smollm_model\"\n\n# Ensure the directory exists|\nos.makedirs(save_dir, exist_ok=True)\n\n# Load tokenizer and model (replace with your model name or path)\n\ntokenizer_saved = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\nmodel_saved = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=token)\n\n# Save tokenizer and model to working directory\ntokenizer_saved.save_pretrained(save_dir)\nmodel_saved.save_pretrained(save_dir)\n\nprint(f\">>> Model and tokenizer saved to {save_dir}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-22T13:46:32.505499Z","iopub.execute_input":"2025-02-22T13:46:32.505764Z","iopub.status.idle":"2025-02-22T13:47:01.583583Z","shell.execute_reply.started":"2025-02-22T13:46:32.505714Z","shell.execute_reply":"2025-02-22T13:47:01.582639Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom dataclasses import dataclass\nimport math\nimport tiktoken\nimport time\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import LambdaLR\n\n\n# ============================================================================\n# TECHNIQUE: Set matmul precision for performance\n# ============================================================================\ntorch.set_float32_matmul_precision('high')\n\n# ============================================================================\n# Configuration Class for SmolLM2 Model\n# ============================================================================\n@ dataclass\nclass SMOLConfig:\n    block_size: int = 1024\n    vocab_size: int = 49152\n    n_layer: int = 30\n    n_head: int = 9\n    n_embed: int = 576\n    rope_theta: float = 10000.0\n    rms_norm_eps: float = 1.0e-5\n    intermediate_size: int = 1536\n\n    def __post_init__(self):\n        # Ensure head dimension is a power of 2\n        head_dim = self.n_embed // self.n_head\n        if head_dim & (head_dim - 1) != 0:\n            raise ValueError(\"Head dimension must be a power of 2!\")\n\n\n# ============================================================================\n# RMSNorm: Root Mean Square Normalization (Better than LayerNorm)\n# ============================================================================\nclass RMSNorm(nn.Module):\n    def __init__(self, dim, eps=1e-8):\n        super().__init__()\n        self.eps = eps\n        self.scale = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x):\n        norm = x.norm(2, dim=-1, keepdim=True)\n        return x * (self.scale / (norm + self.eps))\n\n\n# ============================================================================\n# RoPE (Rotary Positional Embedding) with RoPE-Theta\n# ============================================================================\nclass RotaryEmbedding(nn.Module):\n    def __init__(self, dim, base_theta=10000):\n        super().__init__()\n        self.dim = dim\n        self.base_theta = base_theta\n\n        # Precompute RoPE frequencies\n        inv_freq = 1.0 / (base_theta ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n    def forward(self, seq_len, device):\n        t = torch.arange(seq_len, device=device).float()\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        cos = torch.cos(freqs).unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, dim/2)\n        sin = torch.sin(freqs).unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, dim/2)\n        return cos, sin\n\n    def _rotate_half(self, x):\n        \"\"\"Rotate last dimension by half (for even dimensions only).\"\"\"\n        x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]\n        return torch.cat((-x2, x1), dim=-1)\n\n    def apply_rotary(self, q, k):\n        # Get sequence length and device\n        seq_len = q.size(2)  # Ensure seq_len is from axis 2\n        cos, sin = self(seq_len, q.device)\n    \n        # Ensure correct dimension: head_dim = q.shape[-1]\n        head_dim = q.size(-1)\n        dim = head_dim // 2\n    \n        # Split q, k into two halves (first half for RoPE)\n        q1, q2 = q[..., :dim], q[..., dim:]\n        k1, k2 = k[..., :dim], k[..., dim:]\n    \n        # Apply RoPE to the first half of dimensions\n        q_rot = torch.cat([(q1 * cos) + (self._rotate_half(q1) * sin), q2], dim=-1)\n        k_rot = torch.cat([(k1 * cos) + (self._rotate_half(k1) * sin), k2], dim=-1)\n    \n        return q_rot, k_rot\n\n\n\n\n# ============================================================================\n# Causal Self-Attention Module (Flash Attention + RoPE + RMSNorm)\n# ============================================================================\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        self.n_embed = config.n_embed\n        self.head_dim = config.n_embed // config.n_head\n\n        # Linear projections for Q, K, V and output\n        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed)\n        self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n\n        # Rotary Positional Embedding (RoPE) with custom theta\n        self.rope = RotaryEmbedding(self.head_dim, base_theta=config.rope_theta)\n\n        # RMSNorm for pre-normalization\n        self.norm = RMSNorm(config.n_embed, eps=config.rms_norm_eps)\n\n    def forward(self, x):\n        B, T, C = x.size()\n\n        # Apply RMSNorm before attention\n        x = self.norm(x)\n\n        # Compute Q, K, V projections\n        qkv = self.c_attn(x).split(self.n_embed, dim=2)\n        q, k, v = [qkv[i].view(B, T, self.n_head, self.head_dim).transpose(1, 2) for i in range(3)]\n\n        # Apply RoPE (Rotary Positional Embedding) to Q and K\n        q, k = self.rope.apply_rotary(q, k)\n\n        # Flash Attention with causal masking\n        y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=True)\n\n        # Merge heads and project output\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        return self.c_proj(y)\n\n\n# ============================================================================\n# Multi-Layer Perceptron (MLP) Block\n# ============================================================================\n# ============================================================================\n# Multi-Layer Perceptron (MLP) Block with RMSNorm and Configurable Size\n# ============================================================================\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        # Use configurable intermediate size for better performance\n        hidden_size = config.intermediate_size if config.intermediate_size else 4 * config.n_embed\n\n        self.c_fc = nn.Linear(config.n_embed, hidden_size)  # Input to hidden\n        self.gelu = nn.GELU(approximate=\"tanh\")  # Faster approximation of GELU\n        self.c_proj = nn.Linear(hidden_size, config.n_embed)  # Hidden to output\n\n        # Adding RMSNorm for better stability\n        self.rms_norm = RMSNorm(config.n_embed, eps=config.rms_norm_eps)\n\n    def forward(self, x):\n        x = self.gelu(self.c_fc(x))  # Feed through FC and activation\n        x = self.c_proj(x)  # Project back to embedding size\n        return self.rms_norm(x)  # Normalize output for better stability\n\n\n# ============================================================================\n# Transformer Block with Residual Scaling, RMSNorm, and RoPE\n# ============================================================================\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.rms_1 = RMSNorm(config.n_embed, eps=config.rms_norm_eps)  # Replaces LayerNorm\n        self.attn = CausalSelfAttention(config)\n\n        self.rms_2 = RMSNorm(config.n_embed, eps=config.rms_norm_eps)  # Replaces LayerNorm\n        self.mlp = MLP(config)\n\n        self.res_scale = 1.0 / math.sqrt(2)  # Residual scaling for stability\n\n    def forward(self, x):\n        # Attention block with RMSNorm and residual scaling\n        x = x + self.res_scale * self.attn(self.rms_1(x))\n\n        # MLP block with RMSNorm and residual scaling\n        x = x + self.res_scale * self.mlp(self.rms_2(x))\n\n        return x\n\n\n# ============================================================================\n# SmolLM Model Definition with Weight Sharing\n# ============================================================================\nclass SMOLL(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.transformer = nn.ModuleDict({\n            'wte': nn.Embedding(config.vocab_size, config.n_embed),\n            'wpe': nn.Embedding(config.block_size, config.n_embed),\n            'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            'ln_f': nn.LayerNorm(config.n_embed, eps=config.rms_norm_eps)})\n        self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)\n        self.lm_head.weight = self.transformer['wte'].weight\n\n    def forward(self, idx, targets=None):\n        B, T = idx.size()\n        assert T <= self.config.block_size, \"Sequence length exceeds block size\"\n\n        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n        x = self.transformer['wte'](idx) + self.transformer['wpe'](pos)\n\n        for block in self.transformer['h']:\n            x = block(x)\n\n        logits = self.lm_head(self.transformer['ln_f'](x))\n\n        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) if targets is not None else None\n        return logits, loss\n\n    @classmethod\n    def from_pretrained(cls, model_type, model_name, token=None):\n        assert model_type == 'SmolLM2-135M', \"Only 'SmolLM2-135M' is supported.\"\n        config = SMOLConfig()\n        model = cls(config)\n\n        hf_model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=token)\n        model.load_state_dict(hf_model.state_dict(), strict=False)\n\n        return model\n\n\n# ============================================================================\n# PyTorch Lightning Module (SMOLL)\n# ============================================================================\nclass SMOLLLightningModule(pl.LightningModule):\n    def __init__(self, model_type=\"SmolLM2-135M\", model_name=None, token=None):\n        super().__init__()\n        self.model = SMOLL.from_pretrained(model_type, model_name, token)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n\n    def forward(self, x, targets=None):\n        return self.model(x, targets)\n\n    def training_step(self, batch, batch_idx):\n        # Step timing\n        step_start = time.time()\n        x, y = batch\n        _, loss = self.model(x, targets=y)\n\n        if self.global_step % 200 == 0:\n            print(\">>> processing step: \", self.global_step)\n\n        # Display generation samples every 500 steps\n        if self.global_step % 500 == 0:\n            sample = self.generate(\"Once upon a time\", max_length=100, temperature=1.0)\n            print(f\"\\n\\n >>> Step {self.global_step}: \\n >>> Generated Sample:\\n{sample}\\n\")\n\n        # Calculate and display step time\n        step_time = time.time() - step_start\n\n        self.log(\"train_loss\", loss, on_step=True, prog_bar=True)\n        return loss\n\n    # def on_train_start(self):\n    #     # Dataset and batch details\n    #     dataset_size = len(self.trainer.datamodule.train_dataloader().dataset)\n    #     batch_size = self.trainer.datamodule.batch_size\n    #     max_steps = self.trainer.max_steps\n\n    #     # Calculate estimated epochs\n    #     total_epochs = (max_steps * batch_size) / dataset_size\n    #     print(f\">>> Training will run for ~{total_epochs:.2f} epochs.\")\n\n    #     # Start time for total training\n    #     self.training_start_time = time.time()\n\n    # def on_train_epoch_start(self):\n    #     # Record epoch start time\n    #     self.epoch_start_time = time.time()\n\n    # def on_train_epoch_end(self):\n    #     # Calculate and display epoch time\n    #     epoch_time = time.time() - self.epoch_start_time\n    #     print(f\"Epoch {self.current_epoch + 1} completed in {epoch_time:.2f} seconds ({epoch_time/60:.2f} minutes)\")\n\n    # def on_train_end(self):\n    #     # Calculate total training time\n    #     total_time = time.time() - self.training_start_time\n    #     print(f\"Training completed in {total_time:.2f} seconds ({total_time/3600:.2f} hours).\")\n    #     print(f\"Total epochs: {self.current_epoch + 1}, Total steps: {self.global_step}\")\n\n    def generate(self, prompt, max_length=100, temperature=1.0):\n        self.model.eval()  # Set model to evaluation mode\n\n        # Encode the prompt and move to the same device as the model\n        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n\n        with torch.no_grad():\n            for _ in range(max_length):\n                logits, _ = self.model(input_ids)\n\n                # Apply temperature scaling to control randomness\n                next_token_logits = logits[:, -1, :] / temperature\n                probs = F.softmax(next_token_logits, dim=-1)\n\n                # Sample the next token\n                next_token = torch.multinomial(probs, num_samples=1)\n\n                # Append the token to the input sequence\n                input_ids = torch.cat([input_ids, next_token], dim=1)\n\n        return self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n\n    def configure_optimizers(self):\n        optimizer = AdamW(self.model.parameters(), lr=3e-3, betas=(0.9, 0.95), eps=1e-8, weight_decay=0.01)\n\n        def lr_lambda(step):\n            warmup_steps = 2000\n            decay_start = 1600000\n            decay_steps = 400000\n            if step < warmup_steps:\n                return step / warmup_steps\n            elif step < decay_start:\n                return 1.0\n            return max(0.0, 1 - (step - decay_start) / decay_steps)\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": LambdaLR(optimizer, lr_lambda),\n                \"interval\": \"step\"\n            }\n        }\n\n\n# ============================================================================\n# Dataset and DataLoader\n# ============================================================================\n\n# Custom Dataset Class\nclass TextDataset(Dataset):\n    def __init__(self, tokens, block_size):\n        self.tokens = tokens\n        self.block_size = block_size\n\n    def __len__(self):\n        return len(self.tokens) - self.block_size\n\n    def __getitem__(self, idx):\n        return (\n            torch.tensor(self.tokens[idx:idx + self.block_size], dtype=torch.long),\n            torch.tensor(self.tokens[idx + 1:idx + self.block_size + 1], dtype=torch.long)\n        )\n\n# ============================================================================\n# Training Configuration\n# ============================================================================\nT = 64\nbatch_size = 8\n\nfile_path = \"/kaggle/input/shakespeare-texts/input.txt\"\n# Read and tokenize the Shakespeare dataset\nwith open(file_path, 'r', encoding='utf-8') as f:\n    text = f.read()\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)  # TECHNIQUE: Initialize tokenizer\n\ntext = text[:1000]  # TECHNIQUE: Optionally trim the text for demonstration\ntokens = tokenizer.encode(text, add_special_tokens=False)\nif len(tokens) < T + 1:\n    raise ValueError(\"Not enough tokens in input.txt for one training sample.\")\n\n# Encode the full text using the tokenizer\ntokens = tokenizer.encode(text, add_special_tokens=False)\n\n# Create Dataset and DataLoader\ndataset = TextDataset(tokens, T)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\nprint(f\"Dataset size: {len(dataset)}, Batch size: {batch_size}\")\n\nlit_model = SMOLLLightningModule(model_name=model_name, token=token)\ntrainer = Trainer(max_steps=2500, log_every_n_steps=50, accelerator=\"auto\", devices=1,\n                  enable_progress_bar=True)  # Ensures live updates)\n\ntrainer.fit(lit_model, dataloader)\ntrainer.save_checkpoint(\"smollm2_135m.ckpt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T13:47:01.585059Z","iopub.execute_input":"2025-02-22T13:47:01.585585Z","iopub.status.idle":"2025-02-22T14:09:04.951759Z","shell.execute_reply.started":"2025-02-22T13:47:01.585562Z","shell.execute_reply":"2025-02-22T14:09:04.950668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install transformers accelerate bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:09:16.603046Z","iopub.execute_input":"2025-02-22T14:09:16.603335Z","iopub.status.idle":"2025-02-22T14:09:23.505612Z","shell.execute_reply.started":"2025-02-22T14:09:16.603313Z","shell.execute_reply":"2025-02-22T14:09:23.504763Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # # # # # # # # #\n# # QUANTIZATION  #\n# # # # # # # # # #\n# import torch\n# from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# checkpoint_path = \"smollm2_135m.ckpt\"\n\n# # Apply 8-bit quantization\n# model = lit_model.to('cuda')  # Ensure the model is on GPU if available\n# model = model.half()      # Use FP16 for better performance\n# quantized_model = model.quantize(bits=8)\n\n# # Save the quantized model\n# quantized_model.save_pretrained(\"smollM2-135M-quantized\")\n# tokenizer.save_pretrained(\"smollM2-135M-quantized\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:10:53.962401Z","iopub.execute_input":"2025-02-22T14:10:53.962786Z","iopub.status.idle":"2025-02-22T14:10:53.966112Z","shell.execute_reply.started":"2025-02-22T14:10:53.962751Z","shell.execute_reply":"2025-02-22T14:10:53.965263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torchinfo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:11:15.324229Z","iopub.execute_input":"2025-02-22T14:11:15.324559Z","iopub.status.idle":"2025-02-22T14:11:18.728961Z","shell.execute_reply.started":"2025-02-22T14:11:15.324533Z","shell.execute_reply":"2025-02-22T14:11:18.727810Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # # # # # # #\n# PARAMETERS #\n# # # # # # ##\n\nif __name__ == \"__main__\":\n    # Initialize model config\n    config = SMOLConfig()\n    model = SMOLL(config)\n\n    # Print model architecture\n    print(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:11:36.573782Z","iopub.execute_input":"2025-02-22T14:11:36.574096Z","iopub.status.idle":"2025-02-22T14:11:37.483840Z","shell.execute_reply.started":"2025-02-22T14:11:36.574062Z","shell.execute_reply":"2025-02-22T14:11:37.482929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # # # # # # #\n# EXTRA STEPS #\n# # # # # # # #\n\nlit_model_extra = SMOLLLightningModule.load_from_checkpoint(\"smollm2_135m.ckpt\",model_name=model_name, token=token)\ntrainer_extra = Trainer(\n    max_steps=50,\n    log_every_n_steps=10,\n    accelerator=\"auto\",\n    devices=1\n)\ntrainer_extra.fit(lit_model_extra, train_dataloaders=dataloader)\nprint(\"Additional 50 training steps completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:11:41.183416Z","iopub.execute_input":"2025-02-22T14:11:41.183755Z","iopub.status.idle":"2025-02-22T14:12:12.075780Z","shell.execute_reply.started":"2025-02-22T14:11:41.183709Z","shell.execute_reply":"2025-02-22T14:12:12.074600Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# Final Inference: Text Generation\n# ============================================================================\ninference_prompt1 = lit_model_extra.generate(\"Once upon a time\", max_length=100, temperature=1.0)\nprint(\"Final generated text:\", inference_prompt1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:12:12.077278Z","iopub.execute_input":"2025-02-22T14:12:12.077646Z","iopub.status.idle":"2025-02-22T14:12:30.279931Z","shell.execute_reply.started":"2025-02-22T14:12:12.077615Z","shell.execute_reply":"2025-02-22T14:12:30.279171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_prompt2 = lit_model_extra.generate(\"Hey, do you know India is a\", max_length=100, temperature=1.0)\nprint(\"Final generated text:\", inference_prompt2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:12:30.281098Z","iopub.execute_input":"2025-02-22T14:12:30.281338Z","iopub.status.idle":"2025-02-22T14:12:47.496176Z","shell.execute_reply.started":"2025-02-22T14:12:30.281316Z","shell.execute_reply":"2025-02-22T14:12:47.495145Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer\nimport pytorch_lightning as pl\nfrom torch.quantization import quantize_dynamic\nfrom pytorch_lightning import Trainer\n\n# Step 1: Load the trained Lightning checkpoint\ncheckpoint_path = \"smollm2_135m.ckpt\"\nsmoll_lightning_model = SMOLLLightningModule.load_from_checkpoint(checkpoint_path, model_name=model_name, token=token)\nprint(\"✅ Loaded SMOLLLightningModule from checkpoint\")\n\n# Step 2: Apply dynamic quantization to the model (focus on Linear layers for efficiency)\nquantized_model = quantize_dynamic(\n    smoll_lightning_model.model,  # Target model\n    {torch.nn.Linear},            # Quantize Linear layers\n    dtype=torch.qint8             # Use 8-bit quantization\n)\n\n# Step 3: Replace the model with the quantized version\nsmoll_lightning_model.model = quantized_model\nprint(\"✅ Model quantized successfully\")\n\n# Step 4: Save the quantized checkpoint (for Lightning reloading)\nquantized_ckpt_path = \"quantized_smollLMv2.ckpt\"\ntorch.save({\"state_dict\": smoll_lightning_model.state_dict()}, quantized_ckpt_path)\nprint(f\"✅ Quantized checkpoint saved: {quantized_ckpt_path}\")\n\n# Step 5: Save model and tokenizer in Hugging Face-compatible format\n\nhf_save_path = \"quantized-smollM2\"\nos.makedirs(hf_save_path, exist_ok=True)\n\n# Save model weights\ntorch.save(smoll_lightning_model.model.state_dict(), os.path.join(hf_save_path, \"pytorch_model.bin\"))\nprint(f\"✅ Quantized model weights saved: {hf_save_path}/pytorch_model.bin\")\n\n# Save tokenizer using Hugging Face API\nsmoll_lightning_model.tokenizer.save_pretrained(hf_save_path)\nprint(f\"✅ Tokenizer saved for Hugging Face: {hf_save_path}\")\n\n\n# Step 6: Save model configuration (including SMOLConfig details)\nconfig_hf_model = {\n    \"model_type\": \"smollm2-135m\",\n    \"torch_dtype\": \"int8\",  # Quantized model\n    \"architectures\": [\"SMOLL\"],\n    \"vocab_size\": smoll_lightning_model.tokenizer.vocab_size,\n    \n    # Add SMOLConfig parameters\n    \"block_size\": 1024,\n    \"n_layer\": 30,\n    \"n_head\": 9,\n    \"n_embed\": 576,\n    \"rope_theta\": 10000.0,\n    \"rms_norm_eps\": 1.0e-5,\n    \"intermediate_size\": 1536,\n\n    # Additional training parameters\n    \"batch_size\": 8,\n    \"max_position_embeddings\": 64,  # Sequence length\n}\n\nimport json\n# Save the config.json file\nwith open(os.path.join(hf_save_path, \"config.json\"), \"w\") as f:\n    json.dump(config_hf_model, f, indent=4)\n\nprint(f\"✅ Hugging Face config saved: {hf_save_path}/config.json\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:31:05.841456Z","iopub.execute_input":"2025-02-22T14:31:05.841796Z","iopub.status.idle":"2025-02-22T14:31:13.572941Z","shell.execute_reply.started":"2025-02-22T14:31:05.841765Z","shell.execute_reply":"2025-02-22T14:31:13.571938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfolder_path = \"/kaggle/working/quantized-smollM2\"\noutput_zip_path = \"/kaggle/working/quantized-smollM2.zip\"\n\n# Create a zip archive of the folder\nshutil.make_archive(output_zip_path.replace(\".zip\", \"\"), 'zip', folder_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:36:13.519348Z","iopub.execute_input":"2025-02-22T14:36:13.519682Z","iopub.status.idle":"2025-02-22T14:36:24.833961Z","shell.execute_reply.started":"2025-02-22T14:36:13.519655Z","shell.execute_reply":"2025-02-22T14:36:24.832903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfolder_path = \"/kaggle/working/smollm_model\"\noutput_zip_path = \"/kaggle/working/smollm_model.zip\"\n\n# Create a zip archive of the folder\nshutil.make_archive(output_zip_path.replace(\".zip\", \"\"), 'zip', folder_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T14:36:42.089674Z","iopub.execute_input":"2025-02-22T14:36:42.090107Z","iopub.status.idle":"2025-02-22T14:38:24.796336Z","shell.execute_reply.started":"2025-02-22T14:36:42.090058Z","shell.execute_reply":"2025-02-22T14:38:24.795500Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}