{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10869463,"sourceType":"datasetVersion","datasetId":6752754}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n\n\n\n# Saving tokenizer files for hugging face app\nimport shutil\nimport os\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Paths for saving tokenizer and model\nsave_dir = \"/kaggle/working/smollm_model\"\n\n# Ensure the directory exists|\nos.makedirs(save_dir, exist_ok=True)\n\n# Load tokenizer and model (replace with your model name or path)\n\ntokenizer_saved = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\nmodel_saved = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=token)\n\n# Save tokenizer and model to working directory\ntokenizer_saved.save_pretrained(save_dir)\nmodel_saved.save_pretrained(save_dir)\n\nprint(f\">>> Model and tokenizer saved to {save_dir}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T09:58:59.662065Z","iopub.execute_input":"2025-02-27T09:58:59.662240Z","iopub.status.idle":"2025-02-27T09:59:25.406784Z","shell.execute_reply.started":"2025-02-27T09:58:59.662221Z","shell.execute_reply":"2025-02-27T09:59:25.405861Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:810: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.76k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a24a20ccd5b341bf97c8e7fa5a9604c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fe6d4832c54490b82382bbeba646df9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba8c5aeaa6484c6aaf0bbd3b8f0dde53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95d99560ec23495c86a126c9402897a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b21fa9c990446d6b400f6870eacbe45"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/861 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f4adbf8a8844d0c9bb1d0c31aa7a30f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f2a04616c3c4bd783ae91ca1cc0e448"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"648c96e6f8aa4bfbaf35f46e84fb2d35"}},"metadata":{}},{"name":"stdout","text":">>> Model and tokenizer saved to /kaggle/working/smollm_model\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# without ROPE\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom dataclasses import dataclass\nimport math\nimport time\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch.quantization import quantize_dynamic\n\n# -----------------------------------------------------------------------------\n# Set matmul precision for performance\n# -----------------------------------------------------------------------------\ntorch.set_float32_matmul_precision('high')\n\n# -----------------------------------------------------------------------------\n# Configuration Class for SmolLM2 Model\n# -----------------------------------------------------------------------------\n@dataclass\nclass SMOLConfig:\n    block_size: int = 1024\n    vocab_size: int = 49152\n    n_layer: int = 30\n    n_head: int = 9\n    n_embed: int = 576\n    rope_theta: float = 10000.0\n    rms_norm_eps: float = 1.0e-5\n    intermediate_size: int = 1536\n\n    def __post_init__(self):\n        head_dim = self.n_embed // self.n_head\n        if head_dim & (head_dim - 1) != 0:\n            raise ValueError(\"Head dimension must be a power of 2!\")\n\n# -----------------------------------------------------------------------------\n# RMSNorm: Root Mean Square Normalization\n# -----------------------------------------------------------------------------\nclass RMSNorm(nn.Module):\n    def __init__(self, dim, eps=1e-8):\n        super().__init__()\n        self.eps = eps\n        self.scale = nn.Parameter(torch.ones(dim))\n    def forward(self, x):\n        norm = x.norm(2, dim=-1, keepdim=True)\n        return x * (self.scale / (norm + self.eps))\n\n# -----------------------------------------------------------------------------\n# Causal Self-Attention Module (WITHOUT RoPE)\n# -----------------------------------------------------------------------------\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.n_head = config.n_head\n        self.n_embed = config.n_embed\n        self.head_dim = config.n_embed // config.n_head\n\n        # Linear projections for Q, K, V and output\n        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed)\n        self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n\n        # Note: RoPE has been removed to simplify the model and reduce overhead.\n        self.norm = RMSNorm(config.n_embed, eps=config.rms_norm_eps)\n\n    def forward(self, x):\n        B, T, C = x.size()\n        x = self.norm(x)\n        qkv = self.c_attn(x).split(self.n_embed, dim=2)\n        q, k, v = [qkv[i].view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n                   for i in range(3)]\n        # Bypassing RoPE – use q and k directly.\n        y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=True)\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        return self.c_proj(y)\n\n# -----------------------------------------------------------------------------\n# MLP Block with RMSNorm\n# -----------------------------------------------------------------------------\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        hidden_size = config.intermediate_size if config.intermediate_size else 4 * config.n_embed\n        self.c_fc = nn.Linear(config.n_embed, hidden_size)\n        self.gelu = nn.GELU(approximate=\"tanh\")\n        self.c_proj = nn.Linear(hidden_size, config.n_embed)\n        self.rms_norm = RMSNorm(config.n_embed, eps=config.rms_norm_eps)\n\n    def forward(self, x):\n        x = self.gelu(self.c_fc(x))\n        x = self.c_proj(x)\n        return self.rms_norm(x)\n\n# -----------------------------------------------------------------------------\n# Transformer Block with Residual Scaling\n# -----------------------------------------------------------------------------\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.rms_1 = RMSNorm(config.n_embed, eps=config.rms_norm_eps)\n        self.attn = CausalSelfAttention(config)\n        self.rms_2 = RMSNorm(config.n_embed, eps=config.rms_norm_eps)\n        self.mlp = MLP(config)\n        self.res_scale = 1.0 / math.sqrt(2)\n\n    def forward(self, x):\n        x = x + self.res_scale * self.attn(self.rms_1(x))\n        x = x + self.res_scale * self.mlp(self.rms_2(x))\n        return x\n\n# -----------------------------------------------------------------------------\n# SmolLM Model Definition with Weight Sharing\n# -----------------------------------------------------------------------------\nclass SMOLL(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.transformer = nn.ModuleDict({\n            'wte': nn.Embedding(config.vocab_size, config.n_embed),\n            'wpe': nn.Embedding(config.block_size, config.n_embed),\n            'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            'ln_f': nn.LayerNorm(config.n_embed, eps=config.rms_norm_eps)\n        })\n        self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)\n        self.lm_head.weight = self.transformer['wte'].weight\n\n    def forward(self, idx, targets=None):\n        B, T = idx.size()\n        assert T <= self.config.block_size, \"Sequence length exceeds block size\"\n        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n        x = self.transformer['wte'](idx) + self.transformer['wpe'](pos)\n        for block in self.transformer['h']:\n            x = block(x)\n        logits = self.lm_head(self.transformer['ln_f'](x))\n        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) if targets is not None else None\n        return logits, loss\n\n    @classmethod\n    def from_pretrained(cls, model_type, model_name, token=None):\n        assert model_type == 'SmolLM2-135M', \"Only 'SmolLM2-135M' is supported.\"\n        config = SMOLConfig()\n        model = cls(config)\n        hf_model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=token)\n        model.load_state_dict(hf_model.state_dict(), strict=False)\n        return model\n\n# -----------------------------------------------------------------------------\n# PyTorch Lightning Module for SmolLM\n# -----------------------------------------------------------------------------\nclass SMOLLLightningModule(pl.LightningModule):\n    def __init__(self, model_type=\"SmolLM2-135M\", model_name=None, token=None):\n        super().__init__()\n        self.model = SMOLL.from_pretrained(model_type, model_name, token)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n\n    def forward(self, x, targets=None):\n        return self.model(x, targets)\n\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        _, loss = self.model(x, targets=y)\n    \n        # Log training loss\n        self.log(\"train_loss\", loss, on_step=True, prog_bar=True)\n    \n        \n        if self.global_step % 200 == 0:\n            print(\">>> processing step: \", self.global_step)\n\n        # Display generation samples every 500 steps\n        if self.global_step % 500 == 0:\n            sample = self.generate(\"Once upon a time\", max_length=100, temperature=1.0)\n            print(f\"\\n\\n >>> Step {self.global_step}: \\n >>> Generated Sample:\\n{sample}\\n\")\n\n    \n        return loss\n\n\n    def generate(self, prompt, max_length=100, temperature=1.0):\n        self.model.eval()\n        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n        with torch.no_grad():\n            for _ in range(max_length):\n                logits, _ = self.model(input_ids)\n                next_token_logits = logits[:, -1, :] / temperature\n                probs = F.softmax(next_token_logits, dim=-1)\n                next_token = torch.multinomial(probs, num_samples=1)\n                input_ids = torch.cat([input_ids, next_token], dim=1)\n        return self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n\n    def configure_optimizers(self):\n        optimizer = AdamW(self.model.parameters(), lr=3e-3, betas=(0.9, 0.95), eps=1e-8, weight_decay=0.01)\n        def lr_lambda(step):\n            warmup_steps = 2000\n            decay_start = 1600000\n            decay_steps = 400000\n            if step < warmup_steps:\n                return step / warmup_steps\n            elif step < decay_start:\n                return 1.0\n            return max(0.0, 1 - (step - decay_start) / decay_steps)\n        scheduler = LambdaLR(optimizer, lr_lambda)\n        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"}}\n\n# -----------------------------------------------------------------------------\n# Custom Dataset for Text Data\n# -----------------------------------------------------------------------------\nclass TextDataset(Dataset):\n    def __init__(self, tokens, block_size):\n        self.tokens = tokens\n        self.block_size = block_size\n\n    def __len__(self):\n        return len(self.tokens) - self.block_size\n\n    def __getitem__(self, idx):\n        return (\n            torch.tensor(self.tokens[idx:idx + self.block_size], dtype=torch.long),\n            torch.tensor(self.tokens[idx + 1:idx + self.block_size + 1], dtype=torch.long)\n        )\n\n# -----------------------------------------------------------------------------\n# Training Setup\n# -----------------------------------------------------------------------------\nT = 64\nbatch_size = 8\nfile_path = \"/kaggle/input/shakespeare-text-data/input.txt\"\n\nwith open(file_path, 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# Initialize tokenizer and tokenize text\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\ntext = text[:1000]  # Optionally trim for demonstration\ntokens = tokenizer.encode(text, add_special_tokens=False)\nif len(tokens) < T + 1:\n    raise ValueError(\"Not enough tokens in input.txt for one training sample.\")\ntokens = tokenizer.encode(text, add_special_tokens=False)\n\ndataset = TextDataset(tokens, T)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\nprint(f\"Dataset size: {len(dataset)}, Batch size: {batch_size}\")\n\nlit_model = SMOLLLightningModule(model_name=model_name, token=token)\ntrainer = Trainer(max_steps=3500, log_every_n_steps=50, accelerator=\"auto\", devices=1, enable_progress_bar=True)\ntrainer.fit(lit_model, dataloader)\n\n# -----------------------------------------------------------------------------\n# Save the Original (FP32) Checkpoint\n# -----------------------------------------------------------------------------\ntrainer.save_checkpoint(\"smollm2_135m_lat.ckpt\")\nprint(\"✅ Original checkpoint saved.\")\n\n# -----------------------------------------------------------------------------\n# Apply Dynamic Quantization (8-bit) to Reduce Model Size\n# -----------------------------------------------------------------------------\nlit_model.model = quantize_dynamic(lit_model.model, {nn.Linear}, dtype=torch.qint8)\n# Save the quantized model checkpoint (for HF Spaces, this should be < 1GB)\ntorch.save({\"state_dict\": lit_model.state_dict()}, \"lat_smollm2_135m_quantized.ckpt\")\nprint(\"✅ Quantized checkpoint saved (reduced model size).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T10:04:59.328968Z","iopub.execute_input":"2025-02-27T10:04:59.329299Z","iopub.status.idle":"2025-02-27T10:34:49.447353Z","shell.execute_reply.started":"2025-02-27T10:04:59.329272Z","shell.execute_reply":"2025-02-27T10:34:49.446437Z"}},"outputs":[{"name":"stdout","text":"Dataset size: 222, Batch size: 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"973cb3a91a624d4bb0359879dc772722"}},"metadata":{}},{"name":"stdout","text":">>> processing step:  0\n\n\n >>> Step 0: \n >>> Generated Sample:\nOnce upon a time URI ACTION evid strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg enthus comprom strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg necessit necessit authent strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg affili affili affili affili affili strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg strugg interpre strugg strugg strugg dissemin\n\n>>> processing step:  200\n>>> processing step:  400\n\n\n >>> Step 500: \n >>> Generated Sample:\nOnce upon a time:\n\n be done: away, away, away!\n\nSecond Citizen:\nWe are accounted poor citizens, the patricians good.\nAll:\nWhat authority surfeits on would relieve us: if they relieved us: if they\nwould yield us but the superfluity, while whileely;.\nwhanceance is too is tooance of surfe of our superflulicts weouraffaffafflicts more resolved everywhere more more gods\n, whilefe revenge the superflu\n\n>>> processing step:  600\n>>> processing step:  800\n>>> processing step:  1000\n\n\n >>> Step 1000: \n >>> Generated Sample:\nOnce upon a time\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we weely; citizens.\n\n\nwhkes, weWe resolved citizens, we areiseient I\n\nspeak they think thinkak more more gods\nikesomeome, the resolved\n\n>>> processing step:  1200\n>>> processing step:  1400\n\n\n >>> Step 1500: \n >>> Generated Sample:\nOnce upon a time't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't;; let it be done: away, away!!\n\nSecond Citizen:\nOne word, good racius is have corn too is chief chief ra rakes: for the chief chief poorlicts us Mar: for I: for thecius for the thelicts uswoulder\n\n>>> processing step:  1600\n>>> processing step:  1800\n>>> processing step:  2000\n\n\n >>> Step 2000: \n >>> Generated Sample:\nOnce upon a time an\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nsufferance is too is asufferance e talking on e e e becomekeskes: for a gain to the gain to them price thiserancesuffer\n\n>>> processing step:  2200\n>>> processing step:  2400\n\n\n >>> Step 2500: \n >>> Generated Sample:\nOnce upon a timeWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particulariseolesome, is as we is as an\ninventory a good citizens, is as an\ninventory to particularise their misery, weinventory to particular\n\n>>> processing step:  2600\n>>> processing step:  2800\n>>> processing step:  3000\n\n\n >>> Step 3000: \n >>> Generated Sample:\nOnce upon a time patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us are too us, the object of our misery, the object us. think we are toolictslicts our\nafflicts: if they think we are toolicts: the the patricafflicts\n\n>>> processing step:  3200\n>>> processing step:  3400\n✅ Original checkpoint saved.\n✅ Quantized checkpoint saved (reduced model size).\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # # # # # # #\n# PARAMETERS #\n# # # # # # ##\n\nif __name__ == \"__main__\":\n    # Initialize model config\n    config = SMOLConfig()\n    model = SMOLL(config)\n\n    # Print model architecture\n    print(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T10:56:07.345054Z","iopub.execute_input":"2025-02-27T10:56:07.345368Z","iopub.status.idle":"2025-02-27T10:56:08.591730Z","shell.execute_reply.started":"2025-02-27T10:56:07.345346Z","shell.execute_reply":"2025-02-27T10:56:08.590988Z"}},"outputs":[{"name":"stdout","text":"SMOLL(\n  (transformer): ModuleDict(\n    (wte): Embedding(49152, 576)\n    (wpe): Embedding(1024, 576)\n    (h): ModuleList(\n      (0-29): 30 x Block(\n        (rms_1): RMSNorm()\n        (attn): CausalSelfAttention(\n          (c_attn): Linear(in_features=576, out_features=1728, bias=True)\n          (c_proj): Linear(in_features=576, out_features=576, bias=True)\n          (norm): RMSNorm()\n        )\n        (rms_2): RMSNorm()\n        (mlp): MLP(\n          (c_fc): Linear(in_features=576, out_features=1536, bias=True)\n          (gelu): GELU(approximate='tanh')\n          (c_proj): Linear(in_features=1536, out_features=576, bias=True)\n          (rms_norm): RMSNorm()\n        )\n      )\n    )\n    (ln_f): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ==========================================\n# Final Inference Example1: Text Generation\n# ==========================================\ninference_prompt1 = lit_model.generate(\"Once upon a time\", max_length=100, temperature=1.0)\nprint(\"Final generated text:\", inference_prompt1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T10:58:59.766325Z","iopub.execute_input":"2025-02-27T10:58:59.766628Z","iopub.status.idle":"2025-02-27T10:59:09.270520Z","shell.execute_reply.started":"2025-02-27T10:58:59.766608Z","shell.execute_reply":"2025-02-27T10:59:09.269693Z"}},"outputs":[{"name":"stdout","text":"Final generated text: Once upon a time die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our proceed any:\nYou cornness good speak good leancius is chief corn at corn at our own price at our own price price. resolvedish their abundance\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# # # # # # #  # # # # # # #\n# QUANTIZATION & INFERENCE #\n# # # # # # #  # # # # # # #\n\nimport torch\n\n\n# Path to your quantized checkpoint\nquantized_ckpt_path = \"/kaggle/working/lat_smollm2_135m_quantized.ckpt\"\n\n# Step 1: Create an instance of your Lightning model\nlit_model = SMOLLLightningModule(model_name=model_name, token=token)\n\n# Step 2: Load the quantized state dict using strict=False\ncheckpoint = torch.load(quantized_ckpt_path, map_location=\"cpu\")\nlit_model.load_state_dict(checkpoint[\"state_dict\"], strict=False)\nlit_model.eval()  # Set to evaluation mode\n\nprint(\"✅ Quantized model loaded for inference.\")\n\n# Step 3: Perform inference\ninference_prompt = \"Once upon a time\"\ngenerated_text = lit_model.generate(inference_prompt, max_length=100, temperature=1.0)\n\nprint(\"Final generated text:\", generated_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T10:59:42.722023Z","iopub.execute_input":"2025-02-27T10:59:42.722362Z","iopub.status.idle":"2025-02-27T11:00:00.140874Z","shell.execute_reply.started":"2025-02-27T10:59:42.722337Z","shell.execute_reply":"2025-02-27T11:00:00.140058Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-9-f8882c922430>:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(quantized_ckpt_path, map_location=\"cpu\")\n","output_type":"stream"},{"name":"stdout","text":"✅ Quantized model loaded for inference.\nFinal generated text: Once upon a time an an an an an an an an an an an anOMrophesnesdaybridsionageitted pillows maize literatures enthus strugg strugg strugg strugg strugg strugg satis enric enric interpre interpre interpre interpre interpre interpre retros interspers strugg strugg strugg affili affili enthusIng propagatingnesday prioritize phthal subprocess dend unem unem includ includ includ evid evid evid comprom policymelyely cites paves offic drierosteringroximatelyrestrialiscopalulesanceanceanceanceanceanceanceanceanceanceanceanceanceanceanceanceanceanceanceanceererererererer\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# # # # # # # #\n# EXTRA STEPS #\n# # # # # # # #\n\nlit_model_extra = SMOLLLightningModule.load_from_checkpoint(\"smollm2_135m_lat.ckpt\",model_name=model_name, token=token)\ntrainer_extra = Trainer(\n    max_steps=50,\n    log_every_n_steps=10,\n    accelerator=\"auto\",\n    devices=1\n)\ntrainer_extra.fit(lit_model_extra, train_dataloaders=dataloader)\nprint(\"Additional 50 training steps completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:00:23.380142Z","iopub.execute_input":"2025-02-27T11:00:23.380434Z","iopub.status.idle":"2025-02-27T11:00:45.646221Z","shell.execute_reply.started":"2025-02-27T11:00:23.380412Z","shell.execute_reply":"2025-02-27T11:00:45.644843Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bf1351d56074de3bf5cde3d4e07cd58"}},"metadata":{}},{"name":"stdout","text":">>> processing step:  0\n\n\n >>> Step 0: \n >>> Generated Sample:\nOnce upon a time die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we proceed any dear:\nYou are'll have corn talking on superflu him, you might corn are corn at corn at at resolved rather at our own price price. resolved resolved resolved.\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-a6ea61e532c1>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrainer_extra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit_model_extra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Additional 50 training steps completed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         )\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected state {self.state}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36mon_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_train_epoch_end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitoring_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_lightning_module_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_train_epoch_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_train_epoch_end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitoring_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Callback]{callback.state_key}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpl_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36mon_train_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m_save_topk_checkpoint\u001b[0;34m(self, trainer, monitor_candidates)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m_save_none_monitor_checkpoint\u001b[0;34m(self, trainer, monitor_candidates)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, trainer, filepath)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(self, filepath, weights_only, storage_options)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"save_checkpoint\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1368\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarrier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trainer.save_checkpoint\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(self, checkpoint, filepath, storage_options)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \"\"\"\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_global_zero\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mremove_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_PATH\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning_fabric/plugins/io/torch_io.py\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(self, checkpoint, path, storage_options)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning_fabric/utilities/cloud_io.py\u001b[0m in \u001b[0;36m_atomic_save\u001b[0;34m(checkpoint, filepath)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# We use a transaction here to avoid file corruption if the save gets interrupted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murlpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl_to_fs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransaction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytesbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/spec.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m             \u001b[0mac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"autocommit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m             f = self._open(\n\u001b[0m\u001b[1;32m   1311\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_mkdir\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mLocalFileOpener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtouch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocksize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0;31m# TODO: check if path is writable?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkstemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# we want normal open and normal buffered file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/tempfile.py\u001b[0m in \u001b[0;36mmkstemp\u001b[0;34m(suffix, prefix, dir, text)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/tempfile.py\u001b[0m in \u001b[0;36m_mkstemp_inner\u001b[0;34m(dir, pre, suf, flags, output_type)\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/tmp/tmpt411p2d3'"],"ename":"OSError","evalue":"[Errno 30] Read-only file system: '/tmp/tmpt411p2d3'","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"import shutil\nimport os\n\nfolder_path = \"/kaggle/working/\"\noutput_zip_path = os.path.join(folder_path, \"smollM2.zip\")\n\n# Create a zip archive of the folder\nshutil.make_archive(output_zip_path[:-4], 'zip', folder_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T11:02:34.862703Z","iopub.execute_input":"2025-02-27T11:02:34.863052Z","execution_failed":"2025-02-27T11:03:31.869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}