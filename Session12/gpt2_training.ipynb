{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/kishkath/ERA3.git","metadata":{"id":"mXFJ2i4qtpOy","outputId":"32e4c71a-af60-4c18-c253-9e6aef17892c","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T04:52:52.901478Z","iopub.execute_input":"2025-01-24T04:52:52.901773Z","iopub.status.idle":"2025-01-24T04:52:54.136957Z","shell.execute_reply.started":"2025-01-24T04:52:52.901745Z","shell.execute_reply":"2025-01-24T04:52:54.135959Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'ERA3'...\nremote: Enumerating objects: 510, done.\u001b[K\nremote: Counting objects: 100% (16/16), done.\u001b[K\nremote: Compressing objects: 100% (16/16), done.\u001b[K\nremote: Total 510 (delta 5), reused 0 (delta 0), pack-reused 494 (from 3)\u001b[K\nReceiving objects: 100% (510/510), 7.41 MiB | 23.50 MiB/s, done.\nResolving deltas: 100% (199/199), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/working//ERA3/Session12\")\n\nprint('current directory: ', os.getcwd())","metadata":{"id":"afcsZ_rwu5_9","outputId":"f5cd19a7-9b7e-4d73-bb6f-7ad196cf606b","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T04:52:54.137781Z","iopub.execute_input":"2025-01-24T04:52:54.137999Z","iopub.status.idle":"2025-01-24T04:52:54.142640Z","shell.execute_reply.started":"2025-01-24T04:52:54.137976Z","shell.execute_reply":"2025-01-24T04:52:54.141709Z"}},"outputs":[{"name":"stdout","text":"current directory:  /kaggle/working/ERA3/Session12\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# prompt: remove al files in gpt2_model directory\n\nimport shutil\n\ngpt2_model_dir = \"/kaggle/working/gpt2_model\"  # Replace with the actual path if different\n\ntry:\n    shutil.rmtree(gpt2_model_dir)\n    print(f\"Successfully removed directory: {gpt2_model_dir}\")\nexcept FileNotFoundError:\n    print(f\"Directory not found: {gpt2_model_dir}\")\nexcept OSError as e:\n    print(f\"Error removing directory: {e}\")","metadata":{"id":"9Ao4smFBP69I","outputId":"25d1d99f-2181-49f6-f369-d67a25ae3dbf","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T04:52:54.143669Z","iopub.execute_input":"2025-01-24T04:52:54.143861Z","iopub.status.idle":"2025-01-24T04:52:54.159199Z","shell.execute_reply.started":"2025-01-24T04:52:54.143845Z","shell.execute_reply":"2025-01-24T04:52:54.158543Z"}},"outputs":[{"name":"stdout","text":"Directory not found: /kaggle/working/gpt2_model\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"os.mkdir(\"/kaggle/working/gpt2_model\")","metadata":{"id":"hLUt0I2vMyhV","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T04:52:54.160099Z","iopub.execute_input":"2025-01-24T04:52:54.160394Z","iopub.status.idle":"2025-01-24T04:52:54.172605Z","shell.execute_reply.started":"2025-01-24T04:52:54.160367Z","shell.execute_reply":"2025-01-24T04:52:54.171966Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!pip install tiktoken transformers torch\n","metadata":{"id":"qXU9Jd6WuM7y","outputId":"37f22b13-7e49-45a3-9f2f-4650175326a9","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T04:52:54.173400Z","iopub.execute_input":"2025-01-24T04:52:54.173664Z","iopub.status.idle":"2025-01-24T04:52:58.281903Z","shell.execute_reply.started":"2025-01-24T04:52:54.173638Z","shell.execute_reply":"2025-01-24T04:52:58.280870Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Solving for residual std scaling issue\nimport os\nimport math\nimport time\nimport inspect\nfrom dataclasses import dataclass\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        self.c_proj.NANGPT_SCALE_INIT = 1\n        # regularization\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n        qkv = self.c_attn(x)\n        q, k, v = qkv.split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n        # output projection\n        y = self.c_proj(y)\n        return y\n\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n        self.gelu    = nn.GELU(approximate='tanh')\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n        self.c_proj.NANOGPT_SCALE_INIT = 1\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 1024 # max sequence length\n    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n    n_layer: int = 12 # number of layers\n    n_head: int = 12 # number of heads\n    n_embd: int = 768 # embedding dimension\n\n\nclass GPT(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = nn.LayerNorm(config.n_embd),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\n        # weight sharing\n        self.transformer.wte.weight = self.lm_head.weight\n\n        # weight initialization\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            std = 0.02\n            if hasattr(module, 'NANGPT_SCALE_INIT'):\n                std *= (2 * self.config.n_layer) ** -0.5\n            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n\n\n\n    def forward(self, idx, targets=None):\n        # idx is of shape (B, T)\n        B, T = idx.size()\n        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n        # forward the token and posisition embeddings\n        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n        x = tok_emb + pos_emb\n        # forward the blocks of the transformer\n        for block in self.transformer.h:\n            x = block(x)\n        # forward the final layernorm and the classifier\n        x = self.transformer.ln_f(x)\n        logits = self.lm_head(x) # (B, T, vocab_size)\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n        return logits, loss\n\n    @classmethod\n    def from_pretrained(cls, model_type):\n        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n        from transformers import GPT2LMHeadModel\n        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n        # n_layer, n_head and n_embd are determined from model_type\n        config_args = {\n            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n        }[model_type]\n        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n        # create a from-scratch initialized minGPT model\n        config = GPTConfig(**config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n\n        # init a huggingface/transformers model\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n        sd_hf = model_hf.state_dict()\n\n        # copy while ensuring all of the parameters are aligned and match in names and shapes\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n        # this means that we have to transpose these weights when we import them\n        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n        for k in sd_keys_hf:\n            if any(k.endswith(w) for w in transposed):\n                # special treatment for the Conv1D weights we need to transpose\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                # vanilla copy over the other parameters\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n        return model\n\n# model = GPT.from_pretrained('gpt2')\n\ndevice = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'\nelif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n    device = \"mps\"\nprint(f\"using device: {device}\")\n\n# SEED\ntorch.manual_seed(1337)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(1337)\n\n# STOP\nnum_return_sequences = 5\nmax_length = 30\n\n\n\nimport tiktoken\n\nclass DataLoaderLite:\n    def __init__(self, B, T):\n        self.B = B\n        self.T = T\n\n        # at init load tokens from disk and store them in memory\n        with open(os.getcwd() + \"/input/input.txt\", 'r') as f:\n            text = f.read()\n        enc = tiktoken.get_encoding('gpt2')\n        tokens = enc.encode(text)\n        self.tokens = torch.tensor(tokens)\n        print(f'loaded {len(self.tokens)} tokens')\n        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n\n        # state\n        self.current_position = 0\n\n    def next_batch(self):\n        B, T = self.B, self.T\n        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n        x = (buf[:-1]).view(B, T) # inputs\n        y = (buf[1:]).view(B, T) # targets\n        # advance the position in the tensor\n        self.current_position += B*T\n        # if loading the next batch would be out of bounds, reset\n        if self.current_position + (B * T + 1) > len(self.tokens):\n            self.current_position = 0\n        return x, y\n\n\n\nmodel = GPT(GPTConfig())\nmodel.to(device)\n\ntrain_loader = DataLoaderLite(B = 8, T = 512)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)\nfor i in range(9600):\n    x, y = train_loader.next_batch()\n    x, y = x.to(device), y.to(device)\n    optimizer.zero_grad()\n    logits, loss = model(x, y)\n    loss.backward()\n    optimizer.step()\n    print(f'step{i}, loss: {loss.item()}')\n\n\n# print(loss)\n# import sys; sys.exit(0)\n\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\nwhile x.size(1) < max_length:\n    # forward the model to get the logits\n    with torch.no_grad():\n        logits = model(x)[0] # (B, T, vocab_size)\n        # take the logits at the last position\n        logits = logits[:, -1, :] # (B, vocab_size)\n        # get the probabilities\n        probs = F.softmax(logits, dim=-1)\n        # do top-k sampling of 50 (huggingface pipeline default)\n        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n        # select a token from the top-k probabilities\n        # note: multinomial does not demand the input to sum to 1\n        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n        # gather the corresponding indices\n        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n        # append to the sequence\n        x = torch.cat((x, xcol), dim=1)\n\n# print the generated text\nfor i in range(num_return_sequences):\n    enc = tiktoken.get_encoding('gpt2')\n    tokens = x[i, :max_length].tolist()\n    decoded = enc.decode(tokens)\n    print(\">\", decoded)\n\nprint(\"saving models\")\n\n# Assuming `model` is your trained GPT model\ndef save_model(model, save_directory):\n    # Save the model's state_dict (weights)\n    torch.save(model.state_dict(), f\"{save_directory}/model_state_dict.pth\")\n\n    # Save the model's configuration (optional)\n    torch.save(model.config, f\"{save_directory}/config.pth\")\n\n    print(f\"Model saved to {save_directory}\")\n\nfrom transformers import GPT2Tokenizer\n\ndef save_tokenizer(save_directory):\n    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n    tokenizer.save_pretrained(save_directory)\n    print(f\"Tokenizer saved to {save_directory}\")\n\nsave_model(model, \"/kaggle/working/gpt2_model\")\nsave_tokenizer(\"/kaggle/working/gpt2_model\")\n","metadata":{"id":"RWosJNb1u7kg","outputId":"d09549ef-2782-4ee4-9f8a-c9b34c4256ab","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T04:52:58.283771Z","iopub.execute_input":"2025-01-24T04:52:58.284072Z","iopub.status.idle":"2025-01-24T06:23:00.800182Z","shell.execute_reply.started":"2025-01-24T04:52:58.284037Z","shell.execute_reply":"2025-01-24T06:23:00.799402Z"}},"outputs":[{"name":"stdout","text":"using device: cuda\nloaded 338025 tokens\n1 epoch = 82 batches\nstep0, loss: 10.96817398071289\nstep1, loss: 9.445795059204102\nstep2, loss: 9.425582885742188\nstep3, loss: 8.719521522521973\nstep4, loss: 8.436490058898926\nstep5, loss: 8.073561668395996\nstep6, loss: 7.924859523773193\nstep7, loss: 7.725032329559326\nstep8, loss: 7.648287773132324\nstep9, loss: 7.348138809204102\nstep10, loss: 7.359038829803467\nstep11, loss: 7.361432075500488\nstep12, loss: 7.43705415725708\nstep13, loss: 7.342714309692383\nstep14, loss: 6.97465705871582\nstep15, loss: 6.974205493927002\nstep16, loss: 6.771080017089844\nstep17, loss: 6.588798522949219\nstep18, loss: 6.773491859436035\nstep19, loss: 6.721819877624512\nstep20, loss: 6.929699420928955\nstep21, loss: 6.737103462219238\nstep22, loss: 6.696321487426758\nstep23, loss: 6.768130302429199\nstep24, loss: 6.786308765411377\nstep25, loss: 6.782924652099609\nstep26, loss: 6.594720363616943\nstep27, loss: 6.656871318817139\nstep28, loss: 6.671631813049316\nstep29, loss: 6.502857685089111\nstep30, loss: 6.423130512237549\nstep31, loss: 6.358510494232178\nstep32, loss: 6.425535202026367\nstep33, loss: 6.529300212860107\nstep34, loss: 6.514527797698975\nstep35, loss: 6.507079601287842\nstep36, loss: 6.3286237716674805\nstep37, loss: 6.484100341796875\nstep38, loss: 6.2956461906433105\nstep39, loss: 6.125344276428223\nstep40, loss: 6.2365875244140625\nstep41, loss: 6.327980995178223\nstep42, loss: 6.175322532653809\nstep43, loss: 6.174806118011475\nstep44, loss: 6.323832988739014\nstep45, loss: 6.223538875579834\nstep46, loss: 6.0797119140625\nstep47, loss: 6.124049663543701\nstep48, loss: 6.137714385986328\nstep49, loss: 6.029858112335205\nstep50, loss: 6.156731128692627\nstep51, loss: 6.099180698394775\nstep52, loss: 6.499173641204834\nstep53, loss: 6.438144683837891\nstep54, loss: 6.2215447425842285\nstep55, loss: 6.342721462249756\nstep56, loss: 6.595836639404297\nstep57, loss: 6.497611999511719\nstep58, loss: 6.201019763946533\nstep59, loss: 6.379694938659668\nstep60, loss: 6.224828243255615\nstep61, loss: 6.190147876739502\nstep62, loss: 6.317755222320557\nstep63, loss: 6.185497760772705\nstep64, loss: 6.02191686630249\nstep65, loss: 6.196703910827637\nstep66, loss: 6.415100574493408\nstep67, loss: 6.311424732208252\nstep68, loss: 6.265488624572754\nstep69, loss: 6.059118270874023\nstep70, loss: 5.944218635559082\nstep71, loss: 6.297621726989746\nstep72, loss: 6.25553035736084\nstep73, loss: 6.033632755279541\nstep74, loss: 6.074534893035889\nstep75, loss: 6.230832576751709\nstep76, loss: 6.011192798614502\nstep77, loss: 5.773349285125732\nstep78, loss: 5.794866561889648\nstep79, loss: 5.820981979370117\nstep80, loss: 6.37580680847168\nstep81, loss: 6.238350868225098\nstep82, loss: 6.152252197265625\nstep83, loss: 6.120233535766602\nstep84, loss: 6.135134696960449\nstep85, loss: 6.135409355163574\nstep86, loss: 5.852039813995361\nstep87, loss: 5.71901798248291\nstep88, loss: 5.706539630889893\nstep89, loss: 5.669126510620117\nstep90, loss: 5.771190643310547\nstep91, loss: 5.6736369132995605\nstep92, loss: 5.9053521156311035\nstep93, loss: 5.98199987411499\nstep94, loss: 6.0988450050354\nstep95, loss: 6.081949710845947\nstep96, loss: 5.895505905151367\nstep97, loss: 5.917560577392578\nstep98, loss: 5.722748279571533\nstep99, loss: 5.709737777709961\nstep100, loss: 5.948135852813721\nstep101, loss: 5.848276615142822\nstep102, loss: 6.099879264831543\nstep103, loss: 5.852154731750488\nstep104, loss: 6.012629508972168\nstep105, loss: 6.114825248718262\nstep106, loss: 6.167909145355225\nstep107, loss: 6.135548114776611\nstep108, loss: 5.974634647369385\nstep109, loss: 6.009465217590332\nstep110, loss: 6.017391681671143\nstep111, loss: 5.8895134925842285\nstep112, loss: 5.767980098724365\nstep113, loss: 5.667701244354248\nstep114, loss: 5.769280433654785\nstep115, loss: 5.909041404724121\nstep116, loss: 5.925837993621826\nstep117, loss: 5.884975433349609\nstep118, loss: 5.706509590148926\nstep119, loss: 5.883606910705566\nstep120, loss: 5.69465970993042\nstep121, loss: 5.505385875701904\nstep122, loss: 5.643381118774414\nstep123, loss: 5.76422643661499\nstep124, loss: 5.592621326446533\nstep125, loss: 5.60735559463501\nstep126, loss: 5.865959167480469\nstep127, loss: 5.739822864532471\nstep128, loss: 5.610042572021484\nstep129, loss: 5.59467077255249\nstep130, loss: 5.62123966217041\nstep131, loss: 5.5217461585998535\nstep132, loss: 5.65207576751709\nstep133, loss: 5.595767974853516\nstep134, loss: 6.0184831619262695\nstep135, loss: 5.939815044403076\nstep136, loss: 5.728033542633057\nstep137, loss: 5.867696762084961\nstep138, loss: 6.172586917877197\nstep139, loss: 6.038339138031006\nstep140, loss: 5.720555305480957\nstep141, loss: 5.84726619720459\nstep142, loss: 5.726109981536865\nstep143, loss: 5.662861347198486\nstep144, loss: 5.69563102722168\nstep145, loss: 5.583279609680176\nstep146, loss: 5.448187351226807\nstep147, loss: 5.64174222946167\nstep148, loss: 5.742843151092529\nstep149, loss: 5.6428046226501465\nstep150, loss: 5.603615760803223\nstep151, loss: 5.400905132293701\nstep152, loss: 5.295141696929932\nstep153, loss: 5.85046911239624\nstep154, loss: 5.649561405181885\nstep155, loss: 5.422387599945068\nstep156, loss: 5.477815628051758\nstep157, loss: 5.6294050216674805\nstep158, loss: 5.459655284881592\nstep159, loss: 5.19281005859375\nstep160, loss: 5.168211460113525\nstep161, loss: 5.197029113769531\nstep162, loss: 5.907796859741211\nstep163, loss: 5.754588603973389\nstep164, loss: 5.790070056915283\nstep165, loss: 5.7406768798828125\nstep166, loss: 5.791325092315674\nstep167, loss: 5.78136682510376\nstep168, loss: 5.516982555389404\nstep169, loss: 5.323155403137207\nstep170, loss: 5.328166484832764\nstep171, loss: 5.2958526611328125\nstep172, loss: 5.415927410125732\nstep173, loss: 5.278221130371094\nstep174, loss: 5.518054008483887\nstep175, loss: 5.556090354919434\nstep176, loss: 5.627833366394043\nstep177, loss: 5.617954254150391\nstep178, loss: 5.446441173553467\nstep179, loss: 5.468822002410889\nstep180, loss: 5.251683235168457\nstep181, loss: 5.241332530975342\nstep182, loss: 5.5065083503723145\nstep183, loss: 5.339474678039551\nstep184, loss: 5.601737976074219\nstep185, loss: 5.322931289672852\nstep186, loss: 5.581211566925049\nstep187, loss: 5.6549248695373535\nstep188, loss: 5.724498748779297\nstep189, loss: 5.695464134216309\nstep190, loss: 5.566058158874512\nstep191, loss: 5.596986293792725\nstep192, loss: 5.601629734039307\nstep193, loss: 5.4961676597595215\nstep194, loss: 5.36506462097168\nstep195, loss: 5.296258449554443\nstep196, loss: 5.403896331787109\nstep197, loss: 5.551340103149414\nstep198, loss: 5.606138229370117\nstep199, loss: 5.543200969696045\nstep200, loss: 5.378672122955322\nstep201, loss: 5.545451641082764\nstep202, loss: 5.359720230102539\nstep203, loss: 5.182342529296875\nstep204, loss: 5.320008754730225\nstep205, loss: 5.442966461181641\nstep206, loss: 5.262720108032227\nstep207, loss: 5.297779560089111\nstep208, loss: 5.580202102661133\nstep209, loss: 5.44003963470459\nstep210, loss: 5.331474781036377\nstep211, loss: 5.220547199249268\nstep212, loss: 5.309938907623291\nstep213, loss: 5.229709148406982\nstep214, loss: 5.364161968231201\nstep215, loss: 5.243493556976318\nstep216, loss: 5.649456024169922\nstep217, loss: 5.594463348388672\nstep218, loss: 5.419422626495361\nstep219, loss: 5.552110195159912\nstep220, loss: 5.858375549316406\nstep221, loss: 5.697390556335449\nstep222, loss: 5.3765788078308105\nstep223, loss: 5.487957954406738\nstep224, loss: 5.454689979553223\nstep225, loss: 5.362797260284424\nstep226, loss: 5.333270072937012\nstep227, loss: 5.182299613952637\nstep228, loss: 5.0890092849731445\nstep229, loss: 5.285878658294678\nstep230, loss: 5.34795618057251\nstep231, loss: 5.273802757263184\nstep232, loss: 5.260293483734131\nstep233, loss: 5.087549209594727\nstep234, loss: 4.964885234832764\nstep235, loss: 5.568811416625977\nstep236, loss: 5.326838970184326\nstep237, loss: 5.092920780181885\nstep238, loss: 5.134282112121582\nstep239, loss: 5.291452884674072\nstep240, loss: 5.1478424072265625\nstep241, loss: 4.872616291046143\nstep242, loss: 4.827140808105469\nstep243, loss: 4.8588762283325195\nstep244, loss: 5.580377101898193\nstep245, loss: 5.39666748046875\nstep246, loss: 5.477110862731934\nstep247, loss: 5.4054460525512695\nstep248, loss: 5.426384449005127\nstep249, loss: 5.417434215545654\nstep250, loss: 5.176507949829102\nstep251, loss: 4.928525447845459\nstep252, loss: 5.006063461303711\nstep253, loss: 4.97215461730957\nstep254, loss: 5.087414741516113\nstep255, loss: 4.996840953826904\nstep256, loss: 5.2298784255981445\nstep257, loss: 5.249208927154541\nstep258, loss: 5.298003673553467\nstep259, loss: 5.269013404846191\nstep260, loss: 5.1738810539245605\nstep261, loss: 5.186020374298096\nstep262, loss: 4.942826271057129\nstep263, loss: 4.930578708648682\nstep264, loss: 5.236754417419434\nstep265, loss: 4.996281623840332\nstep266, loss: 5.274547576904297\nstep267, loss: 4.9657673835754395\nstep268, loss: 5.32941198348999\nstep269, loss: 5.395144939422607\nstep270, loss: 5.465818405151367\nstep271, loss: 5.418356895446777\nstep272, loss: 5.286142826080322\nstep273, loss: 5.313915252685547\nstep274, loss: 5.3290019035339355\nstep275, loss: 5.241943359375\nstep276, loss: 5.080735683441162\nstep277, loss: 4.976221561431885\nstep278, loss: 5.159492015838623\nstep279, loss: 5.303632736206055\nstep280, loss: 5.362167835235596\nstep281, loss: 5.277499198913574\nstep282, loss: 5.121103763580322\nstep283, loss: 5.315608501434326\nstep284, loss: 5.141964435577393\nstep285, loss: 4.91872501373291\nstep286, loss: 5.079530239105225\nstep287, loss: 5.243354797363281\nstep288, loss: 5.023584842681885\nstep289, loss: 5.065398216247559\nstep290, loss: 5.377237319946289\nstep291, loss: 5.229753017425537\nstep292, loss: 5.106899261474609\nstep293, loss: 4.992732048034668\nstep294, loss: 5.115166664123535\nstep295, loss: 5.026949882507324\nstep296, loss: 5.133542537689209\nstep297, loss: 5.007135391235352\nstep298, loss: 5.451135635375977\nstep299, loss: 5.423740863800049\nstep300, loss: 5.245893478393555\nstep301, loss: 5.386501789093018\nstep302, loss: 5.6814751625061035\nstep303, loss: 5.526360988616943\nstep304, loss: 5.190845012664795\nstep305, loss: 5.282575607299805\nstep306, loss: 5.287626266479492\nstep307, loss: 5.172074317932129\nstep308, loss: 5.140886306762695\nstep309, loss: 4.999176979064941\nstep310, loss: 4.902903079986572\nstep311, loss: 5.077996730804443\nstep312, loss: 5.104382038116455\nstep313, loss: 5.044314384460449\nstep314, loss: 5.011754512786865\nstep315, loss: 4.82517147064209\nstep316, loss: 4.745498180389404\nstep317, loss: 5.408820629119873\nstep318, loss: 5.1061553955078125\nstep319, loss: 4.861540794372559\nstep320, loss: 4.908939361572266\nstep321, loss: 5.077474594116211\nstep322, loss: 4.9534173011779785\nstep323, loss: 4.66185998916626\nstep324, loss: 4.616915702819824\nstep325, loss: 4.668505668640137\nstep326, loss: 5.408797264099121\nstep327, loss: 5.213370323181152\nstep328, loss: 5.293381690979004\nstep329, loss: 5.223830699920654\nstep330, loss: 5.27882719039917\nstep331, loss: 5.229067325592041\nstep332, loss: 4.9814276695251465\nstep333, loss: 4.720749855041504\nstep334, loss: 4.816417217254639\nstep335, loss: 4.7766594886779785\nstep336, loss: 4.905428409576416\nstep337, loss: 4.8529839515686035\nstep338, loss: 5.079329490661621\nstep339, loss: 5.097559928894043\nstep340, loss: 5.077537536621094\nstep341, loss: 5.041193008422852\nstep342, loss: 4.9746994972229\nstep343, loss: 5.001473903656006\nstep344, loss: 4.744757175445557\nstep345, loss: 4.745173931121826\nstep346, loss: 5.0569000244140625\nstep347, loss: 4.782729148864746\nstep348, loss: 5.0484514236450195\nstep349, loss: 4.704106330871582\nstep350, loss: 5.155365943908691\nstep351, loss: 5.202834129333496\nstep352, loss: 5.28649377822876\nstep353, loss: 5.247432231903076\nstep354, loss: 5.091367244720459\nstep355, loss: 5.141539096832275\nstep356, loss: 5.157000541687012\nstep357, loss: 5.061526775360107\nstep358, loss: 4.905936241149902\nstep359, loss: 4.7994890213012695\nstep360, loss: 5.042752742767334\nstep361, loss: 5.157438278198242\nstep362, loss: 5.210440158843994\nstep363, loss: 5.116255283355713\nstep364, loss: 4.985633373260498\nstep365, loss: 5.191822052001953\nstep366, loss: 5.014328479766846\nstep367, loss: 4.802491664886475\nstep368, loss: 4.949883937835693\nstep369, loss: 5.125631809234619\nstep370, loss: 4.912807464599609\nstep371, loss: 4.938110828399658\nstep372, loss: 5.236014366149902\nstep373, loss: 5.0996174812316895\nstep374, loss: 4.977817535400391\nstep375, loss: 4.845583438873291\nstep376, loss: 4.981224060058594\nstep377, loss: 4.882203102111816\nstep378, loss: 4.9973368644714355\nstep379, loss: 4.866725444793701\nstep380, loss: 5.295748710632324\nstep381, loss: 5.2582879066467285\nstep382, loss: 5.123692035675049\nstep383, loss: 5.285062789916992\nstep384, loss: 5.53552770614624\nstep385, loss: 5.381439208984375\nstep386, loss: 5.03933048248291\nstep387, loss: 5.1412248611450195\nstep388, loss: 5.167886257171631\nstep389, loss: 5.054529190063477\nstep390, loss: 4.9974284172058105\nstep391, loss: 4.835385322570801\nstep392, loss: 4.743797779083252\nstep393, loss: 4.919191837310791\nstep394, loss: 4.947685718536377\nstep395, loss: 4.883925437927246\nstep396, loss: 4.867593765258789\nstep397, loss: 4.661460876464844\nstep398, loss: 4.5683135986328125\nstep399, loss: 5.263609409332275\nstep400, loss: 4.951760292053223\nstep401, loss: 4.723826885223389\nstep402, loss: 4.7495598793029785\nstep403, loss: 4.9144287109375\nstep404, loss: 4.804549217224121\nstep405, loss: 4.533120632171631\nstep406, loss: 4.479221820831299\nstep407, loss: 4.527514934539795\nstep408, loss: 5.270105838775635\nstep409, loss: 5.087458610534668\nstep410, loss: 5.156100273132324\nstep411, loss: 5.055835247039795\nstep412, loss: 5.133876800537109\nstep413, loss: 5.096149921417236\nstep414, loss: 4.867053508758545\nstep415, loss: 4.579586982727051\nstep416, loss: 4.67894172668457\nstep417, loss: 4.649348258972168\nstep418, loss: 4.767951965332031\nstep419, loss: 4.747584342956543\nstep420, loss: 4.985129356384277\nstep421, loss: 4.976273059844971\nstep422, loss: 4.930525302886963\nstep423, loss: 4.889464855194092\nstep424, loss: 4.857212066650391\nstep425, loss: 4.879665374755859\nstep426, loss: 4.6135945320129395\nstep427, loss: 4.628551006317139\nstep428, loss: 4.932689666748047\nstep429, loss: 4.615086555480957\nstep430, loss: 4.902871608734131\nstep431, loss: 4.530801773071289\nstep432, loss: 5.005734920501709\nstep433, loss: 5.05388069152832\nstep434, loss: 5.143333435058594\nstep435, loss: 5.107789993286133\nstep436, loss: 4.987463474273682\nstep437, loss: 5.0088791847229\nstep438, loss: 5.022647857666016\nstep439, loss: 4.9478559494018555\nstep440, loss: 4.783939361572266\nstep441, loss: 4.696188926696777\nstep442, loss: 4.9123077392578125\nstep443, loss: 5.058095932006836\nstep444, loss: 5.129269123077393\nstep445, loss: 5.028878688812256\nstep446, loss: 4.88606595993042\nstep447, loss: 5.0819902420043945\nstep448, loss: 4.90996789932251\nstep449, loss: 4.700469970703125\nstep450, loss: 4.846477508544922\nstep451, loss: 5.030203342437744\nstep452, loss: 4.828064918518066\nstep453, loss: 4.874241828918457\nstep454, loss: 5.15292501449585\nstep455, loss: 4.993437767028809\nstep456, loss: 4.88808536529541\nstep457, loss: 4.746622562408447\nstep458, loss: 4.891971111297607\nstep459, loss: 4.771591663360596\nstep460, loss: 4.877567291259766\nstep461, loss: 4.771111488342285\nstep462, loss: 5.220290660858154\nstep463, loss: 5.191657543182373\nstep464, loss: 5.006119728088379\nstep465, loss: 5.191192150115967\nstep466, loss: 5.4563164710998535\nstep467, loss: 5.293860912322998\nstep468, loss: 4.950766086578369\nstep469, loss: 5.04711389541626\nstep470, loss: 5.0886149406433105\nstep471, loss: 4.9813456535339355\nstep472, loss: 4.964775085449219\nstep473, loss: 4.759293556213379\nstep474, loss: 4.641078948974609\nstep475, loss: 4.837125778198242\nstep476, loss: 4.865488052368164\nstep477, loss: 4.771239757537842\nstep478, loss: 4.788158416748047\nstep479, loss: 4.604198455810547\nstep480, loss: 4.506280899047852\nstep481, loss: 5.186888694763184\nstep482, loss: 4.856972694396973\nstep483, loss: 4.624401569366455\nstep484, loss: 4.61943244934082\nstep485, loss: 4.802449703216553\nstep486, loss: 4.732987403869629\nstep487, loss: 4.423978328704834\nstep488, loss: 4.358293533325195\nstep489, loss: 4.420368194580078\nstep490, loss: 5.182331085205078\nstep491, loss: 4.9688520431518555\nstep492, loss: 5.0727996826171875\nstep493, loss: 4.984377384185791\nstep494, loss: 5.02700662612915\nstep495, loss: 5.003410339355469\nstep496, loss: 4.787106513977051\nstep497, loss: 4.516664028167725\nstep498, loss: 4.592520713806152\nstep499, loss: 4.546041488647461\nstep500, loss: 4.667715549468994\nstep501, loss: 4.663778781890869\nstep502, loss: 4.890886306762695\nstep503, loss: 4.879623889923096\nstep504, loss: 4.802773952484131\nstep505, loss: 4.781538009643555\nstep506, loss: 4.759068965911865\nstep507, loss: 4.784724235534668\nstep508, loss: 4.487499713897705\nstep509, loss: 4.541093826293945\nstep510, loss: 4.854321002960205\nstep511, loss: 4.527918338775635\nstep512, loss: 4.798818588256836\nstep513, loss: 4.408881187438965\nstep514, loss: 4.933640956878662\nstep515, loss: 4.982730388641357\nstep516, loss: 5.057807445526123\nstep517, loss: 5.028980255126953\nstep518, loss: 4.881299018859863\nstep519, loss: 4.913389682769775\nstep520, loss: 4.930668354034424\nstep521, loss: 4.875001430511475\nstep522, loss: 4.716433048248291\nstep523, loss: 4.603691101074219\nstep524, loss: 4.857044696807861\nstep525, loss: 4.984563827514648\nstep526, loss: 5.042563438415527\nstep527, loss: 4.94265604019165\nstep528, loss: 4.833492279052734\nstep529, loss: 4.996094226837158\nstep530, loss: 4.802929878234863\nstep531, loss: 4.619167804718018\nstep532, loss: 4.759705066680908\nstep533, loss: 4.953742980957031\nstep534, loss: 4.7492356300354\nstep535, loss: 4.791794776916504\nstep536, loss: 5.052242279052734\nstep537, loss: 4.920673370361328\nstep538, loss: 4.8238067626953125\nstep539, loss: 4.63567590713501\nstep540, loss: 4.782495498657227\nstep541, loss: 4.691050052642822\nstep542, loss: 4.783335208892822\nstep543, loss: 4.666501998901367\nstep544, loss: 5.1254472732543945\nstep545, loss: 5.104849815368652\nstep546, loss: 4.928590774536133\nstep547, loss: 5.113306045532227\nstep548, loss: 5.367923736572266\nstep549, loss: 5.228099346160889\nstep550, loss: 4.906392574310303\nstep551, loss: 4.970846652984619\nstep552, loss: 5.0267534255981445\nstep553, loss: 4.90440034866333\nstep554, loss: 4.840042591094971\nstep555, loss: 4.66873836517334\nstep556, loss: 4.593924045562744\nstep557, loss: 4.769440174102783\nstep558, loss: 4.770891189575195\nstep559, loss: 4.699679851531982\nstep560, loss: 4.679271221160889\nstep561, loss: 4.513920307159424\nstep562, loss: 4.426713943481445\nstep563, loss: 5.111876010894775\nstep564, loss: 4.820734977722168\nstep565, loss: 4.577290058135986\nstep566, loss: 4.568022727966309\nstep567, loss: 4.7685112953186035\nstep568, loss: 4.655099391937256\nstep569, loss: 4.369568824768066\nstep570, loss: 4.308559894561768\nstep571, loss: 4.352205276489258\nstep572, loss: 5.083970069885254\nstep573, loss: 4.89475679397583\nstep574, loss: 5.018774032592773\nstep575, loss: 4.9194655418396\nstep576, loss: 4.972379207611084\nstep577, loss: 4.925865173339844\nstep578, loss: 4.707587718963623\nstep579, loss: 4.40730619430542\nstep580, loss: 4.5380401611328125\nstep581, loss: 4.493852138519287\nstep582, loss: 4.590334892272949\nstep583, loss: 4.608147621154785\nstep584, loss: 4.821328163146973\nstep585, loss: 4.805550575256348\nstep586, loss: 4.750408172607422\nstep587, loss: 4.722982406616211\nstep588, loss: 4.6752824783325195\nstep589, loss: 4.7515153884887695\nstep590, loss: 4.455727577209473\nstep591, loss: 4.507741928100586\nstep592, loss: 4.8043532371521\nstep593, loss: 4.448171615600586\nstep594, loss: 4.7307939529418945\nstep595, loss: 4.328011989593506\nstep596, loss: 4.86754846572876\nstep597, loss: 4.919267177581787\nstep598, loss: 5.013148784637451\nstep599, loss: 4.982151031494141\nstep600, loss: 4.826055526733398\nstep601, loss: 4.867725849151611\nstep602, loss: 4.869476318359375\nstep603, loss: 4.827740669250488\nstep604, loss: 4.642739772796631\nstep605, loss: 4.5465497970581055\nstep606, loss: 4.812036991119385\nstep607, loss: 4.945847511291504\nstep608, loss: 4.977872371673584\nstep609, loss: 4.915429592132568\nstep610, loss: 4.807206153869629\nstep611, loss: 4.979292869567871\nstep612, loss: 4.796187400817871\nstep613, loss: 4.629963397979736\nstep614, loss: 4.708553314208984\nstep615, loss: 4.914901256561279\nstep616, loss: 4.705793380737305\nstep617, loss: 4.745131015777588\nstep618, loss: 5.0257439613342285\nstep619, loss: 4.879213809967041\nstep620, loss: 4.762631416320801\nstep621, loss: 4.600656032562256\nstep622, loss: 4.745039463043213\nstep623, loss: 4.650032043457031\nstep624, loss: 4.744938850402832\nstep625, loss: 4.6060709953308105\nstep626, loss: 5.043670177459717\nstep627, loss: 5.0422139167785645\nstep628, loss: 4.898303985595703\nstep629, loss: 5.067107677459717\nstep630, loss: 5.302998065948486\nstep631, loss: 5.155196666717529\nstep632, loss: 4.831052780151367\nstep633, loss: 4.910099506378174\nstep634, loss: 4.974177837371826\nstep635, loss: 4.855804443359375\nstep636, loss: 4.774774074554443\nstep637, loss: 4.648853302001953\nstep638, loss: 4.544731140136719\nstep639, loss: 4.706736087799072\nstep640, loss: 4.743273735046387\nstep641, loss: 4.657865524291992\nstep642, loss: 4.648523330688477\nstep643, loss: 4.4789886474609375\nstep644, loss: 4.399593353271484\nstep645, loss: 5.094346523284912\nstep646, loss: 4.767444133758545\nstep647, loss: 4.549971103668213\nstep648, loss: 4.543457508087158\nstep649, loss: 4.7233710289001465\nstep650, loss: 4.618917465209961\nstep651, loss: 4.323737621307373\nstep652, loss: 4.283729076385498\nstep653, loss: 4.333073139190674\nstep654, loss: 5.056782245635986\nstep655, loss: 4.8646087646484375\nstep656, loss: 4.992099285125732\nstep657, loss: 4.896154403686523\nstep658, loss: 4.946857929229736\nstep659, loss: 4.917751312255859\nstep660, loss: 4.68033504486084\nstep661, loss: 4.4034905433654785\nstep662, loss: 4.514186859130859\nstep663, loss: 4.468272686004639\nstep664, loss: 4.580087661743164\nstep665, loss: 4.5922932624816895\nstep666, loss: 4.7986626625061035\nstep667, loss: 4.7882843017578125\nstep668, loss: 4.748556137084961\nstep669, loss: 4.718825340270996\nstep670, loss: 4.68690824508667\nstep671, loss: 4.7313618659973145\nstep672, loss: 4.399923324584961\nstep673, loss: 4.449150085449219\nstep674, loss: 4.789294719696045\nstep675, loss: 4.452115058898926\nstep676, loss: 4.702836036682129\nstep677, loss: 4.34069299697876\nstep678, loss: 4.834127902984619\nstep679, loss: 4.88256311416626\nstep680, loss: 4.963945388793945\nstep681, loss: 4.949573993682861\nstep682, loss: 4.787781715393066\nstep683, loss: 4.836726188659668\nstep684, loss: 4.837735652923584\nstep685, loss: 4.7692341804504395\nstep686, loss: 4.589790344238281\nstep687, loss: 4.5138068199157715\nstep688, loss: 4.76636266708374\nstep689, loss: 4.927788734436035\nstep690, loss: 4.926015853881836\nstep691, loss: 4.842274188995361\nstep692, loss: 4.7323479652404785\nstep693, loss: 4.90871524810791\nstep694, loss: 4.744569301605225\nstep695, loss: 4.529295444488525\nstep696, loss: 4.642326354980469\nstep697, loss: 4.87468957901001\nstep698, loss: 4.63657283782959\nstep699, loss: 4.644340515136719\nstep700, loss: 4.943599700927734\nstep701, loss: 4.802139759063721\nstep702, loss: 4.719204425811768\nstep703, loss: 4.544157028198242\nstep704, loss: 4.699887275695801\nstep705, loss: 4.6008992195129395\nstep706, loss: 4.6821088790893555\nstep707, loss: 4.548127174377441\nstep708, loss: 5.0049638748168945\nstep709, loss: 4.963129043579102\nstep710, loss: 4.812961578369141\nstep711, loss: 5.005393028259277\nstep712, loss: 5.277016639709473\nstep713, loss: 5.129566669464111\nstep714, loss: 4.817150115966797\nstep715, loss: 4.863191604614258\nstep716, loss: 4.941586971282959\nstep717, loss: 4.821353435516357\nstep718, loss: 4.72300910949707\nstep719, loss: 4.569165229797363\nstep720, loss: 4.494848728179932\nstep721, loss: 4.645695209503174\nstep722, loss: 4.7198920249938965\nstep723, loss: 4.609705924987793\nstep724, loss: 4.58722448348999\nstep725, loss: 4.388182640075684\nstep726, loss: 4.335520267486572\nstep727, loss: 5.043252468109131\nstep728, loss: 4.7299041748046875\nstep729, loss: 4.4772725105285645\nstep730, loss: 4.4577178955078125\nstep731, loss: 4.609093189239502\nstep732, loss: 4.532883167266846\nstep733, loss: 4.250973224639893\nstep734, loss: 4.184654712677002\nstep735, loss: 4.231742858886719\nstep736, loss: 4.926517009735107\nstep737, loss: 4.767035007476807\nstep738, loss: 4.863563537597656\nstep739, loss: 4.772763252258301\nstep740, loss: 4.83025598526001\nstep741, loss: 4.802505970001221\nstep742, loss: 4.582274436950684\nstep743, loss: 4.315682411193848\nstep744, loss: 4.414228439331055\nstep745, loss: 4.374759674072266\nstep746, loss: 4.473291873931885\nstep747, loss: 4.514491081237793\nstep748, loss: 4.719951629638672\nstep749, loss: 4.6851067543029785\nstep750, loss: 4.607123851776123\nstep751, loss: 4.597960948944092\nstep752, loss: 4.564478397369385\nstep753, loss: 4.629974842071533\nstep754, loss: 4.299257755279541\nstep755, loss: 4.345842361450195\nstep756, loss: 4.700650215148926\nstep757, loss: 4.30516242980957\nstep758, loss: 4.53930139541626\nstep759, loss: 4.181723117828369\nstep760, loss: 4.762938976287842\nstep761, loss: 4.819856643676758\nstep762, loss: 4.880141258239746\nstep763, loss: 4.850192070007324\nstep764, loss: 4.69829797744751\nstep765, loss: 4.731827259063721\nstep766, loss: 4.733254432678223\nstep767, loss: 4.704668045043945\nstep768, loss: 4.526876926422119\nstep769, loss: 4.41519021987915\nstep770, loss: 4.618078231811523\nstep771, loss: 4.77847957611084\nstep772, loss: 4.8386030197143555\nstep773, loss: 4.748517036437988\nstep774, loss: 4.630310535430908\nstep775, loss: 4.785423755645752\nstep776, loss: 4.636375904083252\nstep777, loss: 4.4693145751953125\nstep778, loss: 4.569173336029053\nstep779, loss: 4.804964542388916\nstep780, loss: 4.573677062988281\nstep781, loss: 4.631485462188721\nstep782, loss: 4.893244743347168\nstep783, loss: 4.7484869956970215\nstep784, loss: 4.6383562088012695\nstep785, loss: 4.44938850402832\nstep786, loss: 4.611327648162842\nstep787, loss: 4.526081562042236\nstep788, loss: 4.634369850158691\nstep789, loss: 4.494580268859863\nstep790, loss: 4.972926139831543\nstep791, loss: 4.921520233154297\nstep792, loss: 4.791925430297852\nstep793, loss: 4.974104404449463\nstep794, loss: 5.202996730804443\nstep795, loss: 5.04340934753418\nstep796, loss: 4.755976676940918\nstep797, loss: 4.782657146453857\nstep798, loss: 4.900804042816162\nstep799, loss: 4.768698215484619\nstep800, loss: 4.641172409057617\nstep801, loss: 4.50599479675293\nstep802, loss: 4.412359237670898\nstep803, loss: 4.550910472869873\nstep804, loss: 4.596305847167969\nstep805, loss: 4.523650169372559\nstep806, loss: 4.5074782371521\nstep807, loss: 4.306678295135498\nstep808, loss: 4.240830421447754\nstep809, loss: 4.931400299072266\nstep810, loss: 4.614820957183838\nstep811, loss: 4.387354850769043\nstep812, loss: 4.3798136711120605\nstep813, loss: 4.526824951171875\nstep814, loss: 4.434131145477295\nstep815, loss: 4.151790142059326\nstep816, loss: 4.118501663208008\nstep817, loss: 4.186635494232178\nstep818, loss: 4.84963846206665\nstep819, loss: 4.6634087562561035\nstep820, loss: 4.769437313079834\nstep821, loss: 4.641087532043457\nstep822, loss: 4.741724967956543\nstep823, loss: 4.699051380157471\nstep824, loss: 4.516147136688232\nstep825, loss: 4.248847961425781\nstep826, loss: 4.3618597984313965\nstep827, loss: 4.31365442276001\nstep828, loss: 4.410647392272949\nstep829, loss: 4.454367160797119\nstep830, loss: 4.660505771636963\nstep831, loss: 4.612610816955566\nstep832, loss: 4.487514495849609\nstep833, loss: 4.519176483154297\nstep834, loss: 4.4837188720703125\nstep835, loss: 4.555960655212402\nstep836, loss: 4.229538917541504\nstep837, loss: 4.280720233917236\nstep838, loss: 4.651611328125\nstep839, loss: 4.244154930114746\nstep840, loss: 4.490037441253662\nstep841, loss: 4.118631839752197\nstep842, loss: 4.665299892425537\nstep843, loss: 4.718356609344482\nstep844, loss: 4.7925214767456055\nstep845, loss: 4.788551330566406\nstep846, loss: 4.64251708984375\nstep847, loss: 4.672672271728516\nstep848, loss: 4.648970603942871\nstep849, loss: 4.636336803436279\nstep850, loss: 4.471214771270752\nstep851, loss: 4.3644304275512695\nstep852, loss: 4.558320999145508\nstep853, loss: 4.685388088226318\nstep854, loss: 4.745193958282471\nstep855, loss: 4.682106018066406\nstep856, loss: 4.570249080657959\nstep857, loss: 4.735193729400635\nstep858, loss: 4.568632125854492\nstep859, loss: 4.376629829406738\nstep860, loss: 4.498979568481445\nstep861, loss: 4.740828514099121\nstep862, loss: 4.500979900360107\nstep863, loss: 4.553162097930908\nstep864, loss: 4.8403801918029785\nstep865, loss: 4.714241981506348\nstep866, loss: 4.599463939666748\nstep867, loss: 4.415769100189209\nstep868, loss: 4.554658889770508\nstep869, loss: 4.485118865966797\nstep870, loss: 4.597505569458008\nstep871, loss: 4.491000175476074\nstep872, loss: 4.94834041595459\nstep873, loss: 4.854618549346924\nstep874, loss: 4.704125881195068\nstep875, loss: 4.893789291381836\nstep876, loss: 5.131497383117676\nstep877, loss: 4.998225212097168\nstep878, loss: 4.710600852966309\nstep879, loss: 4.739215850830078\nstep880, loss: 4.84782600402832\nstep881, loss: 4.730610370635986\nstep882, loss: 4.598989486694336\nstep883, loss: 4.448097229003906\nstep884, loss: 4.406540393829346\nstep885, loss: 4.529671669006348\nstep886, loss: 4.51964807510376\nstep887, loss: 4.4563446044921875\nstep888, loss: 4.462062835693359\nstep889, loss: 4.284713268280029\nstep890, loss: 4.236588001251221\nstep891, loss: 4.895709037780762\nstep892, loss: 4.565920829772949\nstep893, loss: 4.2945380210876465\nstep894, loss: 4.295229911804199\nstep895, loss: 4.477736949920654\nstep896, loss: 4.431408882141113\nstep897, loss: 4.153337478637695\nstep898, loss: 4.059443950653076\nstep899, loss: 4.132327079772949\nstep900, loss: 4.828786373138428\nstep901, loss: 4.660360813140869\nstep902, loss: 4.736362457275391\nstep903, loss: 4.589354991912842\nstep904, loss: 4.6750407218933105\nstep905, loss: 4.624154567718506\nstep906, loss: 4.467225551605225\nstep907, loss: 4.173811912536621\nstep908, loss: 4.304011344909668\nstep909, loss: 4.270740985870361\nstep910, loss: 4.3590617179870605\nstep911, loss: 4.397202014923096\nstep912, loss: 4.610562324523926\nstep913, loss: 4.561692714691162\nstep914, loss: 4.451837539672852\nstep915, loss: 4.496272087097168\nstep916, loss: 4.470815658569336\nstep917, loss: 4.497378349304199\nstep918, loss: 4.173038482666016\nstep919, loss: 4.247562408447266\nstep920, loss: 4.607168197631836\nstep921, loss: 4.193838119506836\nstep922, loss: 4.462210178375244\nstep923, loss: 4.079516887664795\nstep924, loss: 4.627090930938721\nstep925, loss: 4.677700996398926\nstep926, loss: 4.732908248901367\nstep927, loss: 4.723596572875977\nstep928, loss: 4.567184925079346\nstep929, loss: 4.6392927169799805\nstep930, loss: 4.616176128387451\nstep931, loss: 4.608171463012695\nstep932, loss: 4.423409461975098\nstep933, loss: 4.298652172088623\nstep934, loss: 4.516352653503418\nstep935, loss: 4.687535285949707\nstep936, loss: 4.715577125549316\nstep937, loss: 4.65899133682251\nstep938, loss: 4.520187854766846\nstep939, loss: 4.6962361335754395\nstep940, loss: 4.536400318145752\nstep941, loss: 4.327625751495361\nstep942, loss: 4.445268154144287\nstep943, loss: 4.683004379272461\nstep944, loss: 4.442257404327393\nstep945, loss: 4.493485450744629\nstep946, loss: 4.782956600189209\nstep947, loss: 4.641793727874756\nstep948, loss: 4.541808128356934\nstep949, loss: 4.326005458831787\nstep950, loss: 4.516903877258301\nstep951, loss: 4.43277645111084\nstep952, loss: 4.529378414154053\nstep953, loss: 4.401229381561279\nstep954, loss: 4.85748291015625\nstep955, loss: 4.822135925292969\nstep956, loss: 4.716280460357666\nstep957, loss: 4.864293575286865\nstep958, loss: 5.122661590576172\nstep959, loss: 4.922939300537109\nstep960, loss: 4.621760368347168\nstep961, loss: 4.695781707763672\nstep962, loss: 4.828542709350586\nstep963, loss: 4.69848108291626\nstep964, loss: 4.543313026428223\nstep965, loss: 4.414172172546387\nstep966, loss: 4.354492664337158\nstep967, loss: 4.497550010681152\nstep968, loss: 4.49955940246582\nstep969, loss: 4.437707901000977\nstep970, loss: 4.426699638366699\nstep971, loss: 4.226509094238281\nstep972, loss: 4.20182466506958\nstep973, loss: 4.858132362365723\nstep974, loss: 4.588235378265381\nstep975, loss: 4.307260990142822\nstep976, loss: 4.29177713394165\nstep977, loss: 4.429538726806641\nstep978, loss: 4.350316524505615\nstep979, loss: 4.0757575035095215\nstep980, loss: 4.010030269622803\nstep981, loss: 4.085359573364258\nstep982, loss: 4.777955532073975\nstep983, loss: 4.589845657348633\nstep984, loss: 4.660903453826904\nstep985, loss: 4.539139747619629\nstep986, loss: 4.6113176345825195\nstep987, loss: 4.542645454406738\nstep988, loss: 4.402236461639404\nstep989, loss: 4.120826721191406\nstep990, loss: 4.253322124481201\nstep991, loss: 4.211380481719971\nstep992, loss: 4.2645111083984375\nstep993, loss: 4.357818126678467\nstep994, loss: 4.566809177398682\nstep995, loss: 4.525540351867676\nstep996, loss: 4.410791873931885\nstep997, loss: 4.440652370452881\nstep998, loss: 4.374192237854004\nstep999, loss: 4.489503860473633\nstep1000, loss: 4.146468162536621\nstep1001, loss: 4.194983959197998\nstep1002, loss: 4.542898654937744\nstep1003, loss: 4.147315979003906\nstep1004, loss: 4.417407035827637\nstep1005, loss: 4.039425373077393\nstep1006, loss: 4.577582359313965\nstep1007, loss: 4.601588726043701\nstep1008, loss: 4.684523582458496\nstep1009, loss: 4.672873020172119\nstep1010, loss: 4.520606994628906\nstep1011, loss: 4.603470325469971\nstep1012, loss: 4.583626747131348\nstep1013, loss: 4.541755676269531\nstep1014, loss: 4.357464790344238\nstep1015, loss: 4.264361381530762\nstep1016, loss: 4.501275062561035\nstep1017, loss: 4.6562676429748535\nstep1018, loss: 4.666474342346191\nstep1019, loss: 4.598260879516602\nstep1020, loss: 4.46646785736084\nstep1021, loss: 4.635035514831543\nstep1022, loss: 4.4763569831848145\nstep1023, loss: 4.277195930480957\nstep1024, loss: 4.405356407165527\nstep1025, loss: 4.6367902755737305\nstep1026, loss: 4.389867782592773\nstep1027, loss: 4.444818496704102\nstep1028, loss: 4.744233131408691\nstep1029, loss: 4.603839874267578\nstep1030, loss: 4.52346658706665\nstep1031, loss: 4.3024468421936035\nstep1032, loss: 4.484548091888428\nstep1033, loss: 4.38324499130249\nstep1034, loss: 4.479950428009033\nstep1035, loss: 4.336355209350586\nstep1036, loss: 4.762572288513184\nstep1037, loss: 4.765293598175049\nstep1038, loss: 4.628791332244873\nstep1039, loss: 4.780825614929199\nstep1040, loss: 5.0758514404296875\nstep1041, loss: 4.888493537902832\nstep1042, loss: 4.60006046295166\nstep1043, loss: 4.657390117645264\nstep1044, loss: 4.733814239501953\nstep1045, loss: 4.622460842132568\nstep1046, loss: 4.547305107116699\nstep1047, loss: 4.373291492462158\nstep1048, loss: 4.34658670425415\nstep1049, loss: 4.464391708374023\nstep1050, loss: 4.508823871612549\nstep1051, loss: 4.402447700500488\nstep1052, loss: 4.391965866088867\nstep1053, loss: 4.179557800292969\nstep1054, loss: 4.143245697021484\nstep1055, loss: 4.77596378326416\nstep1056, loss: 4.473663806915283\nstep1057, loss: 4.244501113891602\nstep1058, loss: 4.262058258056641\nstep1059, loss: 4.395700454711914\nstep1060, loss: 4.306122779846191\nstep1061, loss: 4.031028747558594\nstep1062, loss: 3.985332727432251\nstep1063, loss: 4.033134460449219\nstep1064, loss: 4.7455925941467285\nstep1065, loss: 4.535067558288574\nstep1066, loss: 4.640854358673096\nstep1067, loss: 4.496003150939941\nstep1068, loss: 4.568878650665283\nstep1069, loss: 4.516850471496582\nstep1070, loss: 4.358434677124023\nstep1071, loss: 4.072016716003418\nstep1072, loss: 4.210355281829834\nstep1073, loss: 4.157446384429932\nstep1074, loss: 4.219631671905518\nstep1075, loss: 4.327086448669434\nstep1076, loss: 4.5316948890686035\nstep1077, loss: 4.4352006912231445\nstep1078, loss: 4.3121562004089355\nstep1079, loss: 4.383787155151367\nstep1080, loss: 4.358534812927246\nstep1081, loss: 4.494302272796631\nstep1082, loss: 4.143462181091309\nstep1083, loss: 4.163975238800049\nstep1084, loss: 4.518198013305664\nstep1085, loss: 4.097474575042725\nstep1086, loss: 4.367420673370361\nstep1087, loss: 3.9810056686401367\nstep1088, loss: 4.520294666290283\nstep1089, loss: 4.5616068840026855\nstep1090, loss: 4.638742923736572\nstep1091, loss: 4.6322550773620605\nstep1092, loss: 4.464053153991699\nstep1093, loss: 4.5460076332092285\nstep1094, loss: 4.547087669372559\nstep1095, loss: 4.527902126312256\nstep1096, loss: 4.351533889770508\nstep1097, loss: 4.23574161529541\nstep1098, loss: 4.440798282623291\nstep1099, loss: 4.627820014953613\nstep1100, loss: 4.654911994934082\nstep1101, loss: 4.588021278381348\nstep1102, loss: 4.483206748962402\nstep1103, loss: 4.632728099822998\nstep1104, loss: 4.463123321533203\nstep1105, loss: 4.267324447631836\nstep1106, loss: 4.368647575378418\nstep1107, loss: 4.596585273742676\nstep1108, loss: 4.3512091636657715\nstep1109, loss: 4.4113359451293945\nstep1110, loss: 4.7017717361450195\nstep1111, loss: 4.575967311859131\nstep1112, loss: 4.4867095947265625\nstep1113, loss: 4.208932876586914\nstep1114, loss: 4.421379566192627\nstep1115, loss: 4.324494361877441\nstep1116, loss: 4.432701587677002\nstep1117, loss: 4.291432857513428\nstep1118, loss: 4.692366600036621\nstep1119, loss: 4.645298004150391\nstep1120, loss: 4.529476165771484\nstep1121, loss: 4.712680816650391\nstep1122, loss: 5.051452159881592\nstep1123, loss: 4.834226131439209\nstep1124, loss: 4.530832767486572\nstep1125, loss: 4.613595008850098\nstep1126, loss: 4.713093280792236\nstep1127, loss: 4.620732307434082\nstep1128, loss: 4.503990650177002\nstep1129, loss: 4.310673713684082\nstep1130, loss: 4.236607074737549\nstep1131, loss: 4.375180244445801\nstep1132, loss: 4.4964752197265625\nstep1133, loss: 4.397944927215576\nstep1134, loss: 4.370805740356445\nstep1135, loss: 4.130748748779297\nstep1136, loss: 4.112297058105469\nstep1137, loss: 4.761031150817871\nstep1138, loss: 4.517945289611816\nstep1139, loss: 4.285353660583496\nstep1140, loss: 4.25531530380249\nstep1141, loss: 4.404774188995361\nstep1142, loss: 4.288475513458252\nstep1143, loss: 4.049431800842285\nstep1144, loss: 4.031803131103516\nstep1145, loss: 4.067605018615723\nstep1146, loss: 4.734703540802002\nstep1147, loss: 4.552669525146484\nstep1148, loss: 4.692187309265137\nstep1149, loss: 4.609408855438232\nstep1150, loss: 4.646457672119141\nstep1151, loss: 4.639377593994141\nstep1152, loss: 4.443508148193359\nstep1153, loss: 4.173353672027588\nstep1154, loss: 4.248334884643555\nstep1155, loss: 4.201789379119873\nstep1156, loss: 4.288390159606934\nstep1157, loss: 4.327683448791504\nstep1158, loss: 4.530779838562012\nstep1159, loss: 4.467559337615967\nstep1160, loss: 4.357129096984863\nstep1161, loss: 4.438947677612305\nstep1162, loss: 4.4157514572143555\nstep1163, loss: 4.505681037902832\nstep1164, loss: 4.182579040527344\nstep1165, loss: 4.222558975219727\nstep1166, loss: 4.553971767425537\nstep1167, loss: 4.2005767822265625\nstep1168, loss: 4.403240203857422\nstep1169, loss: 4.022402286529541\nstep1170, loss: 4.568361282348633\nstep1171, loss: 4.626823425292969\nstep1172, loss: 4.656230449676514\nstep1173, loss: 4.607924461364746\nstep1174, loss: 4.473282814025879\nstep1175, loss: 4.545891284942627\nstep1176, loss: 4.511194229125977\nstep1177, loss: 4.481203079223633\nstep1178, loss: 4.295833587646484\nstep1179, loss: 4.226337432861328\nstep1180, loss: 4.459132671356201\nstep1181, loss: 4.671414852142334\nstep1182, loss: 4.660900592803955\nstep1183, loss: 4.585203170776367\nstep1184, loss: 4.465542793273926\nstep1185, loss: 4.572689056396484\nstep1186, loss: 4.452401161193848\nstep1187, loss: 4.260614395141602\nstep1188, loss: 4.352621078491211\nstep1189, loss: 4.5501627922058105\nstep1190, loss: 4.3094072341918945\nstep1191, loss: 4.364508628845215\nstep1192, loss: 4.693811893463135\nstep1193, loss: 4.559847831726074\nstep1194, loss: 4.460858345031738\nstep1195, loss: 4.172687530517578\nstep1196, loss: 4.405067443847656\nstep1197, loss: 4.337308883666992\nstep1198, loss: 4.439262390136719\nstep1199, loss: 4.277839660644531\nstep1200, loss: 4.648126602172852\nstep1201, loss: 4.579469203948975\nstep1202, loss: 4.4920477867126465\nstep1203, loss: 4.640937805175781\nstep1204, loss: 4.933251857757568\nstep1205, loss: 4.752600193023682\nstep1206, loss: 4.517746448516846\nstep1207, loss: 4.655627727508545\nstep1208, loss: 4.675635814666748\nstep1209, loss: 4.545772075653076\nstep1210, loss: 4.407630443572998\nstep1211, loss: 4.303905963897705\nstep1212, loss: 4.288635730743408\nstep1213, loss: 4.399781227111816\nstep1214, loss: 4.40850830078125\nstep1215, loss: 4.286553382873535\nstep1216, loss: 4.2812819480896\nstep1217, loss: 4.110572814941406\nstep1218, loss: 4.10300350189209\nstep1219, loss: 4.778234481811523\nstep1220, loss: 4.476667404174805\nstep1221, loss: 4.214744567871094\nstep1222, loss: 4.199071884155273\nstep1223, loss: 4.39417028427124\nstep1224, loss: 4.281548500061035\nstep1225, loss: 4.0271735191345215\nstep1226, loss: 3.9691109657287598\nstep1227, loss: 3.996697425842285\nstep1228, loss: 4.658744812011719\nstep1229, loss: 4.520827770233154\nstep1230, loss: 4.64335298538208\nstep1231, loss: 4.53451681137085\nstep1232, loss: 4.56533145904541\nstep1233, loss: 4.571134090423584\nstep1234, loss: 4.413594722747803\nstep1235, loss: 4.155409812927246\nstep1236, loss: 4.221270561218262\nstep1237, loss: 4.16180419921875\nstep1238, loss: 4.213640213012695\nstep1239, loss: 4.302342891693115\nstep1240, loss: 4.4930267333984375\nstep1241, loss: 4.384428024291992\nstep1242, loss: 4.2227678298950195\nstep1243, loss: 4.31577730178833\nstep1244, loss: 4.29978084564209\nstep1245, loss: 4.408127307891846\nstep1246, loss: 4.074521541595459\nstep1247, loss: 4.125001907348633\nstep1248, loss: 4.484589099884033\nstep1249, loss: 4.123737812042236\nstep1250, loss: 4.356056213378906\nstep1251, loss: 3.984694242477417\nstep1252, loss: 4.490767478942871\nstep1253, loss: 4.5410943031311035\nstep1254, loss: 4.615134239196777\nstep1255, loss: 4.582309722900391\nstep1256, loss: 4.4842987060546875\nstep1257, loss: 4.562129974365234\nstep1258, loss: 4.5116071701049805\nstep1259, loss: 4.475706100463867\nstep1260, loss: 4.309427261352539\nstep1261, loss: 4.212939262390137\nstep1262, loss: 4.413790702819824\nstep1263, loss: 4.586757183074951\nstep1264, loss: 4.624819755554199\nstep1265, loss: 4.595282077789307\nstep1266, loss: 4.511495113372803\nstep1267, loss: 4.620445728302002\nstep1268, loss: 4.449960708618164\nstep1269, loss: 4.241244792938232\nstep1270, loss: 4.348603248596191\nstep1271, loss: 4.573725700378418\nstep1272, loss: 4.334125995635986\nstep1273, loss: 4.366953372955322\nstep1274, loss: 4.661499977111816\nstep1275, loss: 4.500486373901367\nstep1276, loss: 4.406783103942871\nstep1277, loss: 4.174118518829346\nstep1278, loss: 4.385001182556152\nstep1279, loss: 4.289189338684082\nstep1280, loss: 4.38575553894043\nstep1281, loss: 4.234772205352783\nstep1282, loss: 4.590142250061035\nstep1283, loss: 4.5296735763549805\nstep1284, loss: 4.468874931335449\nstep1285, loss: 4.639178276062012\nstep1286, loss: 4.878252029418945\nstep1287, loss: 4.711794853210449\nstep1288, loss: 4.413658142089844\nstep1289, loss: 4.525895118713379\nstep1290, loss: 4.602832794189453\nstep1291, loss: 4.517448425292969\nstep1292, loss: 4.395092487335205\nstep1293, loss: 4.233619689941406\nstep1294, loss: 4.189551830291748\nstep1295, loss: 4.281327724456787\nstep1296, loss: 4.344017028808594\nstep1297, loss: 4.326461315155029\nstep1298, loss: 4.335599899291992\nstep1299, loss: 4.086543560028076\nstep1300, loss: 4.06064510345459\nstep1301, loss: 4.652989387512207\nstep1302, loss: 4.39342737197876\nstep1303, loss: 4.1517486572265625\nstep1304, loss: 4.151169776916504\nstep1305, loss: 4.306096076965332\nstep1306, loss: 4.205399513244629\nstep1307, loss: 3.9596121311187744\nstep1308, loss: 3.9142284393310547\nstep1309, loss: 3.9593448638916016\nstep1310, loss: 4.626739025115967\nstep1311, loss: 4.473001956939697\nstep1312, loss: 4.559365749359131\nstep1313, loss: 4.425490856170654\nstep1314, loss: 4.490427494049072\nstep1315, loss: 4.505217552185059\nstep1316, loss: 4.384572505950928\nstep1317, loss: 4.112079620361328\nstep1318, loss: 4.188172817230225\nstep1319, loss: 4.125472545623779\nstep1320, loss: 4.199245929718018\nstep1321, loss: 4.273772239685059\nstep1322, loss: 4.4448018074035645\nstep1323, loss: 4.381597995758057\nstep1324, loss: 4.217755317687988\nstep1325, loss: 4.336153984069824\nstep1326, loss: 4.284425258636475\nstep1327, loss: 4.3528032302856445\nstep1328, loss: 4.005378246307373\nstep1329, loss: 4.06410551071167\nstep1330, loss: 4.422822952270508\nstep1331, loss: 4.047078609466553\nstep1332, loss: 4.300384521484375\nstep1333, loss: 3.933218002319336\nstep1334, loss: 4.466796398162842\nstep1335, loss: 4.515835762023926\nstep1336, loss: 4.574811935424805\nstep1337, loss: 4.553628444671631\nstep1338, loss: 4.442182540893555\nstep1339, loss: 4.481700420379639\nstep1340, loss: 4.455834865570068\nstep1341, loss: 4.409172534942627\nstep1342, loss: 4.262115478515625\nstep1343, loss: 4.161444187164307\nstep1344, loss: 4.337391376495361\nstep1345, loss: 4.526894569396973\nstep1346, loss: 4.548425197601318\nstep1347, loss: 4.476319789886475\nstep1348, loss: 4.373841762542725\nstep1349, loss: 4.49727201461792\nstep1350, loss: 4.376623630523682\nstep1351, loss: 4.166545391082764\nstep1352, loss: 4.262470245361328\nstep1353, loss: 4.48976993560791\nstep1354, loss: 4.235597133636475\nstep1355, loss: 4.302583694458008\nstep1356, loss: 4.59217643737793\nstep1357, loss: 4.462316513061523\nstep1358, loss: 4.364083290100098\nstep1359, loss: 4.085297584533691\nstep1360, loss: 4.3013200759887695\nstep1361, loss: 4.217445373535156\nstep1362, loss: 4.337090969085693\nstep1363, loss: 4.171088695526123\nstep1364, loss: 4.554779529571533\nstep1365, loss: 4.482850074768066\nstep1366, loss: 4.406523704528809\nstep1367, loss: 4.568622589111328\nstep1368, loss: 4.7925333976745605\nstep1369, loss: 4.613834381103516\nstep1370, loss: 4.352075099945068\nstep1371, loss: 4.415245056152344\nstep1372, loss: 4.521036148071289\nstep1373, loss: 4.42446756362915\nstep1374, loss: 4.296199798583984\nstep1375, loss: 4.1400299072265625\nstep1376, loss: 4.145279407501221\nstep1377, loss: 4.260052680969238\nstep1378, loss: 4.271562099456787\nstep1379, loss: 4.202084064483643\nstep1380, loss: 4.2041497230529785\nstep1381, loss: 4.015301704406738\nstep1382, loss: 4.008130073547363\nstep1383, loss: 4.61539888381958\nstep1384, loss: 4.374081134796143\nstep1385, loss: 4.1040215492248535\nstep1386, loss: 4.064968585968018\nstep1387, loss: 4.225364685058594\nstep1388, loss: 4.160655498504639\nstep1389, loss: 3.9186935424804688\nstep1390, loss: 3.86429500579834\nstep1391, loss: 3.9008305072784424\nstep1392, loss: 4.527645587921143\nstep1393, loss: 4.369155406951904\nstep1394, loss: 4.485834121704102\nstep1395, loss: 4.3692755699157715\nstep1396, loss: 4.417634963989258\nstep1397, loss: 4.36721658706665\nstep1398, loss: 4.245045185089111\nstep1399, loss: 3.9673829078674316\nstep1400, loss: 4.100807189941406\nstep1401, loss: 4.058700084686279\nstep1402, loss: 4.10722017288208\nstep1403, loss: 4.220833778381348\nstep1404, loss: 4.38229513168335\nstep1405, loss: 4.304311275482178\nstep1406, loss: 4.1283745765686035\nstep1407, loss: 4.22028112411499\nstep1408, loss: 4.184680938720703\nstep1409, loss: 4.260989665985107\nstep1410, loss: 3.9556617736816406\nstep1411, loss: 4.022002696990967\nstep1412, loss: 4.344676494598389\nstep1413, loss: 3.9943625926971436\nstep1414, loss: 4.210254669189453\nstep1415, loss: 3.8821892738342285\nstep1416, loss: 4.381279945373535\nstep1417, loss: 4.445815563201904\nstep1418, loss: 4.497878074645996\nstep1419, loss: 4.479921817779541\nstep1420, loss: 4.392601013183594\nstep1421, loss: 4.406893253326416\nstep1422, loss: 4.373535633087158\nstep1423, loss: 4.311445713043213\nstep1424, loss: 4.147847652435303\nstep1425, loss: 4.0549726486206055\nstep1426, loss: 4.2523908615112305\nstep1427, loss: 4.498866558074951\nstep1428, loss: 4.50946044921875\nstep1429, loss: 4.432662010192871\nstep1430, loss: 4.3044753074646\nstep1431, loss: 4.414801120758057\nstep1432, loss: 4.318308353424072\nstep1433, loss: 4.172242641448975\nstep1434, loss: 4.270233631134033\nstep1435, loss: 4.489535808563232\nstep1436, loss: 4.196422576904297\nstep1437, loss: 4.234766006469727\nstep1438, loss: 4.522740840911865\nstep1439, loss: 4.404273986816406\nstep1440, loss: 4.310554504394531\nstep1441, loss: 4.076100826263428\nstep1442, loss: 4.298807621002197\nstep1443, loss: 4.193787097930908\nstep1444, loss: 4.273815155029297\nstep1445, loss: 4.122954368591309\nstep1446, loss: 4.494844913482666\nstep1447, loss: 4.463552951812744\nstep1448, loss: 4.389865398406982\nstep1449, loss: 4.524747848510742\nstep1450, loss: 4.723077297210693\nstep1451, loss: 4.5385050773620605\nstep1452, loss: 4.282437801361084\nstep1453, loss: 4.383631229400635\nstep1454, loss: 4.490102291107178\nstep1455, loss: 4.397035121917725\nstep1456, loss: 4.2557268142700195\nstep1457, loss: 4.090618133544922\nstep1458, loss: 4.049896240234375\nstep1459, loss: 4.179556369781494\nstep1460, loss: 4.209282875061035\nstep1461, loss: 4.177052021026611\nstep1462, loss: 4.176249980926514\nstep1463, loss: 3.9834694862365723\nstep1464, loss: 3.955065965652466\nstep1465, loss: 4.52052116394043\nstep1466, loss: 4.292557239532471\nstep1467, loss: 4.064289093017578\nstep1468, loss: 4.031676769256592\nstep1469, loss: 4.225759983062744\nstep1470, loss: 4.1284565925598145\nstep1471, loss: 3.871891736984253\nstep1472, loss: 3.834254503250122\nstep1473, loss: 3.8569397926330566\nstep1474, loss: 4.465328693389893\nstep1475, loss: 4.344371318817139\nstep1476, loss: 4.4679274559021\nstep1477, loss: 4.355614185333252\nstep1478, loss: 4.3932318687438965\nstep1479, loss: 4.366540431976318\nstep1480, loss: 4.230103969573975\nstep1481, loss: 3.9499197006225586\nstep1482, loss: 4.050577640533447\nstep1483, loss: 4.02376651763916\nstep1484, loss: 4.0515875816345215\nstep1485, loss: 4.174108505249023\nstep1486, loss: 4.36067533493042\nstep1487, loss: 4.276467323303223\nstep1488, loss: 4.122446060180664\nstep1489, loss: 4.166193008422852\nstep1490, loss: 4.141358375549316\nstep1491, loss: 4.250153064727783\nstep1492, loss: 3.899292469024658\nstep1493, loss: 3.965775728225708\nstep1494, loss: 4.290282726287842\nstep1495, loss: 3.956796407699585\nstep1496, loss: 4.172532558441162\nstep1497, loss: 3.8581645488739014\nstep1498, loss: 4.3532562255859375\nstep1499, loss: 4.415966510772705\nstep1500, loss: 4.472827911376953\nstep1501, loss: 4.431302547454834\nstep1502, loss: 4.352336406707764\nstep1503, loss: 4.407992362976074\nstep1504, loss: 4.389568328857422\nstep1505, loss: 4.37178897857666\nstep1506, loss: 4.212096214294434\nstep1507, loss: 4.0718092918396\nstep1508, loss: 4.245107650756836\nstep1509, loss: 4.414477825164795\nstep1510, loss: 4.421257019042969\nstep1511, loss: 4.349644660949707\nstep1512, loss: 4.288249492645264\nstep1513, loss: 4.4674072265625\nstep1514, loss: 4.330809593200684\nstep1515, loss: 4.114192485809326\nstep1516, loss: 4.193932056427002\nstep1517, loss: 4.411020278930664\nstep1518, loss: 4.182162761688232\nstep1519, loss: 4.2334113121032715\nstep1520, loss: 4.522830486297607\nstep1521, loss: 4.358006477355957\nstep1522, loss: 4.267063140869141\nstep1523, loss: 3.965421676635742\nstep1524, loss: 4.2158684730529785\nstep1525, loss: 4.106192111968994\nstep1526, loss: 4.2699174880981445\nstep1527, loss: 4.125946998596191\nstep1528, loss: 4.484755516052246\nstep1529, loss: 4.428044319152832\nstep1530, loss: 4.324278831481934\nstep1531, loss: 4.482285022735596\nstep1532, loss: 4.676019191741943\nstep1533, loss: 4.513919353485107\nstep1534, loss: 4.238542079925537\nstep1535, loss: 4.302284240722656\nstep1536, loss: 4.399535655975342\nstep1537, loss: 4.302604675292969\nstep1538, loss: 4.157535552978516\nstep1539, loss: 4.007076740264893\nstep1540, loss: 4.007988929748535\nstep1541, loss: 4.11648416519165\nstep1542, loss: 4.089090824127197\nstep1543, loss: 4.032580852508545\nstep1544, loss: 4.057119369506836\nstep1545, loss: 3.871197462081909\nstep1546, loss: 3.869080066680908\nstep1547, loss: 4.4594950675964355\nstep1548, loss: 4.2385573387146\nstep1549, loss: 3.992382526397705\nstep1550, loss: 3.942612886428833\nstep1551, loss: 4.105806827545166\nstep1552, loss: 4.0034708976745605\nstep1553, loss: 3.791595697402954\nstep1554, loss: 3.7838025093078613\nstep1555, loss: 3.820452928543091\nstep1556, loss: 4.466570854187012\nstep1557, loss: 4.290493965148926\nstep1558, loss: 4.3838043212890625\nstep1559, loss: 4.271293640136719\nstep1560, loss: 4.282686233520508\nstep1561, loss: 4.295703411102295\nstep1562, loss: 4.187260627746582\nstep1563, loss: 3.90822696685791\nstep1564, loss: 4.040190696716309\nstep1565, loss: 3.9951894283294678\nstep1566, loss: 4.011712074279785\nstep1567, loss: 4.123726844787598\nstep1568, loss: 4.28226375579834\nstep1569, loss: 4.197346210479736\nstep1570, loss: 4.0060954093933105\nstep1571, loss: 4.0949482917785645\nstep1572, loss: 4.076259613037109\nstep1573, loss: 4.172175407409668\nstep1574, loss: 3.828702688217163\nstep1575, loss: 3.8610024452209473\nstep1576, loss: 4.220071315765381\nstep1577, loss: 3.8335442543029785\nstep1578, loss: 4.063470363616943\nstep1579, loss: 3.741328477859497\nstep1580, loss: 4.212486743927002\nstep1581, loss: 4.262155055999756\nstep1582, loss: 4.32065486907959\nstep1583, loss: 4.309328556060791\nstep1584, loss: 4.226656436920166\nstep1585, loss: 4.297339916229248\nstep1586, loss: 4.230069160461426\nstep1587, loss: 4.170331001281738\nstep1588, loss: 4.044448375701904\nstep1589, loss: 3.9374635219573975\nstep1590, loss: 4.137021541595459\nstep1591, loss: 4.354284286499023\nstep1592, loss: 4.375422954559326\nstep1593, loss: 4.2948760986328125\nstep1594, loss: 4.1428728103637695\nstep1595, loss: 4.287893772125244\nstep1596, loss: 4.170549392700195\nstep1597, loss: 4.015711784362793\nstep1598, loss: 4.141707897186279\nstep1599, loss: 4.348190784454346\nstep1600, loss: 4.095735549926758\nstep1601, loss: 4.171786785125732\nstep1602, loss: 4.451414585113525\nstep1603, loss: 4.322153568267822\nstep1604, loss: 4.255823612213135\nstep1605, loss: 3.949077606201172\nstep1606, loss: 4.178829669952393\nstep1607, loss: 4.073129177093506\nstep1608, loss: 4.175165176391602\nstep1609, loss: 4.022961616516113\nstep1610, loss: 4.36201810836792\nstep1611, loss: 4.3434906005859375\nstep1612, loss: 4.31533670425415\nstep1613, loss: 4.472792625427246\nstep1614, loss: 4.63223123550415\nstep1615, loss: 4.443626880645752\nstep1616, loss: 4.165534496307373\nstep1617, loss: 4.268036365509033\nstep1618, loss: 4.383686542510986\nstep1619, loss: 4.287490367889404\nstep1620, loss: 4.096667289733887\nstep1621, loss: 3.899916172027588\nstep1622, loss: 3.940721035003662\nstep1623, loss: 4.102680206298828\nstep1624, loss: 4.082537651062012\nstep1625, loss: 4.0330939292907715\nstep1626, loss: 4.0227274894714355\nstep1627, loss: 3.8170998096466064\nstep1628, loss: 3.8154497146606445\nstep1629, loss: 4.3647565841674805\nstep1630, loss: 4.09798002243042\nstep1631, loss: 3.8898298740386963\nstep1632, loss: 3.8736064434051514\nstep1633, loss: 4.048687934875488\nstep1634, loss: 3.95914888381958\nstep1635, loss: 3.6910159587860107\nstep1636, loss: 3.675670623779297\nstep1637, loss: 3.7026894092559814\nstep1638, loss: 4.32131814956665\nstep1639, loss: 4.173842430114746\nstep1640, loss: 4.285670757293701\nstep1641, loss: 4.137551784515381\nstep1642, loss: 4.216042518615723\nstep1643, loss: 4.191246032714844\nstep1644, loss: 4.06920862197876\nstep1645, loss: 3.818472146987915\nstep1646, loss: 3.949819564819336\nstep1647, loss: 3.895815134048462\nstep1648, loss: 3.937633514404297\nstep1649, loss: 4.068385124206543\nstep1650, loss: 4.2490620613098145\nstep1651, loss: 4.1725077629089355\nstep1652, loss: 3.9713079929351807\nstep1653, loss: 4.057136058807373\nstep1654, loss: 4.0263352394104\nstep1655, loss: 4.1361284255981445\nstep1656, loss: 3.820005416870117\nstep1657, loss: 3.867035150527954\nstep1658, loss: 4.19536828994751\nstep1659, loss: 3.851310968399048\nstep1660, loss: 4.051445960998535\nstep1661, loss: 3.683239459991455\nstep1662, loss: 4.149611473083496\nstep1663, loss: 4.201841831207275\nstep1664, loss: 4.284843921661377\nstep1665, loss: 4.293146133422852\nstep1666, loss: 4.200127601623535\nstep1667, loss: 4.245643138885498\nstep1668, loss: 4.186331272125244\nstep1669, loss: 4.124382019042969\nstep1670, loss: 4.038586616516113\nstep1671, loss: 3.8883862495422363\nstep1672, loss: 4.012162208557129\nstep1673, loss: 4.237509727478027\nstep1674, loss: 4.23462438583374\nstep1675, loss: 4.198647975921631\nstep1676, loss: 4.053633213043213\nstep1677, loss: 4.1930646896362305\nstep1678, loss: 4.0733866691589355\nstep1679, loss: 3.9257168769836426\nstep1680, loss: 3.996063709259033\nstep1681, loss: 4.238123893737793\nstep1682, loss: 4.012351036071777\nstep1683, loss: 4.057898044586182\nstep1684, loss: 4.3608479499816895\nstep1685, loss: 4.216887474060059\nstep1686, loss: 4.153234004974365\nstep1687, loss: 3.8517277240753174\nstep1688, loss: 4.140544414520264\nstep1689, loss: 4.009705066680908\nstep1690, loss: 4.137451648712158\nstep1691, loss: 3.9329986572265625\nstep1692, loss: 4.268455982208252\nstep1693, loss: 4.200917720794678\nstep1694, loss: 4.161262035369873\nstep1695, loss: 4.306328773498535\nstep1696, loss: 4.523726940155029\nstep1697, loss: 4.347862720489502\nstep1698, loss: 4.0965352058410645\nstep1699, loss: 4.122990608215332\nstep1700, loss: 4.254270076751709\nstep1701, loss: 4.189831733703613\nstep1702, loss: 4.10476016998291\nstep1703, loss: 3.8616950511932373\nstep1704, loss: 3.918818473815918\nstep1705, loss: 4.028199672698975\nstep1706, loss: 3.9873530864715576\nstep1707, loss: 3.929053783416748\nstep1708, loss: 3.9336087703704834\nstep1709, loss: 3.783233880996704\nstep1710, loss: 3.7957098484039307\nstep1711, loss: 4.40512752532959\nstep1712, loss: 4.104029655456543\nstep1713, loss: 3.8627214431762695\nstep1714, loss: 3.8051769733428955\nstep1715, loss: 3.9512743949890137\nstep1716, loss: 3.856627941131592\nstep1717, loss: 3.635098695755005\nstep1718, loss: 3.628521203994751\nstep1719, loss: 3.6541457176208496\nstep1720, loss: 4.251163005828857\nstep1721, loss: 4.086766719818115\nstep1722, loss: 4.176286220550537\nstep1723, loss: 4.04110050201416\nstep1724, loss: 4.116853713989258\nstep1725, loss: 4.085165977478027\nstep1726, loss: 3.9824142456054688\nstep1727, loss: 3.6632227897644043\nstep1728, loss: 3.839482307434082\nstep1729, loss: 3.8097615242004395\nstep1730, loss: 3.85009765625\nstep1731, loss: 4.000970363616943\nstep1732, loss: 4.1313652992248535\nstep1733, loss: 4.048716068267822\nstep1734, loss: 3.8889377117156982\nstep1735, loss: 3.983524799346924\nstep1736, loss: 3.9536705017089844\nstep1737, loss: 4.1160759925842285\nstep1738, loss: 3.8019821643829346\nstep1739, loss: 3.828047513961792\nstep1740, loss: 4.141628742218018\nstep1741, loss: 3.7937636375427246\nstep1742, loss: 4.0148539543151855\nstep1743, loss: 3.6934921741485596\nstep1744, loss: 4.160939693450928\nstep1745, loss: 4.184288024902344\nstep1746, loss: 4.2074103355407715\nstep1747, loss: 4.197878837585449\nstep1748, loss: 4.1324615478515625\nstep1749, loss: 4.182271480560303\nstep1750, loss: 4.144241809844971\nstep1751, loss: 4.094905853271484\nstep1752, loss: 3.9971017837524414\nstep1753, loss: 3.8609297275543213\nstep1754, loss: 3.9773035049438477\nstep1755, loss: 4.160562992095947\nstep1756, loss: 4.164897441864014\nstep1757, loss: 4.106632709503174\nstep1758, loss: 3.9656810760498047\nstep1759, loss: 4.119266033172607\nstep1760, loss: 3.993502378463745\nstep1761, loss: 3.8455724716186523\nstep1762, loss: 3.900424003601074\nstep1763, loss: 4.151242733001709\nstep1764, loss: 3.9005188941955566\nstep1765, loss: 3.9795475006103516\nstep1766, loss: 4.279574394226074\nstep1767, loss: 4.129339694976807\nstep1768, loss: 4.06918478012085\nstep1769, loss: 3.773829936981201\nstep1770, loss: 4.054348468780518\nstep1771, loss: 3.9519355297088623\nstep1772, loss: 4.065535068511963\nstep1773, loss: 3.9052670001983643\nstep1774, loss: 4.246430397033691\nstep1775, loss: 4.147957801818848\nstep1776, loss: 4.100944995880127\nstep1777, loss: 4.226284027099609\nstep1778, loss: 4.38795280456543\nstep1779, loss: 4.265827178955078\nstep1780, loss: 4.0350799560546875\nstep1781, loss: 4.047973155975342\nstep1782, loss: 4.171669960021973\nstep1783, loss: 4.084394931793213\nstep1784, loss: 3.953684091567993\nstep1785, loss: 3.7092819213867188\nstep1786, loss: 3.8000025749206543\nstep1787, loss: 3.937385320663452\nstep1788, loss: 3.9035863876342773\nstep1789, loss: 3.870506763458252\nstep1790, loss: 3.86855149269104\nstep1791, loss: 3.719257354736328\nstep1792, loss: 3.676053762435913\nstep1793, loss: 4.250444412231445\nstep1794, loss: 3.937504768371582\nstep1795, loss: 3.7347540855407715\nstep1796, loss: 3.711754560470581\nstep1797, loss: 3.9202232360839844\nstep1798, loss: 3.8009560108184814\nstep1799, loss: 3.5931713581085205\nstep1800, loss: 3.561624526977539\nstep1801, loss: 3.584775924682617\nstep1802, loss: 4.168220520019531\nstep1803, loss: 3.9974751472473145\nstep1804, loss: 4.109291076660156\nstep1805, loss: 3.9731285572052\nstep1806, loss: 4.020749568939209\nstep1807, loss: 3.984060287475586\nstep1808, loss: 3.8989267349243164\nstep1809, loss: 3.641864538192749\nstep1810, loss: 3.7892777919769287\nstep1811, loss: 3.766719341278076\nstep1812, loss: 3.792689085006714\nstep1813, loss: 3.9572184085845947\nstep1814, loss: 4.102349281311035\nstep1815, loss: 3.9854369163513184\nstep1816, loss: 3.8551039695739746\nstep1817, loss: 3.8823773860931396\nstep1818, loss: 3.8457977771759033\nstep1819, loss: 4.009146690368652\nstep1820, loss: 3.6676621437072754\nstep1821, loss: 3.723499298095703\nstep1822, loss: 4.080384731292725\nstep1823, loss: 3.7516708374023438\nstep1824, loss: 3.930861473083496\nstep1825, loss: 3.5837714672088623\nstep1826, loss: 4.033173561096191\nstep1827, loss: 4.14146614074707\nstep1828, loss: 4.21513557434082\nstep1829, loss: 4.208545207977295\nstep1830, loss: 4.0475287437438965\nstep1831, loss: 4.067770481109619\nstep1832, loss: 4.015302658081055\nstep1833, loss: 3.970339775085449\nstep1834, loss: 3.913675308227539\nstep1835, loss: 3.8251800537109375\nstep1836, loss: 3.9815001487731934\nstep1837, loss: 4.204843521118164\nstep1838, loss: 4.173390865325928\nstep1839, loss: 4.110949993133545\nstep1840, loss: 3.966207504272461\nstep1841, loss: 4.083233833312988\nstep1842, loss: 3.9734854698181152\nstep1843, loss: 3.8364601135253906\nstep1844, loss: 3.9155170917510986\nstep1845, loss: 4.109874248504639\nstep1846, loss: 3.8674232959747314\nstep1847, loss: 3.932957172393799\nstep1848, loss: 4.23668909072876\nstep1849, loss: 4.080018520355225\nstep1850, loss: 4.044908046722412\nstep1851, loss: 3.7198071479797363\nstep1852, loss: 3.954263687133789\nstep1853, loss: 3.860147714614868\nstep1854, loss: 3.9710335731506348\nstep1855, loss: 3.82214617729187\nstep1856, loss: 4.136096000671387\nstep1857, loss: 4.0995354652404785\nstep1858, loss: 4.111149311065674\nstep1859, loss: 4.215052127838135\nstep1860, loss: 4.3237714767456055\nstep1861, loss: 4.17275857925415\nstep1862, loss: 3.926807403564453\nstep1863, loss: 4.001983642578125\nstep1864, loss: 4.156317710876465\nstep1865, loss: 4.084527492523193\nstep1866, loss: 3.9107587337493896\nstep1867, loss: 3.6692893505096436\nstep1868, loss: 3.74809193611145\nstep1869, loss: 3.8640620708465576\nstep1870, loss: 3.842174768447876\nstep1871, loss: 3.7686333656311035\nstep1872, loss: 3.794799566268921\nstep1873, loss: 3.6637866497039795\nstep1874, loss: 3.6475019454956055\nstep1875, loss: 4.255550384521484\nstep1876, loss: 4.001954555511475\nstep1877, loss: 3.7369167804718018\nstep1878, loss: 3.702484369277954\nstep1879, loss: 3.8236420154571533\nstep1880, loss: 3.6958811283111572\nstep1881, loss: 3.548877239227295\nstep1882, loss: 3.5749409198760986\nstep1883, loss: 3.5786292552948\nstep1884, loss: 4.160826683044434\nstep1885, loss: 3.99851393699646\nstep1886, loss: 4.105401039123535\nstep1887, loss: 3.986856698989868\nstep1888, loss: 4.015262126922607\nstep1889, loss: 3.972925901412964\nstep1890, loss: 3.87807035446167\nstep1891, loss: 3.5906264781951904\nstep1892, loss: 3.745748996734619\nstep1893, loss: 3.7155182361602783\nstep1894, loss: 3.7405059337615967\nstep1895, loss: 3.876643657684326\nstep1896, loss: 4.06094217300415\nstep1897, loss: 3.9458260536193848\nstep1898, loss: 3.8047122955322266\nstep1899, loss: 3.85768723487854\nstep1900, loss: 3.820868730545044\nstep1901, loss: 3.9393041133880615\nstep1902, loss: 3.5862669944763184\nstep1903, loss: 3.635756015777588\nstep1904, loss: 4.003870010375977\nstep1905, loss: 3.6806628704071045\nstep1906, loss: 3.903324604034424\nstep1907, loss: 3.5680885314941406\nstep1908, loss: 4.021218776702881\nstep1909, loss: 4.10991907119751\nstep1910, loss: 4.146877288818359\nstep1911, loss: 4.1317315101623535\nstep1912, loss: 4.016390323638916\nstep1913, loss: 4.045230865478516\nstep1914, loss: 4.057956218719482\nstep1915, loss: 3.995958089828491\nstep1916, loss: 3.8614940643310547\nstep1917, loss: 3.7619597911834717\nstep1918, loss: 3.8706517219543457\nstep1919, loss: 4.086332321166992\nstep1920, loss: 4.07572603225708\nstep1921, loss: 4.0126776695251465\nstep1922, loss: 3.8883237838745117\nstep1923, loss: 4.064055442810059\nstep1924, loss: 3.997891902923584\nstep1925, loss: 3.8262884616851807\nstep1926, loss: 3.8379673957824707\nstep1927, loss: 4.0352373123168945\nstep1928, loss: 3.854119300842285\nstep1929, loss: 3.9325203895568848\nstep1930, loss: 4.251708984375\nstep1931, loss: 4.058147430419922\nstep1932, loss: 3.9853363037109375\nstep1933, loss: 3.6751129627227783\nstep1934, loss: 3.964804172515869\nstep1935, loss: 3.8906211853027344\nstep1936, loss: 3.9957163333892822\nstep1937, loss: 3.8304967880249023\nstep1938, loss: 4.133048057556152\nstep1939, loss: 4.044868469238281\nstep1940, loss: 4.054140090942383\nstep1941, loss: 4.189407825469971\nstep1942, loss: 4.346862316131592\nstep1943, loss: 4.188941478729248\nstep1944, loss: 3.9692976474761963\nstep1945, loss: 3.9940595626831055\nstep1946, loss: 4.113544464111328\nstep1947, loss: 4.0127105712890625\nstep1948, loss: 3.8693108558654785\nstep1949, loss: 3.6042914390563965\nstep1950, loss: 3.7210893630981445\nstep1951, loss: 3.8176493644714355\nstep1952, loss: 3.824023723602295\nstep1953, loss: 3.7572021484375\nstep1954, loss: 3.7826688289642334\nstep1955, loss: 3.6399385929107666\nstep1956, loss: 3.617647647857666\nstep1957, loss: 4.1609697341918945\nstep1958, loss: 3.8993563652038574\nstep1959, loss: 3.6343789100646973\nstep1960, loss: 3.61130428314209\nstep1961, loss: 3.76668119430542\nstep1962, loss: 3.671013116836548\nstep1963, loss: 3.4918570518493652\nstep1964, loss: 3.4859304428100586\nstep1965, loss: 3.474771499633789\nstep1966, loss: 4.034351825714111\nstep1967, loss: 3.8687000274658203\nstep1968, loss: 3.9966137409210205\nstep1969, loss: 3.9109063148498535\nstep1970, loss: 3.9564292430877686\nstep1971, loss: 3.896148920059204\nstep1972, loss: 3.7860615253448486\nstep1973, loss: 3.4973831176757812\nstep1974, loss: 3.6505889892578125\nstep1975, loss: 3.6206297874450684\nstep1976, loss: 3.6606407165527344\nstep1977, loss: 3.7947559356689453\nstep1978, loss: 4.007477283477783\nstep1979, loss: 3.9000532627105713\nstep1980, loss: 3.7785232067108154\nstep1981, loss: 3.838961601257324\nstep1982, loss: 3.7543210983276367\nstep1983, loss: 3.8825995922088623\nstep1984, loss: 3.5823965072631836\nstep1985, loss: 3.6023736000061035\nstep1986, loss: 3.9590797424316406\nstep1987, loss: 3.5625696182250977\nstep1988, loss: 3.775902271270752\nstep1989, loss: 3.416980266571045\nstep1990, loss: 3.921736717224121\nstep1991, loss: 4.0010528564453125\nstep1992, loss: 4.039783000946045\nstep1993, loss: 4.080292701721191\nstep1994, loss: 3.9480960369110107\nstep1995, loss: 3.965053081512451\nstep1996, loss: 3.9387290477752686\nstep1997, loss: 3.893178701400757\nstep1998, loss: 3.7786569595336914\nstep1999, loss: 3.698176383972168\nstep2000, loss: 3.7969443798065186\nstep2001, loss: 4.0103759765625\nstep2002, loss: 4.015080451965332\nstep2003, loss: 3.949791193008423\nstep2004, loss: 3.775021553039551\nstep2005, loss: 3.961832046508789\nstep2006, loss: 3.871903419494629\nstep2007, loss: 3.735678195953369\nstep2008, loss: 3.8001325130462646\nstep2009, loss: 4.040775775909424\nstep2010, loss: 3.8371620178222656\nstep2011, loss: 3.8269927501678467\nstep2012, loss: 4.080596923828125\nstep2013, loss: 3.9666857719421387\nstep2014, loss: 3.966610908508301\nstep2015, loss: 3.6976330280303955\nstep2016, loss: 3.9655356407165527\nstep2017, loss: 3.8333547115325928\nstep2018, loss: 3.8726208209991455\nstep2019, loss: 3.720808506011963\nstep2020, loss: 4.02772855758667\nstep2021, loss: 3.992328643798828\nstep2022, loss: 4.029909133911133\nstep2023, loss: 4.156737804412842\nstep2024, loss: 4.2699875831604\nstep2025, loss: 4.076811790466309\nstep2026, loss: 3.842374086380005\nstep2027, loss: 3.874573230743408\nstep2028, loss: 4.086523532867432\nstep2029, loss: 4.013564586639404\nstep2030, loss: 3.8634397983551025\nstep2031, loss: 3.585721254348755\nstep2032, loss: 3.6996922492980957\nstep2033, loss: 3.7662296295166016\nstep2034, loss: 3.7214198112487793\nstep2035, loss: 3.688807249069214\nstep2036, loss: 3.6970374584198\nstep2037, loss: 3.563932418823242\nstep2038, loss: 3.547698974609375\nstep2039, loss: 4.127671241760254\nstep2040, loss: 3.8521885871887207\nstep2041, loss: 3.6276838779449463\nstep2042, loss: 3.6007206439971924\nstep2043, loss: 3.7234532833099365\nstep2044, loss: 3.57253098487854\nstep2045, loss: 3.3673019409179688\nstep2046, loss: 3.406965494155884\nstep2047, loss: 3.4056692123413086\nstep2048, loss: 3.967237710952759\nstep2049, loss: 3.821427345275879\nstep2050, loss: 3.938389539718628\nstep2051, loss: 3.8500754833221436\nstep2052, loss: 3.865285873413086\nstep2053, loss: 3.818704128265381\nstep2054, loss: 3.724534749984741\nstep2055, loss: 3.440094470977783\nstep2056, loss: 3.607687473297119\nstep2057, loss: 3.5599477291107178\nstep2058, loss: 3.5799903869628906\nstep2059, loss: 3.7421329021453857\nstep2060, loss: 3.889460802078247\nstep2061, loss: 3.8540666103363037\nstep2062, loss: 3.688133478164673\nstep2063, loss: 3.7760369777679443\nstep2064, loss: 3.71933650970459\nstep2065, loss: 3.910322666168213\nstep2066, loss: 3.5794084072113037\nstep2067, loss: 3.6017632484436035\nstep2068, loss: 3.8796308040618896\nstep2069, loss: 3.5176196098327637\nstep2070, loss: 3.7476277351379395\nstep2071, loss: 3.4429781436920166\nstep2072, loss: 3.8823068141937256\nstep2073, loss: 3.918731451034546\nstep2074, loss: 3.952749490737915\nstep2075, loss: 3.956331729888916\nstep2076, loss: 3.9073054790496826\nstep2077, loss: 3.9350388050079346\nstep2078, loss: 3.9546985626220703\nstep2079, loss: 3.840911865234375\nstep2080, loss: 3.7142672538757324\nstep2081, loss: 3.5863187313079834\nstep2082, loss: 3.7154059410095215\nstep2083, loss: 3.906249761581421\nstep2084, loss: 3.893446683883667\nstep2085, loss: 3.872908115386963\nstep2086, loss: 3.7199015617370605\nstep2087, loss: 3.8847692012786865\nstep2088, loss: 3.77488374710083\nstep2089, loss: 3.6317880153656006\nstep2090, loss: 3.6710751056671143\nstep2091, loss: 3.8829116821289062\nstep2092, loss: 3.6814064979553223\nstep2093, loss: 3.727708578109741\nstep2094, loss: 3.999789237976074\nstep2095, loss: 3.883553981781006\nstep2096, loss: 3.82436466217041\nstep2097, loss: 3.5497822761535645\nstep2098, loss: 3.79612135887146\nstep2099, loss: 3.7186474800109863\nstep2100, loss: 3.821497678756714\nstep2101, loss: 3.6931796073913574\nstep2102, loss: 3.9896693229675293\nstep2103, loss: 3.93772292137146\nstep2104, loss: 3.920832395553589\nstep2105, loss: 3.9810092449188232\nstep2106, loss: 4.102354049682617\nstep2107, loss: 3.99420428276062\nstep2108, loss: 3.843426465988159\nstep2109, loss: 3.8471360206604004\nstep2110, loss: 3.9832191467285156\nstep2111, loss: 3.858316659927368\nstep2112, loss: 3.732516288757324\nstep2113, loss: 3.480417013168335\nstep2114, loss: 3.663407564163208\nstep2115, loss: 3.7639660835266113\nstep2116, loss: 3.6970300674438477\nstep2117, loss: 3.619004487991333\nstep2118, loss: 3.605696439743042\nstep2119, loss: 3.512739896774292\nstep2120, loss: 3.483428478240967\nstep2121, loss: 3.9997334480285645\nstep2122, loss: 3.7884912490844727\nstep2123, loss: 3.5733160972595215\nstep2124, loss: 3.537949562072754\nstep2125, loss: 3.676758289337158\nstep2126, loss: 3.5557444095611572\nstep2127, loss: 3.339787721633911\nstep2128, loss: 3.3623156547546387\nstep2129, loss: 3.3491718769073486\nstep2130, loss: 3.8591225147247314\nstep2131, loss: 3.7043347358703613\nstep2132, loss: 3.840010643005371\nstep2133, loss: 3.752268075942993\nstep2134, loss: 3.7896337509155273\nstep2135, loss: 3.7657291889190674\nstep2136, loss: 3.6576807498931885\nstep2137, loss: 3.360907554626465\nstep2138, loss: 3.535093069076538\nstep2139, loss: 3.525231122970581\nstep2140, loss: 3.525135040283203\nstep2141, loss: 3.658787250518799\nstep2142, loss: 3.796267509460449\nstep2143, loss: 3.751237630844116\nstep2144, loss: 3.5666091442108154\nstep2145, loss: 3.6503818035125732\nstep2146, loss: 3.5945088863372803\nstep2147, loss: 3.7676286697387695\nstep2148, loss: 3.4643354415893555\nstep2149, loss: 3.489001512527466\nstep2150, loss: 3.752136707305908\nstep2151, loss: 3.4240128993988037\nstep2152, loss: 3.6158084869384766\nstep2153, loss: 3.3349807262420654\nstep2154, loss: 3.738827705383301\nstep2155, loss: 3.810600996017456\nstep2156, loss: 3.889373779296875\nstep2157, loss: 3.8984439373016357\nstep2158, loss: 3.8249709606170654\nstep2159, loss: 3.853280782699585\nstep2160, loss: 3.842949151992798\nstep2161, loss: 3.7692980766296387\nstep2162, loss: 3.6711506843566895\nstep2163, loss: 3.5492279529571533\nstep2164, loss: 3.6632273197174072\nstep2165, loss: 3.8278965950012207\nstep2166, loss: 3.8247971534729004\nstep2167, loss: 3.7967934608459473\nstep2168, loss: 3.6336684226989746\nstep2169, loss: 3.780216932296753\nstep2170, loss: 3.730393648147583\nstep2171, loss: 3.6136484146118164\nstep2172, loss: 3.6417744159698486\nstep2173, loss: 3.8891220092773438\nstep2174, loss: 3.712625026702881\nstep2175, loss: 3.7007288932800293\nstep2176, loss: 3.9510414600372314\nstep2177, loss: 3.7887003421783447\nstep2178, loss: 3.7533881664276123\nstep2179, loss: 3.4609692096710205\nstep2180, loss: 3.7419309616088867\nstep2181, loss: 3.663344383239746\nstep2182, loss: 3.7512574195861816\nstep2183, loss: 3.6003687381744385\nstep2184, loss: 3.924410820007324\nstep2185, loss: 3.8627166748046875\nstep2186, loss: 3.876070261001587\nstep2187, loss: 4.005849361419678\nstep2188, loss: 4.219301223754883\nstep2189, loss: 4.031374931335449\nstep2190, loss: 3.806741952896118\nstep2191, loss: 3.7379202842712402\nstep2192, loss: 3.891230821609497\nstep2193, loss: 3.853464126586914\nstep2194, loss: 3.772278308868408\nstep2195, loss: 3.4960038661956787\nstep2196, loss: 3.5750105381011963\nstep2197, loss: 3.657839775085449\nstep2198, loss: 3.6098997592926025\nstep2199, loss: 3.5882997512817383\nstep2200, loss: 3.6254007816314697\nstep2201, loss: 3.5250704288482666\nstep2202, loss: 3.5004961490631104\nstep2203, loss: 3.9541168212890625\nstep2204, loss: 3.73360013961792\nstep2205, loss: 3.5479230880737305\nstep2206, loss: 3.5123918056488037\nstep2207, loss: 3.6179938316345215\nstep2208, loss: 3.5362818241119385\nstep2209, loss: 3.317324161529541\nstep2210, loss: 3.3139595985412598\nstep2211, loss: 3.339991569519043\nstep2212, loss: 3.928332805633545\nstep2213, loss: 3.7167532444000244\nstep2214, loss: 3.911496639251709\nstep2215, loss: 3.8019368648529053\nstep2216, loss: 3.780552387237549\nstep2217, loss: 3.7681779861450195\nstep2218, loss: 3.6712963581085205\nstep2219, loss: 3.3383734226226807\nstep2220, loss: 3.4970433712005615\nstep2221, loss: 3.499136447906494\nstep2222, loss: 3.51322865486145\nstep2223, loss: 3.6640243530273438\nstep2224, loss: 3.823647975921631\nstep2225, loss: 3.784165859222412\nstep2226, loss: 3.55169677734375\nstep2227, loss: 3.636294364929199\nstep2228, loss: 3.6021885871887207\nstep2229, loss: 3.766543388366699\nstep2230, loss: 3.445641040802002\nstep2231, loss: 3.483966588973999\nstep2232, loss: 3.771790027618408\nstep2233, loss: 3.443037509918213\nstep2234, loss: 3.6312241554260254\nstep2235, loss: 3.291139841079712\nstep2236, loss: 3.691732168197632\nstep2237, loss: 3.77459716796875\nstep2238, loss: 3.8002567291259766\nstep2239, loss: 3.8234622478485107\nstep2240, loss: 3.724994421005249\nstep2241, loss: 3.794499158859253\nstep2242, loss: 3.7926228046417236\nstep2243, loss: 3.721820116043091\nstep2244, loss: 3.649801731109619\nstep2245, loss: 3.4811675548553467\nstep2246, loss: 3.6211609840393066\nstep2247, loss: 3.7603859901428223\nstep2248, loss: 3.763894557952881\nstep2249, loss: 3.7299294471740723\nstep2250, loss: 3.580282211303711\nstep2251, loss: 3.746676206588745\nstep2252, loss: 3.61782169342041\nstep2253, loss: 3.4797017574310303\nstep2254, loss: 3.504749059677124\nstep2255, loss: 3.7742276191711426\nstep2256, loss: 3.614579677581787\nstep2257, loss: 3.678730010986328\nstep2258, loss: 3.9399020671844482\nstep2259, loss: 3.7862303256988525\nstep2260, loss: 3.758425712585449\nstep2261, loss: 3.4536006450653076\nstep2262, loss: 3.676002264022827\nstep2263, loss: 3.5915729999542236\nstep2264, loss: 3.6616814136505127\nstep2265, loss: 3.5154407024383545\nstep2266, loss: 3.7851428985595703\nstep2267, loss: 3.7314164638519287\nstep2268, loss: 3.7820260524749756\nstep2269, loss: 3.9001810550689697\nstep2270, loss: 4.021189212799072\nstep2271, loss: 3.898885488510132\nstep2272, loss: 3.717102527618408\nstep2273, loss: 3.694504499435425\nstep2274, loss: 3.900339365005493\nstep2275, loss: 3.7680115699768066\nstep2276, loss: 3.670680284500122\nstep2277, loss: 3.3593008518218994\nstep2278, loss: 3.509580373764038\nstep2279, loss: 3.5769379138946533\nstep2280, loss: 3.5954909324645996\nstep2281, loss: 3.5592122077941895\nstep2282, loss: 3.5325100421905518\nstep2283, loss: 3.426495313644409\nstep2284, loss: 3.378636598587036\nstep2285, loss: 3.9349355697631836\nstep2286, loss: 3.715390682220459\nstep2287, loss: 3.488374710083008\nstep2288, loss: 3.4542489051818848\nstep2289, loss: 3.5430960655212402\nstep2290, loss: 3.5121231079101562\nstep2291, loss: 3.329347848892212\nstep2292, loss: 3.309384822845459\nstep2293, loss: 3.299504518508911\nstep2294, loss: 3.8760924339294434\nstep2295, loss: 3.6169040203094482\nstep2296, loss: 3.7848620414733887\nstep2297, loss: 3.665417432785034\nstep2298, loss: 3.691495656967163\nstep2299, loss: 3.7338383197784424\nstep2300, loss: 3.6668283939361572\nstep2301, loss: 3.4314775466918945\nstep2302, loss: 3.5681021213531494\nstep2303, loss: 3.4647438526153564\nstep2304, loss: 3.4360790252685547\nstep2305, loss: 3.5921339988708496\nstep2306, loss: 3.747730016708374\nstep2307, loss: 3.698268175125122\nstep2308, loss: 3.575136661529541\nstep2309, loss: 3.6503517627716064\nstep2310, loss: 3.612877368927002\nstep2311, loss: 3.7740349769592285\nstep2312, loss: 3.460252523422241\nstep2313, loss: 3.50254487991333\nstep2314, loss: 3.752460479736328\nstep2315, loss: 3.423051357269287\nstep2316, loss: 3.596153974533081\nstep2317, loss: 3.3081400394439697\nstep2318, loss: 3.768911361694336\nstep2319, loss: 3.908113479614258\nstep2320, loss: 3.8624966144561768\nstep2321, loss: 3.8807883262634277\nstep2322, loss: 3.707794427871704\nstep2323, loss: 3.7566487789154053\nstep2324, loss: 3.7529354095458984\nstep2325, loss: 3.7194857597351074\nstep2326, loss: 3.6104679107666016\nstep2327, loss: 3.4796533584594727\nstep2328, loss: 3.576451539993286\nstep2329, loss: 3.7330236434936523\nstep2330, loss: 3.745718240737915\nstep2331, loss: 3.6819496154785156\nstep2332, loss: 3.5612711906433105\nstep2333, loss: 3.6763224601745605\nstep2334, loss: 3.596409797668457\nstep2335, loss: 3.450927495956421\nstep2336, loss: 3.4649341106414795\nstep2337, loss: 3.70119047164917\nstep2338, loss: 3.513617515563965\nstep2339, loss: 3.587019920349121\nstep2340, loss: 3.824312686920166\nstep2341, loss: 3.7345876693725586\nstep2342, loss: 3.720909595489502\nstep2343, loss: 3.437042713165283\nstep2344, loss: 3.640580654144287\nstep2345, loss: 3.5478625297546387\nstep2346, loss: 3.661151170730591\nstep2347, loss: 3.521127223968506\nstep2348, loss: 3.7765872478485107\nstep2349, loss: 3.6964919567108154\nstep2350, loss: 3.722663640975952\nstep2351, loss: 3.7830452919006348\nstep2352, loss: 3.920694351196289\nstep2353, loss: 3.800455331802368\nstep2354, loss: 3.6192829608917236\nstep2355, loss: 3.6177446842193604\nstep2356, loss: 3.820028305053711\nstep2357, loss: 3.7195212841033936\nstep2358, loss: 3.6015515327453613\nstep2359, loss: 3.3410820960998535\nstep2360, loss: 3.4649837017059326\nstep2361, loss: 3.4986062049865723\nstep2362, loss: 3.5380971431732178\nstep2363, loss: 3.4782211780548096\nstep2364, loss: 3.494339942932129\nstep2365, loss: 3.4146530628204346\nstep2366, loss: 3.350172758102417\nstep2367, loss: 3.8578248023986816\nstep2368, loss: 3.658324718475342\nstep2369, loss: 3.4262535572052\nstep2370, loss: 3.412450075149536\nstep2371, loss: 3.507326364517212\nstep2372, loss: 3.4403746128082275\nstep2373, loss: 3.2681450843811035\nstep2374, loss: 3.240835428237915\nstep2375, loss: 3.2392218112945557\nstep2376, loss: 3.770515203475952\nstep2377, loss: 3.6139025688171387\nstep2378, loss: 3.7597861289978027\nstep2379, loss: 3.6435182094573975\nstep2380, loss: 3.6362409591674805\nstep2381, loss: 3.589508295059204\nstep2382, loss: 3.473750114440918\nstep2383, loss: 3.2588188648223877\nstep2384, loss: 3.4597723484039307\nstep2385, loss: 3.436021327972412\nstep2386, loss: 3.4638891220092773\nstep2387, loss: 3.709651470184326\nstep2388, loss: 3.8065145015716553\nstep2389, loss: 3.7154195308685303\nstep2390, loss: 3.4885826110839844\nstep2391, loss: 3.513493061065674\nstep2392, loss: 3.4692859649658203\nstep2393, loss: 3.7276859283447266\nstep2394, loss: 3.4596147537231445\nstep2395, loss: 3.5184032917022705\nstep2396, loss: 3.7742116451263428\nstep2397, loss: 3.50461745262146\nstep2398, loss: 3.622338056564331\nstep2399, loss: 3.2836031913757324\nstep2400, loss: 3.6601076126098633\nstep2401, loss: 3.729140520095825\nstep2402, loss: 3.740583658218384\nstep2403, loss: 3.816770553588867\nstep2404, loss: 3.7114343643188477\nstep2405, loss: 3.8092801570892334\nstep2406, loss: 3.811709403991699\nstep2407, loss: 3.734895944595337\nstep2408, loss: 3.5826287269592285\nstep2409, loss: 3.449591875076294\nstep2410, loss: 3.4806389808654785\nstep2411, loss: 3.64430832862854\nstep2412, loss: 3.6941561698913574\nstep2413, loss: 3.6402206420898438\nstep2414, loss: 3.4874989986419678\nstep2415, loss: 3.6430325508117676\nstep2416, loss: 3.5645229816436768\nstep2417, loss: 3.4274861812591553\nstep2418, loss: 3.4465115070343018\nstep2419, loss: 3.6538782119750977\nstep2420, loss: 3.476823568344116\nstep2421, loss: 3.529326915740967\nstep2422, loss: 3.776324510574341\nstep2423, loss: 3.632180690765381\nstep2424, loss: 3.6112074851989746\nstep2425, loss: 3.323153257369995\nstep2426, loss: 3.588284969329834\nstep2427, loss: 3.5159080028533936\nstep2428, loss: 3.618997812271118\nstep2429, loss: 3.459165096282959\nstep2430, loss: 3.724050521850586\nstep2431, loss: 3.6187920570373535\nstep2432, loss: 3.6871914863586426\nstep2433, loss: 3.74086594581604\nstep2434, loss: 3.8156583309173584\nstep2435, loss: 3.707502603530884\nstep2436, loss: 3.485980987548828\nstep2437, loss: 3.5172863006591797\nstep2438, loss: 3.676607131958008\nstep2439, loss: 3.561734199523926\nstep2440, loss: 3.4655046463012695\nstep2441, loss: 3.2412495613098145\nstep2442, loss: 3.36185884475708\nstep2443, loss: 3.417506217956543\nstep2444, loss: 3.410182476043701\nstep2445, loss: 3.354062557220459\nstep2446, loss: 3.3734383583068848\nstep2447, loss: 3.2622268199920654\nstep2448, loss: 3.2173078060150146\nstep2449, loss: 3.7612388134002686\nstep2450, loss: 3.637129545211792\nstep2451, loss: 3.4036262035369873\nstep2452, loss: 3.3607964515686035\nstep2453, loss: 3.3865647315979004\nstep2454, loss: 3.2964441776275635\nstep2455, loss: 3.1046817302703857\nstep2456, loss: 3.167189359664917\nstep2457, loss: 3.207456588745117\nstep2458, loss: 3.678434371948242\nstep2459, loss: 3.5202174186706543\nstep2460, loss: 3.6684107780456543\nstep2461, loss: 3.506448984146118\nstep2462, loss: 3.5134084224700928\nstep2463, loss: 3.490021228790283\nstep2464, loss: 3.381136178970337\nstep2465, loss: 3.162693977355957\nstep2466, loss: 3.313209056854248\nstep2467, loss: 3.303988456726074\nstep2468, loss: 3.2936134338378906\nstep2469, loss: 3.465996742248535\nstep2470, loss: 3.606933116912842\nstep2471, loss: 3.55780291557312\nstep2472, loss: 3.4067559242248535\nstep2473, loss: 3.4507710933685303\nstep2474, loss: 3.3700668811798096\nstep2475, loss: 3.5605106353759766\nstep2476, loss: 3.291438341140747\nstep2477, loss: 3.2847490310668945\nstep2478, loss: 3.52522611618042\nstep2479, loss: 3.296574831008911\nstep2480, loss: 3.4654781818389893\nstep2481, loss: 3.2255678176879883\nstep2482, loss: 3.5758450031280518\nstep2483, loss: 3.7081823348999023\nstep2484, loss: 3.6916182041168213\nstep2485, loss: 3.6802520751953125\nstep2486, loss: 3.577817440032959\nstep2487, loss: 3.60947322845459\nstep2488, loss: 3.622612237930298\nstep2489, loss: 3.5739495754241943\nstep2490, loss: 3.509608507156372\nstep2491, loss: 3.414626359939575\nstep2492, loss: 3.488312244415283\nstep2493, loss: 3.693004608154297\nstep2494, loss: 3.7024736404418945\nstep2495, loss: 3.5502941608428955\nstep2496, loss: 3.410590648651123\nstep2497, loss: 3.589585542678833\nstep2498, loss: 3.5102455615997314\nstep2499, loss: 3.3852896690368652\nstep2500, loss: 3.371593713760376\nstep2501, loss: 3.6068966388702393\nstep2502, loss: 3.426551342010498\nstep2503, loss: 3.4491705894470215\nstep2504, loss: 3.6860597133636475\nstep2505, loss: 3.5446691513061523\nstep2506, loss: 3.57834792137146\nstep2507, loss: 3.2690744400024414\nstep2508, loss: 3.5182712078094482\nstep2509, loss: 3.390529155731201\nstep2510, loss: 3.4666006565093994\nstep2511, loss: 3.362558126449585\nstep2512, loss: 3.567638397216797\nstep2513, loss: 3.529944658279419\nstep2514, loss: 3.6256258487701416\nstep2515, loss: 3.7154269218444824\nstep2516, loss: 3.7495572566986084\nstep2517, loss: 3.65122389793396\nstep2518, loss: 3.4247100353240967\nstep2519, loss: 3.430246114730835\nstep2520, loss: 3.618048667907715\nstep2521, loss: 3.5125069618225098\nstep2522, loss: 3.4239728450775146\nstep2523, loss: 3.169978618621826\nstep2524, loss: 3.3045096397399902\nstep2525, loss: 3.3414926528930664\nstep2526, loss: 3.3138036727905273\nstep2527, loss: 3.2512288093566895\nstep2528, loss: 3.292752981185913\nstep2529, loss: 3.185798406600952\nstep2530, loss: 3.150296449661255\nstep2531, loss: 3.6604063510894775\nstep2532, loss: 3.433954954147339\nstep2533, loss: 3.213776111602783\nstep2534, loss: 3.206955671310425\nstep2535, loss: 3.2921764850616455\nstep2536, loss: 3.240126848220825\nstep2537, loss: 3.06276273727417\nstep2538, loss: 3.126795530319214\nstep2539, loss: 3.073979377746582\nstep2540, loss: 3.5510120391845703\nstep2541, loss: 3.4193344116210938\nstep2542, loss: 3.5381951332092285\nstep2543, loss: 3.431128740310669\nstep2544, loss: 3.42240571975708\nstep2545, loss: 3.471958637237549\nstep2546, loss: 3.3672947883605957\nstep2547, loss: 3.1368534564971924\nstep2548, loss: 3.2699790000915527\nstep2549, loss: 3.234107255935669\nstep2550, loss: 3.1917638778686523\nstep2551, loss: 3.3773088455200195\nstep2552, loss: 3.5109314918518066\nstep2553, loss: 3.4353785514831543\nstep2554, loss: 3.255986213684082\nstep2555, loss: 3.3329007625579834\nstep2556, loss: 3.2386560440063477\nstep2557, loss: 3.4585607051849365\nstep2558, loss: 3.1724817752838135\nstep2559, loss: 3.1991097927093506\nstep2560, loss: 3.4430959224700928\nstep2561, loss: 3.1757826805114746\nstep2562, loss: 3.3575167655944824\nstep2563, loss: 3.068268299102783\nstep2564, loss: 3.432767391204834\nstep2565, loss: 3.5394906997680664\nstep2566, loss: 3.549363613128662\nstep2567, loss: 3.5561912059783936\nstep2568, loss: 3.484541893005371\nstep2569, loss: 3.5197441577911377\nstep2570, loss: 3.494203567504883\nstep2571, loss: 3.4415762424468994\nstep2572, loss: 3.3710782527923584\nstep2573, loss: 3.25156831741333\nstep2574, loss: 3.338932752609253\nstep2575, loss: 3.522697687149048\nstep2576, loss: 3.5741095542907715\nstep2577, loss: 3.4647164344787598\nstep2578, loss: 3.3398640155792236\nstep2579, loss: 3.4602274894714355\nstep2580, loss: 3.4080393314361572\nstep2581, loss: 3.2750651836395264\nstep2582, loss: 3.2371842861175537\nstep2583, loss: 3.483236789703369\nstep2584, loss: 3.3310983180999756\nstep2585, loss: 3.431321144104004\nstep2586, loss: 3.658658027648926\nstep2587, loss: 3.513162136077881\nstep2588, loss: 3.5010969638824463\nstep2589, loss: 3.1506831645965576\nstep2590, loss: 3.3910157680511475\nstep2591, loss: 3.3085246086120605\nstep2592, loss: 3.410381317138672\nstep2593, loss: 3.280595302581787\nstep2594, loss: 3.4615790843963623\nstep2595, loss: 3.4208998680114746\nstep2596, loss: 3.531461000442505\nstep2597, loss: 3.6385886669158936\nstep2598, loss: 3.6688625812530518\nstep2599, loss: 3.577770471572876\nstep2600, loss: 3.386127471923828\nstep2601, loss: 3.356207847595215\nstep2602, loss: 3.5442299842834473\nstep2603, loss: 3.418649196624756\nstep2604, loss: 3.3159821033477783\nstep2605, loss: 3.06077241897583\nstep2606, loss: 3.251357316970825\nstep2607, loss: 3.3140106201171875\nstep2608, loss: 3.291602373123169\nstep2609, loss: 3.239105224609375\nstep2610, loss: 3.2073256969451904\nstep2611, loss: 3.1016762256622314\nstep2612, loss: 3.0668623447418213\nstep2613, loss: 3.5571141242980957\nstep2614, loss: 3.3591508865356445\nstep2615, loss: 3.1648383140563965\nstep2616, loss: 3.1499061584472656\nstep2617, loss: 3.225592851638794\nstep2618, loss: 3.15356183052063\nstep2619, loss: 2.9929111003875732\nstep2620, loss: 3.024976968765259\nstep2621, loss: 3.005178689956665\nstep2622, loss: 3.4862000942230225\nstep2623, loss: 3.325350761413574\nstep2624, loss: 3.4661452770233154\nstep2625, loss: 3.3196778297424316\nstep2626, loss: 3.350499391555786\nstep2627, loss: 3.313465118408203\nstep2628, loss: 3.2371795177459717\nstep2629, loss: 3.0641777515411377\nstep2630, loss: 3.206697463989258\nstep2631, loss: 3.1506435871124268\nstep2632, loss: 3.16424560546875\nstep2633, loss: 3.36983585357666\nstep2634, loss: 3.514209747314453\nstep2635, loss: 3.433928966522217\nstep2636, loss: 3.2349607944488525\nstep2637, loss: 3.263167381286621\nstep2638, loss: 3.1676461696624756\nstep2639, loss: 3.3906753063201904\nstep2640, loss: 3.116056442260742\nstep2641, loss: 3.1661438941955566\nstep2642, loss: 3.399296760559082\nstep2643, loss: 3.1197774410247803\nstep2644, loss: 3.304123640060425\nstep2645, loss: 3.0181710720062256\nstep2646, loss: 3.3527791500091553\nstep2647, loss: 3.462482213973999\nstep2648, loss: 3.446617841720581\nstep2649, loss: 3.4804253578186035\nstep2650, loss: 3.3628439903259277\nstep2651, loss: 3.429173707962036\nstep2652, loss: 3.4202775955200195\nstep2653, loss: 3.3741698265075684\nstep2654, loss: 3.314537525177002\nstep2655, loss: 3.20354962348938\nstep2656, loss: 3.2500829696655273\nstep2657, loss: 3.445108652114868\nstep2658, loss: 3.422762393951416\nstep2659, loss: 3.3637290000915527\nstep2660, loss: 3.2199203968048096\nstep2661, loss: 3.363861083984375\nstep2662, loss: 3.326913356781006\nstep2663, loss: 3.2431793212890625\nstep2664, loss: 3.191364049911499\nstep2665, loss: 3.4404006004333496\nstep2666, loss: 3.2563652992248535\nstep2667, loss: 3.326356887817383\nstep2668, loss: 3.514678955078125\nstep2669, loss: 3.3887035846710205\nstep2670, loss: 3.3900258541107178\nstep2671, loss: 3.1024842262268066\nstep2672, loss: 3.3476309776306152\nstep2673, loss: 3.2807939052581787\nstep2674, loss: 3.3565890789031982\nstep2675, loss: 3.224032402038574\nstep2676, loss: 3.4456958770751953\nstep2677, loss: 3.361706495285034\nstep2678, loss: 3.3998963832855225\nstep2679, loss: 3.4556267261505127\nstep2680, loss: 3.520930528640747\nstep2681, loss: 3.4818332195281982\nstep2682, loss: 3.313178062438965\nstep2683, loss: 3.3167760372161865\nstep2684, loss: 3.509934902191162\nstep2685, loss: 3.422748565673828\nstep2686, loss: 3.3059885501861572\nstep2687, loss: 3.0419981479644775\nstep2688, loss: 3.192599296569824\nstep2689, loss: 3.2115161418914795\nstep2690, loss: 3.160992383956909\nstep2691, loss: 3.1059060096740723\nstep2692, loss: 3.140068292617798\nstep2693, loss: 3.09972882270813\nstep2694, loss: 3.078455924987793\nstep2695, loss: 3.5871310234069824\nstep2696, loss: 3.4315097332000732\nstep2697, loss: 3.1876022815704346\nstep2698, loss: 3.107670783996582\nstep2699, loss: 3.1402525901794434\nstep2700, loss: 3.0914971828460693\nstep2701, loss: 2.9684882164001465\nstep2702, loss: 3.0133557319641113\nstep2703, loss: 3.0063493251800537\nstep2704, loss: 3.4949512481689453\nstep2705, loss: 3.326883554458618\nstep2706, loss: 3.443113088607788\nstep2707, loss: 3.315089702606201\nstep2708, loss: 3.297334671020508\nstep2709, loss: 3.2681846618652344\nstep2710, loss: 3.176378011703491\nstep2711, loss: 2.9854001998901367\nstep2712, loss: 3.1318695545196533\nstep2713, loss: 3.112415313720703\nstep2714, loss: 3.1079561710357666\nstep2715, loss: 3.2913944721221924\nstep2716, loss: 3.4626476764678955\nstep2717, loss: 3.433669090270996\nstep2718, loss: 3.2303364276885986\nstep2719, loss: 3.290318250656128\nstep2720, loss: 3.170746088027954\nstep2721, loss: 3.4142093658447266\nstep2722, loss: 3.079857349395752\nstep2723, loss: 3.131894111633301\nstep2724, loss: 3.3261139392852783\nstep2725, loss: 3.1029715538024902\nstep2726, loss: 3.2852299213409424\nstep2727, loss: 3.0149426460266113\nstep2728, loss: 3.382608652114868\nstep2729, loss: 3.503629207611084\nstep2730, loss: 3.4722588062286377\nstep2731, loss: 3.4741098880767822\nstep2732, loss: 3.3232123851776123\nstep2733, loss: 3.3828442096710205\nstep2734, loss: 3.3962016105651855\nstep2735, loss: 3.334083080291748\nstep2736, loss: 3.308696746826172\nstep2737, loss: 3.203498601913452\nstep2738, loss: 3.2522828578948975\nstep2739, loss: 3.3593058586120605\nstep2740, loss: 3.362574815750122\nstep2741, loss: 3.302718162536621\nstep2742, loss: 3.176758050918579\nstep2743, loss: 3.332498073577881\nstep2744, loss: 3.319427967071533\nstep2745, loss: 3.1488914489746094\nstep2746, loss: 3.1409590244293213\nstep2747, loss: 3.37699556350708\nstep2748, loss: 3.2590272426605225\nstep2749, loss: 3.336918354034424\nstep2750, loss: 3.5715296268463135\nstep2751, loss: 3.4055545330047607\nstep2752, loss: 3.3717920780181885\nstep2753, loss: 3.087705135345459\nstep2754, loss: 3.2903785705566406\nstep2755, loss: 3.2206554412841797\nstep2756, loss: 3.266662359237671\nstep2757, loss: 3.190095901489258\nstep2758, loss: 3.362623453140259\nstep2759, loss: 3.3093199729919434\nstep2760, loss: 3.402198076248169\nstep2761, loss: 3.4898574352264404\nstep2762, loss: 3.4912896156311035\nstep2763, loss: 3.3842926025390625\nstep2764, loss: 3.2137863636016846\nstep2765, loss: 3.2017998695373535\nstep2766, loss: 3.3883445262908936\nstep2767, loss: 3.349780797958374\nstep2768, loss: 3.242685556411743\nstep2769, loss: 2.96244740486145\nstep2770, loss: 3.125438928604126\nstep2771, loss: 3.1535468101501465\nstep2772, loss: 3.1055662631988525\nstep2773, loss: 3.0708377361297607\nstep2774, loss: 3.047396421432495\nstep2775, loss: 3.016361713409424\nstep2776, loss: 2.9945385456085205\nstep2777, loss: 3.4990155696868896\nstep2778, loss: 3.2608821392059326\nstep2779, loss: 3.0987606048583984\nstep2780, loss: 3.061265468597412\nstep2781, loss: 3.1166608333587646\nstep2782, loss: 3.049441337585449\nstep2783, loss: 2.8584909439086914\nstep2784, loss: 2.8960556983947754\nstep2785, loss: 2.9026098251342773\nstep2786, loss: 3.359812021255493\nstep2787, loss: 3.20080828666687\nstep2788, loss: 3.308962821960449\nstep2789, loss: 3.244441270828247\nstep2790, loss: 3.2405941486358643\nstep2791, loss: 3.2154927253723145\nstep2792, loss: 3.1217105388641357\nstep2793, loss: 2.866267442703247\nstep2794, loss: 3.0548057556152344\nstep2795, loss: 2.976693868637085\nstep2796, loss: 2.983511209487915\nstep2797, loss: 3.1466169357299805\nstep2798, loss: 3.3090908527374268\nstep2799, loss: 3.2478668689727783\nstep2800, loss: 3.070014476776123\nstep2801, loss: 3.1457366943359375\nstep2802, loss: 3.058037757873535\nstep2803, loss: 3.312685966491699\nstep2804, loss: 3.0302584171295166\nstep2805, loss: 3.051171064376831\nstep2806, loss: 3.2365171909332275\nstep2807, loss: 3.008791923522949\nstep2808, loss: 3.173161268234253\nstep2809, loss: 2.9006996154785156\nstep2810, loss: 3.2289538383483887\nstep2811, loss: 3.3418691158294678\nstep2812, loss: 3.362048625946045\nstep2813, loss: 3.406342029571533\nstep2814, loss: 3.316256284713745\nstep2815, loss: 3.4398679733276367\nstep2816, loss: 3.368337631225586\nstep2817, loss: 3.3075063228607178\nstep2818, loss: 3.1788387298583984\nstep2819, loss: 3.063673973083496\nstep2820, loss: 3.1254124641418457\nstep2821, loss: 3.3054587841033936\nstep2822, loss: 3.314216375350952\nstep2823, loss: 3.3198258876800537\nstep2824, loss: 3.2430498600006104\nstep2825, loss: 3.286829948425293\nstep2826, loss: 3.27340030670166\nstep2827, loss: 3.1160364151000977\nstep2828, loss: 3.0591373443603516\nstep2829, loss: 3.2998955249786377\nstep2830, loss: 3.1771764755249023\nstep2831, loss: 3.245326519012451\nstep2832, loss: 3.4276769161224365\nstep2833, loss: 3.315993070602417\nstep2834, loss: 3.3010928630828857\nstep2835, loss: 2.997438430786133\nstep2836, loss: 3.237765073776245\nstep2837, loss: 3.124652147293091\nstep2838, loss: 3.1819260120391846\nstep2839, loss: 3.103188991546631\nstep2840, loss: 3.311065196990967\nstep2841, loss: 3.214735984802246\nstep2842, loss: 3.257960081100464\nstep2843, loss: 3.3441107273101807\nstep2844, loss: 3.3663930892944336\nstep2845, loss: 3.301060438156128\nstep2846, loss: 3.149399757385254\nstep2847, loss: 3.129039764404297\nstep2848, loss: 3.307692289352417\nstep2849, loss: 3.225156784057617\nstep2850, loss: 3.0879786014556885\nstep2851, loss: 2.8357768058776855\nstep2852, loss: 3.0171844959259033\nstep2853, loss: 3.0464210510253906\nstep2854, loss: 3.0213258266448975\nstep2855, loss: 2.985239267349243\nstep2856, loss: 3.0096137523651123\nstep2857, loss: 2.9138824939727783\nstep2858, loss: 2.850163459777832\nstep2859, loss: 3.3680126667022705\nstep2860, loss: 3.129232168197632\nstep2861, loss: 2.9505436420440674\nstep2862, loss: 2.9040985107421875\nstep2863, loss: 3.016062021255493\nstep2864, loss: 2.9220259189605713\nstep2865, loss: 2.8425326347351074\nstep2866, loss: 2.8758809566497803\nstep2867, loss: 2.8587658405303955\nstep2868, loss: 3.3182284832000732\nstep2869, loss: 3.1628191471099854\nstep2870, loss: 3.251664876937866\nstep2871, loss: 3.0937469005584717\nstep2872, loss: 3.0777838230133057\nstep2873, loss: 3.111717700958252\nstep2874, loss: 3.038938283920288\nstep2875, loss: 2.907586097717285\nstep2876, loss: 3.075545310974121\nstep2877, loss: 3.013688087463379\nstep2878, loss: 2.998364210128784\nstep2879, loss: 3.0961313247680664\nstep2880, loss: 3.2113561630249023\nstep2881, loss: 3.1651203632354736\nstep2882, loss: 2.9811599254608154\nstep2883, loss: 3.0474162101745605\nstep2884, loss: 2.9648804664611816\nstep2885, loss: 3.19474196434021\nstep2886, loss: 2.939290761947632\nstep2887, loss: 2.927577257156372\nstep2888, loss: 3.151350498199463\nstep2889, loss: 2.9397239685058594\nstep2890, loss: 3.1042327880859375\nstep2891, loss: 2.8400464057922363\nstep2892, loss: 3.1820898056030273\nstep2893, loss: 3.240158796310425\nstep2894, loss: 3.23175048828125\nstep2895, loss: 3.2750442028045654\nstep2896, loss: 3.1045799255371094\nstep2897, loss: 3.2260282039642334\nstep2898, loss: 3.212810754776001\nstep2899, loss: 3.20339035987854\nstep2900, loss: 3.178809404373169\nstep2901, loss: 3.0644140243530273\nstep2902, loss: 3.1143453121185303\nstep2903, loss: 3.247990369796753\nstep2904, loss: 3.2267234325408936\nstep2905, loss: 3.1557729244232178\nstep2906, loss: 3.0597832202911377\nstep2907, loss: 3.14668345451355\nstep2908, loss: 3.120264768600464\nstep2909, loss: 3.0521798133850098\nstep2910, loss: 3.0463643074035645\nstep2911, loss: 3.2608489990234375\nstep2912, loss: 3.1454219818115234\nstep2913, loss: 3.200357437133789\nstep2914, loss: 3.3652453422546387\nstep2915, loss: 3.1985673904418945\nstep2916, loss: 3.1859002113342285\nstep2917, loss: 2.902837038040161\nstep2918, loss: 3.12697696685791\nstep2919, loss: 3.050013542175293\nstep2920, loss: 3.1271119117736816\nstep2921, loss: 3.020348310470581\nstep2922, loss: 3.1999216079711914\nstep2923, loss: 3.155144214630127\nstep2924, loss: 3.2122039794921875\nstep2925, loss: 3.248579502105713\nstep2926, loss: 3.2555739879608154\nstep2927, loss: 3.2013025283813477\nstep2928, loss: 3.0364558696746826\nstep2929, loss: 3.039276599884033\nstep2930, loss: 3.229706048965454\nstep2931, loss: 3.1411285400390625\nstep2932, loss: 3.0115630626678467\nstep2933, loss: 2.765507221221924\nstep2934, loss: 2.9511501789093018\nstep2935, loss: 2.953155755996704\nstep2936, loss: 2.9134724140167236\nstep2937, loss: 2.8782687187194824\nstep2938, loss: 2.8748738765716553\nstep2939, loss: 2.8070147037506104\nstep2940, loss: 2.8011794090270996\nstep2941, loss: 3.2864224910736084\nstep2942, loss: 3.0667500495910645\nstep2943, loss: 2.868856430053711\nstep2944, loss: 2.793977975845337\nstep2945, loss: 2.892155647277832\nstep2946, loss: 2.873112440109253\nstep2947, loss: 2.7512331008911133\nstep2948, loss: 2.823910713195801\nstep2949, loss: 2.8052639961242676\nstep2950, loss: 3.2472565174102783\nstep2951, loss: 3.1076760292053223\nstep2952, loss: 3.2474567890167236\nstep2953, loss: 3.139280319213867\nstep2954, loss: 3.1211764812469482\nstep2955, loss: 3.1262781620025635\nstep2956, loss: 2.989612102508545\nstep2957, loss: 2.8313045501708984\nstep2958, loss: 2.9667041301727295\nstep2959, loss: 2.9097747802734375\nstep2960, loss: 2.9525835514068604\nstep2961, loss: 3.1186137199401855\nstep2962, loss: 3.2514536380767822\nstep2963, loss: 3.248969078063965\nstep2964, loss: 3.0523157119750977\nstep2965, loss: 3.0491111278533936\nstep2966, loss: 2.9294772148132324\nstep2967, loss: 3.1749844551086426\nstep2968, loss: 2.851642608642578\nstep2969, loss: 2.8918912410736084\nstep2970, loss: 3.108748197555542\nstep2971, loss: 2.901797294616699\nstep2972, loss: 3.0482709407806396\nstep2973, loss: 2.7637741565704346\nstep2974, loss: 3.111243724822998\nstep2975, loss: 3.1717119216918945\nstep2976, loss: 3.223790407180786\nstep2977, loss: 3.234527111053467\nstep2978, loss: 3.0814523696899414\nstep2979, loss: 3.177241802215576\nstep2980, loss: 3.155820846557617\nstep2981, loss: 3.107717752456665\nstep2982, loss: 3.045156240463257\nstep2983, loss: 2.942230701446533\nstep2984, loss: 2.9545724391937256\nstep2985, loss: 3.0916357040405273\nstep2986, loss: 3.1182281970977783\nstep2987, loss: 3.112858772277832\nstep2988, loss: 2.978842258453369\nstep2989, loss: 3.0606799125671387\nstep2990, loss: 3.007138252258301\nstep2991, loss: 2.910306930541992\nstep2992, loss: 2.8689167499542236\nstep2993, loss: 3.0943362712860107\nstep2994, loss: 3.014322519302368\nstep2995, loss: 3.084321975708008\nstep2996, loss: 3.300737142562866\nstep2997, loss: 3.159693717956543\nstep2998, loss: 3.1582584381103516\nstep2999, loss: 2.8791539669036865\nstep3000, loss: 3.0635886192321777\nstep3001, loss: 2.9693186283111572\nstep3002, loss: 3.0088438987731934\nstep3003, loss: 2.9165878295898438\nstep3004, loss: 3.0763070583343506\nstep3005, loss: 3.029042959213257\nstep3006, loss: 3.092910051345825\nstep3007, loss: 3.1663153171539307\nstep3008, loss: 3.146494150161743\nstep3009, loss: 3.0671889781951904\nstep3010, loss: 2.9568135738372803\nstep3011, loss: 2.9072234630584717\nstep3012, loss: 3.100252151489258\nstep3013, loss: 2.9795358180999756\nstep3014, loss: 2.865257978439331\nstep3015, loss: 2.635958194732666\nstep3016, loss: 2.8592326641082764\nstep3017, loss: 2.865510940551758\nstep3018, loss: 2.8229241371154785\nstep3019, loss: 2.775603771209717\nstep3020, loss: 2.79194712638855\nstep3021, loss: 2.7271084785461426\nstep3022, loss: 2.6823365688323975\nstep3023, loss: 3.1495132446289062\nstep3024, loss: 2.951507806777954\nstep3025, loss: 2.7774527072906494\nstep3026, loss: 2.7186028957366943\nstep3027, loss: 2.7813873291015625\nstep3028, loss: 2.7429378032684326\nstep3029, loss: 2.6049864292144775\nstep3030, loss: 2.6600756645202637\nstep3031, loss: 2.6859235763549805\nstep3032, loss: 3.1395931243896484\nstep3033, loss: 2.9771971702575684\nstep3034, loss: 3.128592014312744\nstep3035, loss: 3.0318000316619873\nstep3036, loss: 3.004469871520996\nstep3037, loss: 2.967674493789673\nstep3038, loss: 2.881178140640259\nstep3039, loss: 2.6838247776031494\nstep3040, loss: 2.8802382946014404\nstep3041, loss: 2.7787346839904785\nstep3042, loss: 2.8123393058776855\nstep3043, loss: 2.9506564140319824\nstep3044, loss: 3.132688283920288\nstep3045, loss: 3.0617146492004395\nstep3046, loss: 2.9290049076080322\nstep3047, loss: 2.9581618309020996\nstep3048, loss: 2.810631036758423\nstep3049, loss: 3.068927764892578\nstep3050, loss: 2.8195412158966064\nstep3051, loss: 2.8314361572265625\nstep3052, loss: 3.0355405807495117\nstep3053, loss: 2.815417528152466\nstep3054, loss: 2.9614741802215576\nstep3055, loss: 2.692063331604004\nstep3056, loss: 2.970014810562134\nstep3057, loss: 3.07124400138855\nstep3058, loss: 3.091858386993408\nstep3059, loss: 3.12516713142395\nstep3060, loss: 3.0564260482788086\nstep3061, loss: 3.1238701343536377\nstep3062, loss: 3.097646951675415\nstep3063, loss: 3.0375900268554688\nstep3064, loss: 2.9848580360412598\nstep3065, loss: 2.901918649673462\nstep3066, loss: 2.905482530593872\nstep3067, loss: 3.0538082122802734\nstep3068, loss: 3.057176113128662\nstep3069, loss: 2.9577348232269287\nstep3070, loss: 2.8442306518554688\nstep3071, loss: 2.9641826152801514\nstep3072, loss: 2.9334585666656494\nstep3073, loss: 2.89076566696167\nstep3074, loss: 2.8366918563842773\nstep3075, loss: 3.0351643562316895\nstep3076, loss: 2.9432661533355713\nstep3077, loss: 2.9822452068328857\nstep3078, loss: 3.152524471282959\nstep3079, loss: 3.013014316558838\nstep3080, loss: 3.0523793697357178\nstep3081, loss: 2.812913656234741\nstep3082, loss: 3.039379358291626\nstep3083, loss: 2.92965030670166\nstep3084, loss: 2.977355718612671\nstep3085, loss: 2.8983266353607178\nstep3086, loss: 3.093066930770874\nstep3087, loss: 2.987466335296631\nstep3088, loss: 3.0511550903320312\nstep3089, loss: 3.1274352073669434\nstep3090, loss: 3.0879619121551514\nstep3091, loss: 3.0210914611816406\nstep3092, loss: 2.877906084060669\nstep3093, loss: 2.8634283542633057\nstep3094, loss: 3.067854642868042\nstep3095, loss: 2.9838435649871826\nstep3096, loss: 2.8252124786376953\nstep3097, loss: 2.616433620452881\nstep3098, loss: 2.8159875869750977\nstep3099, loss: 2.7991483211517334\nstep3100, loss: 2.7388477325439453\nstep3101, loss: 2.6881842613220215\nstep3102, loss: 2.7185704708099365\nstep3103, loss: 2.651353120803833\nstep3104, loss: 2.6461257934570312\nstep3105, loss: 3.0810961723327637\nstep3106, loss: 2.901914596557617\nstep3107, loss: 2.716984510421753\nstep3108, loss: 2.648383617401123\nstep3109, loss: 2.696826934814453\nstep3110, loss: 2.624300479888916\nstep3111, loss: 2.514838218688965\nstep3112, loss: 2.5915303230285645\nstep3113, loss: 2.587368965148926\nstep3114, loss: 2.989086151123047\nstep3115, loss: 2.843475580215454\nstep3116, loss: 3.0463356971740723\nstep3117, loss: 2.928006172180176\nstep3118, loss: 2.910586357116699\nstep3119, loss: 2.8714141845703125\nstep3120, loss: 2.7952075004577637\nstep3121, loss: 2.621464490890503\nstep3122, loss: 2.7886581420898438\nstep3123, loss: 2.701390266418457\nstep3124, loss: 2.692105770111084\nstep3125, loss: 2.8613123893737793\nstep3126, loss: 3.0234808921813965\nstep3127, loss: 3.006138801574707\nstep3128, loss: 2.8921711444854736\nstep3129, loss: 2.9420697689056396\nstep3130, loss: 2.7202541828155518\nstep3131, loss: 3.0578091144561768\nstep3132, loss: 2.809809923171997\nstep3133, loss: 2.794759750366211\nstep3134, loss: 2.9724297523498535\nstep3135, loss: 2.773984909057617\nstep3136, loss: 2.916147470474243\nstep3137, loss: 2.6932485103607178\nstep3138, loss: 2.923637866973877\nstep3139, loss: 3.064493179321289\nstep3140, loss: 3.0550363063812256\nstep3141, loss: 3.0571982860565186\nstep3142, loss: 2.9516761302948\nstep3143, loss: 3.047560453414917\nstep3144, loss: 3.069418430328369\nstep3145, loss: 3.001655101776123\nstep3146, loss: 3.0101752281188965\nstep3147, loss: 2.8639187812805176\nstep3148, loss: 2.881042003631592\nstep3149, loss: 3.0367462635040283\nstep3150, loss: 2.999579668045044\nstep3151, loss: 2.975454330444336\nstep3152, loss: 2.8564131259918213\nstep3153, loss: 2.932891607284546\nstep3154, loss: 2.9014556407928467\nstep3155, loss: 2.8156626224517822\nstep3156, loss: 2.7307980060577393\nstep3157, loss: 3.005625009536743\nstep3158, loss: 2.940891742706299\nstep3159, loss: 3.0099315643310547\nstep3160, loss: 3.1945247650146484\nstep3161, loss: 3.0414743423461914\nstep3162, loss: 3.0275895595550537\nstep3163, loss: 2.75598406791687\nstep3164, loss: 2.946376323699951\nstep3165, loss: 2.8859450817108154\nstep3166, loss: 2.9945740699768066\nstep3167, loss: 2.944282054901123\nstep3168, loss: 3.0788707733154297\nstep3169, loss: 3.0419468879699707\nstep3170, loss: 3.064241886138916\nstep3171, loss: 3.161020278930664\nstep3172, loss: 3.1346216201782227\nstep3173, loss: 3.0209665298461914\nstep3174, loss: 2.871091842651367\nstep3175, loss: 2.8165340423583984\nstep3176, loss: 3.0463685989379883\nstep3177, loss: 2.9622175693511963\nstep3178, loss: 2.818037271499634\nstep3179, loss: 2.617969036102295\nstep3180, loss: 2.8641538619995117\nstep3181, loss: 2.834441900253296\nstep3182, loss: 2.754520893096924\nstep3183, loss: 2.7109766006469727\nstep3184, loss: 2.647101402282715\nstep3185, loss: 2.642198085784912\nstep3186, loss: 2.6204946041107178\nstep3187, loss: 3.0408427715301514\nstep3188, loss: 2.860243320465088\nstep3189, loss: 2.71450138092041\nstep3190, loss: 2.6426961421966553\nstep3191, loss: 2.676358699798584\nstep3192, loss: 2.599369525909424\nstep3193, loss: 2.502821207046509\nstep3194, loss: 2.539186716079712\nstep3195, loss: 2.5128819942474365\nstep3196, loss: 2.9623076915740967\nstep3197, loss: 2.7886099815368652\nstep3198, loss: 2.921715021133423\nstep3199, loss: 2.8494136333465576\nstep3200, loss: 2.801569938659668\nstep3201, loss: 2.8275389671325684\nstep3202, loss: 2.7303671836853027\nstep3203, loss: 2.5387604236602783\nstep3204, loss: 2.7226476669311523\nstep3205, loss: 2.65834641456604\nstep3206, loss: 2.6474609375\nstep3207, loss: 2.809041976928711\nstep3208, loss: 2.9239625930786133\nstep3209, loss: 2.8602776527404785\nstep3210, loss: 2.729701042175293\nstep3211, loss: 2.7831103801727295\nstep3212, loss: 2.633881092071533\nstep3213, loss: 2.9274487495422363\nstep3214, loss: 2.692888021469116\nstep3215, loss: 2.7213330268859863\nstep3216, loss: 2.9253528118133545\nstep3217, loss: 2.713054656982422\nstep3218, loss: 2.842167615890503\nstep3219, loss: 2.6043591499328613\nstep3220, loss: 2.873727321624756\nstep3221, loss: 2.9832732677459717\nstep3222, loss: 3.0105738639831543\nstep3223, loss: 3.0106558799743652\nstep3224, loss: 2.9606995582580566\nstep3225, loss: 3.008232831954956\nstep3226, loss: 2.991872549057007\nstep3227, loss: 2.9191172122955322\nstep3228, loss: 2.87131929397583\nstep3229, loss: 2.8272101879119873\nstep3230, loss: 2.838549852371216\nstep3231, loss: 2.999746322631836\nstep3232, loss: 2.9405765533447266\nstep3233, loss: 2.932617664337158\nstep3234, loss: 2.8270423412323\nstep3235, loss: 2.9379563331604004\nstep3236, loss: 2.9067230224609375\nstep3237, loss: 2.8218603134155273\nstep3238, loss: 2.763164520263672\nstep3239, loss: 2.9532485008239746\nstep3240, loss: 2.8600494861602783\nstep3241, loss: 2.9030508995056152\nstep3242, loss: 3.1228182315826416\nstep3243, loss: 2.964005470275879\nstep3244, loss: 2.979024887084961\nstep3245, loss: 2.7352662086486816\nstep3246, loss: 2.9318277835845947\nstep3247, loss: 2.8469643592834473\nstep3248, loss: 2.9219603538513184\nstep3249, loss: 2.8439013957977295\nstep3250, loss: 3.0111050605773926\nstep3251, loss: 2.91689395904541\nstep3252, loss: 3.028118848800659\nstep3253, loss: 3.0757598876953125\nstep3254, loss: 3.093385696411133\nstep3255, loss: 3.00920033454895\nstep3256, loss: 2.889333724975586\nstep3257, loss: 2.8493409156799316\nstep3258, loss: 3.030761241912842\nstep3259, loss: 2.939194679260254\nstep3260, loss: 2.7690834999084473\nstep3261, loss: 2.575899600982666\nstep3262, loss: 2.749035596847534\nstep3263, loss: 2.768336772918701\nstep3264, loss: 2.73077392578125\nstep3265, loss: 2.6966965198516846\nstep3266, loss: 2.723905086517334\nstep3267, loss: 2.7274138927459717\nstep3268, loss: 2.7050721645355225\nstep3269, loss: 3.175236940383911\nstep3270, loss: 2.913095712661743\nstep3271, loss: 2.691779613494873\nstep3272, loss: 2.6287105083465576\nstep3273, loss: 2.642484664916992\nstep3274, loss: 2.5937907695770264\nstep3275, loss: 2.487935781478882\nstep3276, loss: 2.5427417755126953\nstep3277, loss: 2.504560947418213\nstep3278, loss: 2.846146583557129\nstep3279, loss: 2.7301785945892334\nstep3280, loss: 2.879693031311035\nstep3281, loss: 2.798137903213501\nstep3282, loss: 2.7854487895965576\nstep3283, loss: 2.790637493133545\nstep3284, loss: 2.7087056636810303\nstep3285, loss: 2.503305435180664\nstep3286, loss: 2.687129020690918\nstep3287, loss: 2.6185483932495117\nstep3288, loss: 2.6160430908203125\nstep3289, loss: 2.779700994491577\nstep3290, loss: 2.876478672027588\nstep3291, loss: 2.8532967567443848\nstep3292, loss: 2.770094633102417\nstep3293, loss: 2.822387456893921\nstep3294, loss: 2.636958599090576\nstep3295, loss: 2.914719581604004\nstep3296, loss: 2.643420457839966\nstep3297, loss: 2.5921366214752197\nstep3298, loss: 2.8112738132476807\nstep3299, loss: 2.664182662963867\nstep3300, loss: 2.812941789627075\nstep3301, loss: 2.60558819770813\nstep3302, loss: 2.8765180110931396\nstep3303, loss: 2.9901981353759766\nstep3304, loss: 2.9983832836151123\nstep3305, loss: 2.9734315872192383\nstep3306, loss: 2.899332046508789\nstep3307, loss: 2.9479713439941406\nstep3308, loss: 2.93622088432312\nstep3309, loss: 2.8850555419921875\nstep3310, loss: 2.8339056968688965\nstep3311, loss: 2.8024964332580566\nstep3312, loss: 2.833695888519287\nstep3313, loss: 2.9708828926086426\nstep3314, loss: 2.931990385055542\nstep3315, loss: 2.922229528427124\nstep3316, loss: 2.8116016387939453\nstep3317, loss: 2.8920693397521973\nstep3318, loss: 2.8665738105773926\nstep3319, loss: 2.793774127960205\nstep3320, loss: 2.775158166885376\nstep3321, loss: 2.9845285415649414\nstep3322, loss: 2.881448984146118\nstep3323, loss: 2.9476521015167236\nstep3324, loss: 3.13875412940979\nstep3325, loss: 2.957737684249878\nstep3326, loss: 2.980992078781128\nstep3327, loss: 2.6920878887176514\nstep3328, loss: 2.89456844329834\nstep3329, loss: 2.845122814178467\nstep3330, loss: 2.9020984172821045\nstep3331, loss: 2.834211587905884\nstep3332, loss: 2.905768394470215\nstep3333, loss: 2.8670599460601807\nstep3334, loss: 2.9448764324188232\nstep3335, loss: 2.9931998252868652\nstep3336, loss: 2.9875521659851074\nstep3337, loss: 2.927982807159424\nstep3338, loss: 2.801222562789917\nstep3339, loss: 2.7403972148895264\nstep3340, loss: 2.8933844566345215\nstep3341, loss: 2.8425745964050293\nstep3342, loss: 2.7105348110198975\nstep3343, loss: 2.4934778213500977\nstep3344, loss: 2.6773576736450195\nstep3345, loss: 2.6786699295043945\nstep3346, loss: 2.6757872104644775\nstep3347, loss: 2.597099542617798\nstep3348, loss: 2.6178417205810547\nstep3349, loss: 2.5598700046539307\nstep3350, loss: 2.5833847522735596\nstep3351, loss: 3.036160707473755\nstep3352, loss: 2.86746883392334\nstep3353, loss: 2.685060501098633\nstep3354, loss: 2.6275503635406494\nstep3355, loss: 2.6531693935394287\nstep3356, loss: 2.6229195594787598\nstep3357, loss: 2.484044313430786\nstep3358, loss: 2.490565538406372\nstep3359, loss: 2.460339069366455\nstep3360, loss: 2.7961251735687256\nstep3361, loss: 2.705923318862915\nstep3362, loss: 2.847269296646118\nstep3363, loss: 2.7553868293762207\nstep3364, loss: 2.7127184867858887\nstep3365, loss: 2.719693183898926\nstep3366, loss: 2.631187915802002\nstep3367, loss: 2.4671669006347656\nstep3368, loss: 2.624971866607666\nstep3369, loss: 2.5322608947753906\nstep3370, loss: 2.573234796524048\nstep3371, loss: 2.69345760345459\nstep3372, loss: 2.8273584842681885\nstep3373, loss: 2.8015244007110596\nstep3374, loss: 2.673081159591675\nstep3375, loss: 2.7482168674468994\nstep3376, loss: 2.5699737071990967\nstep3377, loss: 2.86826753616333\nstep3378, loss: 2.5572760105133057\nstep3379, loss: 2.553384780883789\nstep3380, loss: 2.7543528079986572\nstep3381, loss: 2.562035322189331\nstep3382, loss: 2.699981212615967\nstep3383, loss: 2.437587261199951\nstep3384, loss: 2.7349610328674316\nstep3385, loss: 2.854274272918701\nstep3386, loss: 2.8551597595214844\nstep3387, loss: 2.8666419982910156\nstep3388, loss: 2.8134841918945312\nstep3389, loss: 2.866334915161133\nstep3390, loss: 2.8101892471313477\nstep3391, loss: 2.763070583343506\nstep3392, loss: 2.7268753051757812\nstep3393, loss: 2.6561105251312256\nstep3394, loss: 2.69966721534729\nstep3395, loss: 2.8113627433776855\nstep3396, loss: 2.8129236698150635\nstep3397, loss: 2.7930588722229004\nstep3398, loss: 2.703660726547241\nstep3399, loss: 2.806049108505249\nstep3400, loss: 2.722824811935425\nstep3401, loss: 2.671988010406494\nstep3402, loss: 2.6247572898864746\nstep3403, loss: 2.863494396209717\nstep3404, loss: 2.8019471168518066\nstep3405, loss: 2.838813304901123\nstep3406, loss: 3.0868637561798096\nstep3407, loss: 2.915261745452881\nstep3408, loss: 2.9467852115631104\nstep3409, loss: 2.624737501144409\nstep3410, loss: 2.871439218521118\nstep3411, loss: 2.74310040473938\nstep3412, loss: 2.773566484451294\nstep3413, loss: 2.767651319503784\nstep3414, loss: 2.8428330421447754\nstep3415, loss: 2.813931465148926\nstep3416, loss: 2.9077630043029785\nstep3417, loss: 2.9784202575683594\nstep3418, loss: 2.9210329055786133\nstep3419, loss: 2.793313980102539\nstep3420, loss: 2.7165677547454834\nstep3421, loss: 2.6588993072509766\nstep3422, loss: 2.85422682762146\nstep3423, loss: 2.756063938140869\nstep3424, loss: 2.6093127727508545\nstep3425, loss: 2.3882479667663574\nstep3426, loss: 2.5809638500213623\nstep3427, loss: 2.5517094135284424\nstep3428, loss: 2.5370311737060547\nstep3429, loss: 2.469480514526367\nstep3430, loss: 2.5174078941345215\nstep3431, loss: 2.471503257751465\nstep3432, loss: 2.447964668273926\nstep3433, loss: 2.8637306690216064\nstep3434, loss: 2.723600149154663\nstep3435, loss: 2.531130790710449\nstep3436, loss: 2.437288761138916\nstep3437, loss: 2.500088930130005\nstep3438, loss: 2.5040512084960938\nstep3439, loss: 2.400383710861206\nstep3440, loss: 2.4700331687927246\nstep3441, loss: 2.4702773094177246\nstep3442, loss: 2.8791663646698\nstep3443, loss: 2.705909252166748\nstep3444, loss: 2.84885835647583\nstep3445, loss: 2.696836233139038\nstep3446, loss: 2.6824498176574707\nstep3447, loss: 2.624694585800171\nstep3448, loss: 2.568467617034912\nstep3449, loss: 2.418290853500366\nstep3450, loss: 2.6195578575134277\nstep3451, loss: 2.548917770385742\nstep3452, loss: 2.5432682037353516\nstep3453, loss: 2.6921632289886475\nstep3454, loss: 2.8209614753723145\nstep3455, loss: 2.767794370651245\nstep3456, loss: 2.6277081966400146\nstep3457, loss: 2.7286877632141113\nstep3458, loss: 2.5972890853881836\nstep3459, loss: 2.802762269973755\nstep3460, loss: 2.5240070819854736\nstep3461, loss: 2.5053577423095703\nstep3462, loss: 2.6740071773529053\nstep3463, loss: 2.5393524169921875\nstep3464, loss: 2.6654415130615234\nstep3465, loss: 2.3925113677978516\nstep3466, loss: 2.700045585632324\nstep3467, loss: 2.7937769889831543\nstep3468, loss: 2.803205728530884\nstep3469, loss: 2.7755184173583984\nstep3470, loss: 2.6611218452453613\nstep3471, loss: 2.769569158554077\nstep3472, loss: 2.75980544090271\nstep3473, loss: 2.7155497074127197\nstep3474, loss: 2.7110259532928467\nstep3475, loss: 2.619734764099121\nstep3476, loss: 2.6496715545654297\nstep3477, loss: 2.836939573287964\nstep3478, loss: 2.7759218215942383\nstep3479, loss: 2.6973845958709717\nstep3480, loss: 2.608384847640991\nstep3481, loss: 2.6648027896881104\nstep3482, loss: 2.6447699069976807\nstep3483, loss: 2.603317975997925\nstep3484, loss: 2.5530238151550293\nstep3485, loss: 2.7837600708007812\nstep3486, loss: 2.764470338821411\nstep3487, loss: 2.787726640701294\nstep3488, loss: 2.979576826095581\nstep3489, loss: 2.806689500808716\nstep3490, loss: 2.8249197006225586\nstep3491, loss: 2.5332748889923096\nstep3492, loss: 2.7769429683685303\nstep3493, loss: 2.66341233253479\nstep3494, loss: 2.728443145751953\nstep3495, loss: 2.7084670066833496\nstep3496, loss: 2.8391151428222656\nstep3497, loss: 2.753432035446167\nstep3498, loss: 2.850231170654297\nstep3499, loss: 2.8934316635131836\nstep3500, loss: 2.8353896141052246\nstep3501, loss: 2.775259494781494\nstep3502, loss: 2.6936323642730713\nstep3503, loss: 2.663156270980835\nstep3504, loss: 2.823190689086914\nstep3505, loss: 2.7480931282043457\nstep3506, loss: 2.598757028579712\nstep3507, loss: 2.43892765045166\nstep3508, loss: 2.549778699874878\nstep3509, loss: 2.5337023735046387\nstep3510, loss: 2.4718432426452637\nstep3511, loss: 2.425022840499878\nstep3512, loss: 2.469144582748413\nstep3513, loss: 2.445158004760742\nstep3514, loss: 2.401564836502075\nstep3515, loss: 2.778052806854248\nstep3516, loss: 2.5827207565307617\nstep3517, loss: 2.42171049118042\nstep3518, loss: 2.364712953567505\nstep3519, loss: 2.4196622371673584\nstep3520, loss: 2.3923356533050537\nstep3521, loss: 2.299084186553955\nstep3522, loss: 2.3500173091888428\nstep3523, loss: 2.316892385482788\nstep3524, loss: 2.7217342853546143\nstep3525, loss: 2.5980231761932373\nstep3526, loss: 2.8129091262817383\nstep3527, loss: 2.70981502532959\nstep3528, loss: 2.7414307594299316\nstep3529, loss: 2.6669812202453613\nstep3530, loss: 2.6214497089385986\nstep3531, loss: 2.413299083709717\nstep3532, loss: 2.5116186141967773\nstep3533, loss: 2.433544397354126\nstep3534, loss: 2.4624834060668945\nstep3535, loss: 2.6299567222595215\nstep3536, loss: 2.8069348335266113\nstep3537, loss: 2.7613298892974854\nstep3538, loss: 2.66959810256958\nstep3539, loss: 2.71541690826416\nstep3540, loss: 2.5256357192993164\nstep3541, loss: 2.824366331100464\nstep3542, loss: 2.51397442817688\nstep3543, loss: 2.5003809928894043\nstep3544, loss: 2.672328472137451\nstep3545, loss: 2.518059253692627\nstep3546, loss: 2.5910120010375977\nstep3547, loss: 2.383788824081421\nstep3548, loss: 2.673624277114868\nstep3549, loss: 2.783118486404419\nstep3550, loss: 2.7621445655822754\nstep3551, loss: 2.7486321926116943\nstep3552, loss: 2.6351871490478516\nstep3553, loss: 2.724182605743408\nstep3554, loss: 2.700061798095703\nstep3555, loss: 2.655888319015503\nstep3556, loss: 2.6182215213775635\nstep3557, loss: 2.5378594398498535\nstep3558, loss: 2.532538652420044\nstep3559, loss: 2.66033673286438\nstep3560, loss: 2.668595552444458\nstep3561, loss: 2.657552480697632\nstep3562, loss: 2.536611318588257\nstep3563, loss: 2.5844476222991943\nstep3564, loss: 2.5703067779541016\nstep3565, loss: 2.520893096923828\nstep3566, loss: 2.434290647506714\nstep3567, loss: 2.648705244064331\nstep3568, loss: 2.591742753982544\nstep3569, loss: 2.668381690979004\nstep3570, loss: 2.8602681159973145\nstep3571, loss: 2.7090890407562256\nstep3572, loss: 2.7176315784454346\nstep3573, loss: 2.4684855937957764\nstep3574, loss: 2.662142038345337\nstep3575, loss: 2.611759901046753\nstep3576, loss: 2.627256393432617\nstep3577, loss: 2.6009037494659424\nstep3578, loss: 2.7278494834899902\nstep3579, loss: 2.6433393955230713\nstep3580, loss: 2.7277255058288574\nstep3581, loss: 2.776211977005005\nstep3582, loss: 2.7388577461242676\nstep3583, loss: 2.6237215995788574\nstep3584, loss: 2.565746545791626\nstep3585, loss: 2.5394341945648193\nstep3586, loss: 2.6797380447387695\nstep3587, loss: 2.6390583515167236\nstep3588, loss: 2.5047552585601807\nstep3589, loss: 2.3205130100250244\nstep3590, loss: 2.508627414703369\nstep3591, loss: 2.526651382446289\nstep3592, loss: 2.443976640701294\nstep3593, loss: 2.378431797027588\nstep3594, loss: 2.3921420574188232\nstep3595, loss: 2.369354248046875\nstep3596, loss: 2.3017704486846924\nstep3597, loss: 2.7104296684265137\nstep3598, loss: 2.5608811378479004\nstep3599, loss: 2.376276731491089\nstep3600, loss: 2.3044261932373047\nstep3601, loss: 2.3949313163757324\nstep3602, loss: 2.3373754024505615\nstep3603, loss: 2.2321202754974365\nstep3604, loss: 2.2901031970977783\nstep3605, loss: 2.2779037952423096\nstep3606, loss: 2.641868829727173\nstep3607, loss: 2.525089740753174\nstep3608, loss: 2.661749839782715\nstep3609, loss: 2.582520008087158\nstep3610, loss: 2.5306854248046875\nstep3611, loss: 2.5112435817718506\nstep3612, loss: 2.439772844314575\nstep3613, loss: 2.312751293182373\nstep3614, loss: 2.4594051837921143\nstep3615, loss: 2.390324354171753\nstep3616, loss: 2.398916244506836\nstep3617, loss: 2.5348005294799805\nstep3618, loss: 2.7166733741760254\nstep3619, loss: 2.6597275733947754\nstep3620, loss: 2.5801849365234375\nstep3621, loss: 2.612295389175415\nstep3622, loss: 2.4543004035949707\nstep3623, loss: 2.7111282348632812\nstep3624, loss: 2.435328483581543\nstep3625, loss: 2.415753126144409\nstep3626, loss: 2.5837650299072266\nstep3627, loss: 2.466437339782715\nstep3628, loss: 2.534156084060669\nstep3629, loss: 2.3493435382843018\nstep3630, loss: 2.620567798614502\nstep3631, loss: 2.6839382648468018\nstep3632, loss: 2.699150800704956\nstep3633, loss: 2.6901695728302\nstep3634, loss: 2.5651936531066895\nstep3635, loss: 2.681941032409668\nstep3636, loss: 2.634390115737915\nstep3637, loss: 2.5966567993164062\nstep3638, loss: 2.5861318111419678\nstep3639, loss: 2.478585720062256\nstep3640, loss: 2.482454776763916\nstep3641, loss: 2.6196422576904297\nstep3642, loss: 2.6152656078338623\nstep3643, loss: 2.5803725719451904\nstep3644, loss: 2.4246177673339844\nstep3645, loss: 2.5161144733428955\nstep3646, loss: 2.469888210296631\nstep3647, loss: 2.416980743408203\nstep3648, loss: 2.379488945007324\nstep3649, loss: 2.568422555923462\nstep3650, loss: 2.482093095779419\nstep3651, loss: 2.5615334510803223\nstep3652, loss: 2.737165927886963\nstep3653, loss: 2.622992515563965\nstep3654, loss: 2.6519644260406494\nstep3655, loss: 2.419405460357666\nstep3656, loss: 2.626706838607788\nstep3657, loss: 2.507868528366089\nstep3658, loss: 2.5561130046844482\nstep3659, loss: 2.496227741241455\nstep3660, loss: 2.6314802169799805\nstep3661, loss: 2.540177822113037\nstep3662, loss: 2.68454647064209\nstep3663, loss: 2.739326000213623\nstep3664, loss: 2.6898045539855957\nstep3665, loss: 2.6090798377990723\nstep3666, loss: 2.4608986377716064\nstep3667, loss: 2.410313606262207\nstep3668, loss: 2.6049158573150635\nstep3669, loss: 2.5266196727752686\nstep3670, loss: 2.3723204135894775\nstep3671, loss: 2.200714588165283\nstep3672, loss: 2.397509813308716\nstep3673, loss: 2.3816823959350586\nstep3674, loss: 2.284587860107422\nstep3675, loss: 2.251007318496704\nstep3676, loss: 2.274838447570801\nstep3677, loss: 2.293977975845337\nstep3678, loss: 2.2830402851104736\nstep3679, loss: 2.6609413623809814\nstep3680, loss: 2.475231170654297\nstep3681, loss: 2.333526372909546\nstep3682, loss: 2.2465877532958984\nstep3683, loss: 2.2627036571502686\nstep3684, loss: 2.2451961040496826\nstep3685, loss: 2.1849188804626465\nstep3686, loss: 2.216947078704834\nstep3687, loss: 2.2204854488372803\nstep3688, loss: 2.545961856842041\nstep3689, loss: 2.42624831199646\nstep3690, loss: 2.6141018867492676\nstep3691, loss: 2.5065839290618896\nstep3692, loss: 2.46903657913208\nstep3693, loss: 2.4269022941589355\nstep3694, loss: 2.3792550563812256\nstep3695, loss: 2.2445712089538574\nstep3696, loss: 2.3571271896362305\nstep3697, loss: 2.2916293144226074\nstep3698, loss: 2.2937629222869873\nstep3699, loss: 2.418555974960327\nstep3700, loss: 2.586549758911133\nstep3701, loss: 2.4989821910858154\nstep3702, loss: 2.417335271835327\nstep3703, loss: 2.5083236694335938\nstep3704, loss: 2.326677083969116\nstep3705, loss: 2.6245336532592773\nstep3706, loss: 2.3526077270507812\nstep3707, loss: 2.306231737136841\nstep3708, loss: 2.4694745540618896\nstep3709, loss: 2.3290317058563232\nstep3710, loss: 2.4544081687927246\nstep3711, loss: 2.24482798576355\nstep3712, loss: 2.491753578186035\nstep3713, loss: 2.5932538509368896\nstep3714, loss: 2.5428755283355713\nstep3715, loss: 2.5831165313720703\nstep3716, loss: 2.488910436630249\nstep3717, loss: 2.5995562076568604\nstep3718, loss: 2.587333917617798\nstep3719, loss: 2.501054286956787\nstep3720, loss: 2.4657161235809326\nstep3721, loss: 2.422531843185425\nstep3722, loss: 2.412245035171509\nstep3723, loss: 2.5500550270080566\nstep3724, loss: 2.536208391189575\nstep3725, loss: 2.529218912124634\nstep3726, loss: 2.3857245445251465\nstep3727, loss: 2.4652771949768066\nstep3728, loss: 2.4400205612182617\nstep3729, loss: 2.3962504863739014\nstep3730, loss: 2.3134961128234863\nstep3731, loss: 2.4815499782562256\nstep3732, loss: 2.4295215606689453\nstep3733, loss: 2.482354164123535\nstep3734, loss: 2.6525893211364746\nstep3735, loss: 2.5326075553894043\nstep3736, loss: 2.5674009323120117\nstep3737, loss: 2.3330090045928955\nstep3738, loss: 2.540138006210327\nstep3739, loss: 2.437786817550659\nstep3740, loss: 2.4675769805908203\nstep3741, loss: 2.469329595565796\nstep3742, loss: 2.5311737060546875\nstep3743, loss: 2.477612257003784\nstep3744, loss: 2.5690643787384033\nstep3745, loss: 2.61262845993042\nstep3746, loss: 2.562556505203247\nstep3747, loss: 2.527634382247925\nstep3748, loss: 2.419720411300659\nstep3749, loss: 2.360304594039917\nstep3750, loss: 2.587562322616577\nstep3751, loss: 2.533207654953003\nstep3752, loss: 2.3712499141693115\nstep3753, loss: 2.153620958328247\nstep3754, loss: 2.3687806129455566\nstep3755, loss: 2.322038173675537\nstep3756, loss: 2.2557168006896973\nstep3757, loss: 2.2282116413116455\nstep3758, loss: 2.2414238452911377\nstep3759, loss: 2.228996753692627\nstep3760, loss: 2.214709520339966\nstep3761, loss: 2.580270528793335\nstep3762, loss: 2.420253276824951\nstep3763, loss: 2.2799019813537598\nstep3764, loss: 2.2434325218200684\nstep3765, loss: 2.224923610687256\nstep3766, loss: 2.216433048248291\nstep3767, loss: 2.1419761180877686\nstep3768, loss: 2.154921531677246\nstep3769, loss: 2.174238681793213\nstep3770, loss: 2.4916064739227295\nstep3771, loss: 2.3699681758880615\nstep3772, loss: 2.5102646350860596\nstep3773, loss: 2.460564136505127\nstep3774, loss: 2.4230380058288574\nstep3775, loss: 2.36934232711792\nstep3776, loss: 2.3374311923980713\nstep3777, loss: 2.1829166412353516\nstep3778, loss: 2.3009490966796875\nstep3779, loss: 2.2516837120056152\nstep3780, loss: 2.2212393283843994\nstep3781, loss: 2.3472890853881836\nstep3782, loss: 2.489751100540161\nstep3783, loss: 2.4243509769439697\nstep3784, loss: 2.358346939086914\nstep3785, loss: 2.3891522884368896\nstep3786, loss: 2.1938929557800293\nstep3787, loss: 2.56061053276062\nstep3788, loss: 2.2748618125915527\nstep3789, loss: 2.253683090209961\nstep3790, loss: 2.420757532119751\nstep3791, loss: 2.2848198413848877\nstep3792, loss: 2.367623805999756\nstep3793, loss: 2.1389834880828857\nstep3794, loss: 2.3799939155578613\nstep3795, loss: 2.489616632461548\nstep3796, loss: 2.5038163661956787\nstep3797, loss: 2.495755672454834\nstep3798, loss: 2.367609739303589\nstep3799, loss: 2.5000102519989014\nstep3800, loss: 2.512552499771118\nstep3801, loss: 2.487128257751465\nstep3802, loss: 2.3877930641174316\nstep3803, loss: 2.3413803577423096\nstep3804, loss: 2.363656997680664\nstep3805, loss: 2.5161428451538086\nstep3806, loss: 2.4880783557891846\nstep3807, loss: 2.4834530353546143\nstep3808, loss: 2.415536642074585\nstep3809, loss: 2.4859814643859863\nstep3810, loss: 2.4724245071411133\nstep3811, loss: 2.4457764625549316\nstep3812, loss: 2.3393728733062744\nstep3813, loss: 2.535059928894043\nstep3814, loss: 2.4726479053497314\nstep3815, loss: 2.4761056900024414\nstep3816, loss: 2.6850452423095703\nstep3817, loss: 2.4866483211517334\nstep3818, loss: 2.52181339263916\nstep3819, loss: 2.2849369049072266\nstep3820, loss: 2.4529247283935547\nstep3821, loss: 2.3865301609039307\nstep3822, loss: 2.414064407348633\nstep3823, loss: 2.404407024383545\nstep3824, loss: 2.4578969478607178\nstep3825, loss: 2.4315664768218994\nstep3826, loss: 2.5352368354797363\nstep3827, loss: 2.598233222961426\nstep3828, loss: 2.535679578781128\nstep3829, loss: 2.4565517902374268\nstep3830, loss: 2.3690223693847656\nstep3831, loss: 2.256704568862915\nstep3832, loss: 2.48091721534729\nstep3833, loss: 2.403651475906372\nstep3834, loss: 2.2584028244018555\nstep3835, loss: 2.112769603729248\nstep3836, loss: 2.3296868801116943\nstep3837, loss: 2.3227663040161133\nstep3838, loss: 2.2736475467681885\nstep3839, loss: 2.166203260421753\nstep3840, loss: 2.156867265701294\nstep3841, loss: 2.1582345962524414\nstep3842, loss: 2.136915445327759\nstep3843, loss: 2.5056376457214355\nstep3844, loss: 2.4077322483062744\nstep3845, loss: 2.303177833557129\nstep3846, loss: 2.1970198154449463\nstep3847, loss: 2.17962646484375\nstep3848, loss: 2.1580300331115723\nstep3849, loss: 2.0821750164031982\nstep3850, loss: 2.1171774864196777\nstep3851, loss: 2.1292972564697266\nstep3852, loss: 2.4896366596221924\nstep3853, loss: 2.325206995010376\nstep3854, loss: 2.429859161376953\nstep3855, loss: 2.403110980987549\nstep3856, loss: 2.400498867034912\nstep3857, loss: 2.3301854133605957\nstep3858, loss: 2.328029155731201\nstep3859, loss: 2.177220582962036\nstep3860, loss: 2.2977869510650635\nstep3861, loss: 2.227513551712036\nstep3862, loss: 2.214254856109619\nstep3863, loss: 2.3004348278045654\nstep3864, loss: 2.458970069885254\nstep3865, loss: 2.411848783493042\nstep3866, loss: 2.328047037124634\nstep3867, loss: 2.370424509048462\nstep3868, loss: 2.170290470123291\nstep3869, loss: 2.4688780307769775\nstep3870, loss: 2.2057034969329834\nstep3871, loss: 2.2145509719848633\nstep3872, loss: 2.446819305419922\nstep3873, loss: 2.2269842624664307\nstep3874, loss: 2.3517820835113525\nstep3875, loss: 2.1351025104522705\nstep3876, loss: 2.3452117443084717\nstep3877, loss: 2.485180377960205\nstep3878, loss: 2.469884157180786\nstep3879, loss: 2.4939253330230713\nstep3880, loss: 2.3387069702148438\nstep3881, loss: 2.480320930480957\nstep3882, loss: 2.4567410945892334\nstep3883, loss: 2.4132819175720215\nstep3884, loss: 2.3849098682403564\nstep3885, loss: 2.2976901531219482\nstep3886, loss: 2.2690200805664062\nstep3887, loss: 2.3910791873931885\nstep3888, loss: 2.4362857341766357\nstep3889, loss: 2.3894684314727783\nstep3890, loss: 2.3108863830566406\nstep3891, loss: 2.3637542724609375\nstep3892, loss: 2.415301561355591\nstep3893, loss: 2.3991713523864746\nstep3894, loss: 2.316007137298584\nstep3895, loss: 2.5177230834960938\nstep3896, loss: 2.448646068572998\nstep3897, loss: 2.4415202140808105\nstep3898, loss: 2.608116626739502\nstep3899, loss: 2.489642381668091\nstep3900, loss: 2.508131980895996\nstep3901, loss: 2.3344614505767822\nstep3902, loss: 2.5267629623413086\nstep3903, loss: 2.392085075378418\nstep3904, loss: 2.425497531890869\nstep3905, loss: 2.389554262161255\nstep3906, loss: 2.451465606689453\nstep3907, loss: 2.396329402923584\nstep3908, loss: 2.4642555713653564\nstep3909, loss: 2.570237398147583\nstep3910, loss: 2.5207717418670654\nstep3911, loss: 2.374260902404785\nstep3912, loss: 2.3394105434417725\nstep3913, loss: 2.2467079162597656\nstep3914, loss: 2.4642324447631836\nstep3915, loss: 2.3616724014282227\nstep3916, loss: 2.22715163230896\nstep3917, loss: 2.0379722118377686\nstep3918, loss: 2.243229627609253\nstep3919, loss: 2.243260145187378\nstep3920, loss: 2.1511518955230713\nstep3921, loss: 2.0773870944976807\nstep3922, loss: 2.0981814861297607\nstep3923, loss: 2.1126036643981934\nstep3924, loss: 2.0996053218841553\nstep3925, loss: 2.4455478191375732\nstep3926, loss: 2.3280625343322754\nstep3927, loss: 2.19058895111084\nstep3928, loss: 2.1501736640930176\nstep3929, loss: 2.1211609840393066\nstep3930, loss: 2.0938005447387695\nstep3931, loss: 2.014518976211548\nstep3932, loss: 2.072058916091919\nstep3933, loss: 2.1026618480682373\nstep3934, loss: 2.3789899349212646\nstep3935, loss: 2.2600488662719727\nstep3936, loss: 2.4081380367279053\nstep3937, loss: 2.335736036300659\nstep3938, loss: 2.2720425128936768\nstep3939, loss: 2.243581533432007\nstep3940, loss: 2.216841220855713\nstep3941, loss: 2.0862598419189453\nstep3942, loss: 2.2094595432281494\nstep3943, loss: 2.1233932971954346\nstep3944, loss: 2.194514036178589\nstep3945, loss: 2.3061015605926514\nstep3946, loss: 2.4148616790771484\nstep3947, loss: 2.37353253364563\nstep3948, loss: 2.242624044418335\nstep3949, loss: 2.307478189468384\nstep3950, loss: 2.150176763534546\nstep3951, loss: 2.456923007965088\nstep3952, loss: 2.1730072498321533\nstep3953, loss: 2.179055690765381\nstep3954, loss: 2.378018379211426\nstep3955, loss: 2.1793112754821777\nstep3956, loss: 2.309614419937134\nstep3957, loss: 2.1320488452911377\nstep3958, loss: 2.3145365715026855\nstep3959, loss: 2.5105903148651123\nstep3960, loss: 2.4801290035247803\nstep3961, loss: 2.4944849014282227\nstep3962, loss: 2.3315606117248535\nstep3963, loss: 2.461371421813965\nstep3964, loss: 2.4275457859039307\nstep3965, loss: 2.385483980178833\nstep3966, loss: 2.351905345916748\nstep3967, loss: 2.2593326568603516\nstep3968, loss: 2.2233002185821533\nstep3969, loss: 2.351484775543213\nstep3970, loss: 2.3925833702087402\nstep3971, loss: 2.3568546772003174\nstep3972, loss: 2.1896657943725586\nstep3973, loss: 2.294201612472534\nstep3974, loss: 2.310504198074341\nstep3975, loss: 2.2643332481384277\nstep3976, loss: 2.188502550125122\nstep3977, loss: 2.385439157485962\nstep3978, loss: 2.385244131088257\nstep3979, loss: 2.420430898666382\nstep3980, loss: 2.5432798862457275\nstep3981, loss: 2.4501380920410156\nstep3982, loss: 2.4656410217285156\nstep3983, loss: 2.221809148788452\nstep3984, loss: 2.445303440093994\nstep3985, loss: 2.320708751678467\nstep3986, loss: 2.3266398906707764\nstep3987, loss: 2.355902671813965\nstep3988, loss: 2.429079532623291\nstep3989, loss: 2.433077812194824\nstep3990, loss: 2.5219316482543945\nstep3991, loss: 2.595937728881836\nstep3992, loss: 2.4965386390686035\nstep3993, loss: 2.3809139728546143\nstep3994, loss: 2.3294360637664795\nstep3995, loss: 2.2252492904663086\nstep3996, loss: 2.410717487335205\nstep3997, loss: 2.3486225605010986\nstep3998, loss: 2.2226550579071045\nstep3999, loss: 2.0402581691741943\nstep4000, loss: 2.2496562004089355\nstep4001, loss: 2.248786449432373\nstep4002, loss: 2.165086507797241\nstep4003, loss: 2.114837169647217\nstep4004, loss: 2.10089373588562\nstep4005, loss: 2.1220290660858154\nstep4006, loss: 2.049089193344116\nstep4007, loss: 2.393939733505249\nstep4008, loss: 2.2733354568481445\nstep4009, loss: 2.1289618015289307\nstep4010, loss: 2.0295650959014893\nstep4011, loss: 2.0793838500976562\nstep4012, loss: 2.0353920459747314\nstep4013, loss: 1.970866084098816\nstep4014, loss: 1.9929494857788086\nstep4015, loss: 2.0025455951690674\nstep4016, loss: 2.299923896789551\nstep4017, loss: 2.1907033920288086\nstep4018, loss: 2.3151111602783203\nstep4019, loss: 2.221592903137207\nstep4020, loss: 2.177138090133667\nstep4021, loss: 2.1583826541900635\nstep4022, loss: 2.120126485824585\nstep4023, loss: 2.0293729305267334\nstep4024, loss: 2.174511432647705\nstep4025, loss: 2.075377941131592\nstep4026, loss: 2.0789732933044434\nstep4027, loss: 2.1952452659606934\nstep4028, loss: 2.304680109024048\nstep4029, loss: 2.228585720062256\nstep4030, loss: 2.119420289993286\nstep4031, loss: 2.1839656829833984\nstep4032, loss: 2.052155017852783\nstep4033, loss: 2.339303493499756\nstep4034, loss: 2.0460703372955322\nstep4035, loss: 2.088113784790039\nstep4036, loss: 2.262392997741699\nstep4037, loss: 2.0956814289093018\nstep4038, loss: 2.2110822200775146\nstep4039, loss: 2.0451905727386475\nstep4040, loss: 2.1994144916534424\nstep4041, loss: 2.290379762649536\nstep4042, loss: 2.3026037216186523\nstep4043, loss: 2.3230268955230713\nstep4044, loss: 2.2145206928253174\nstep4045, loss: 2.379333972930908\nstep4046, loss: 2.3412106037139893\nstep4047, loss: 2.3440041542053223\nstep4048, loss: 2.3075647354125977\nstep4049, loss: 2.2644262313842773\nstep4050, loss: 2.191122531890869\nstep4051, loss: 2.2882001399993896\nstep4052, loss: 2.2794485092163086\nstep4053, loss: 2.288092851638794\nstep4054, loss: 2.1458041667938232\nstep4055, loss: 2.257841110229492\nstep4056, loss: 2.259946346282959\nstep4057, loss: 2.261234998703003\nstep4058, loss: 2.162896156311035\nstep4059, loss: 2.34678053855896\nstep4060, loss: 2.3315539360046387\nstep4061, loss: 2.307264566421509\nstep4062, loss: 2.4796717166900635\nstep4063, loss: 2.3490755558013916\nstep4064, loss: 2.3882853984832764\nstep4065, loss: 2.1844468116760254\nstep4066, loss: 2.375424385070801\nstep4067, loss: 2.2875900268554688\nstep4068, loss: 2.301264762878418\nstep4069, loss: 2.2944583892822266\nstep4070, loss: 2.3312134742736816\nstep4071, loss: 2.3340182304382324\nstep4072, loss: 2.4628326892852783\nstep4073, loss: 2.510101318359375\nstep4074, loss: 2.429826021194458\nstep4075, loss: 2.3318378925323486\nstep4076, loss: 2.2655889987945557\nstep4077, loss: 2.2264246940612793\nstep4078, loss: 2.4421331882476807\nstep4079, loss: 2.3687126636505127\nstep4080, loss: 2.2304024696350098\nstep4081, loss: 2.0902812480926514\nstep4082, loss: 2.251451253890991\nstep4083, loss: 2.2025973796844482\nstep4084, loss: 2.149047613143921\nstep4085, loss: 2.082301378250122\nstep4086, loss: 2.041797161102295\nstep4087, loss: 2.046802520751953\nstep4088, loss: 2.032933235168457\nstep4089, loss: 2.390916347503662\nstep4090, loss: 2.299683094024658\nstep4091, loss: 2.131632089614868\nstep4092, loss: 2.0355567932128906\nstep4093, loss: 2.0534095764160156\nstep4094, loss: 1.9975382089614868\nstep4095, loss: 1.9139745235443115\nstep4096, loss: 1.9648520946502686\nstep4097, loss: 1.9230517148971558\nstep4098, loss: 2.2468647956848145\nstep4099, loss: 2.1282646656036377\nstep4100, loss: 2.294870138168335\nstep4101, loss: 2.2027547359466553\nstep4102, loss: 2.1390914916992188\nstep4103, loss: 2.1350717544555664\nstep4104, loss: 2.080669641494751\nstep4105, loss: 1.96833074092865\nstep4106, loss: 2.0530598163604736\nstep4107, loss: 1.969106674194336\nstep4108, loss: 1.998457670211792\nstep4109, loss: 2.110243797302246\nstep4110, loss: 2.207491874694824\nstep4111, loss: 2.148550271987915\nstep4112, loss: 2.061412811279297\nstep4113, loss: 2.1147656440734863\nstep4114, loss: 2.011230707168579\nstep4115, loss: 2.268674373626709\nstep4116, loss: 1.998209834098816\nstep4117, loss: 1.9711320400238037\nstep4118, loss: 2.1629433631896973\nstep4119, loss: 1.997540831565857\nstep4120, loss: 2.1463561058044434\nstep4121, loss: 1.9265120029449463\nstep4122, loss: 2.1170413494110107\nstep4123, loss: 2.2381391525268555\nstep4124, loss: 2.261864185333252\nstep4125, loss: 2.2597498893737793\nstep4126, loss: 2.129969835281372\nstep4127, loss: 2.260629415512085\nstep4128, loss: 2.2309935092926025\nstep4129, loss: 2.1805570125579834\nstep4130, loss: 2.202341079711914\nstep4131, loss: 2.1239864826202393\nstep4132, loss: 2.102006196975708\nstep4133, loss: 2.2152318954467773\nstep4134, loss: 2.2496213912963867\nstep4135, loss: 2.243521213531494\nstep4136, loss: 2.1161205768585205\nstep4137, loss: 2.2206528186798096\nstep4138, loss: 2.1373324394226074\nstep4139, loss: 2.129539728164673\nstep4140, loss: 2.036892890930176\nstep4141, loss: 2.2595009803771973\nstep4142, loss: 2.2275550365448\nstep4143, loss: 2.26949405670166\nstep4144, loss: 2.414374351501465\nstep4145, loss: 2.3041462898254395\nstep4146, loss: 2.320695400238037\nstep4147, loss: 2.1395015716552734\nstep4148, loss: 2.2818143367767334\nstep4149, loss: 2.1546342372894287\nstep4150, loss: 2.1950459480285645\nstep4151, loss: 2.1551990509033203\nstep4152, loss: 2.252058982849121\nstep4153, loss: 2.2535760402679443\nstep4154, loss: 2.3277475833892822\nstep4155, loss: 2.444887399673462\nstep4156, loss: 2.3752496242523193\nstep4157, loss: 2.2346675395965576\nstep4158, loss: 2.1809961795806885\nstep4159, loss: 2.1132638454437256\nstep4160, loss: 2.357100486755371\nstep4161, loss: 2.2480201721191406\nstep4162, loss: 2.0948405265808105\nstep4163, loss: 2.0078680515289307\nstep4164, loss: 2.1869096755981445\nstep4165, loss: 2.216038703918457\nstep4166, loss: 2.1680212020874023\nstep4167, loss: 2.072359323501587\nstep4168, loss: 2.046113967895508\nstep4169, loss: 2.0514633655548096\nstep4170, loss: 1.9677568674087524\nstep4171, loss: 2.3802835941314697\nstep4172, loss: 2.2125821113586426\nstep4173, loss: 2.0803537368774414\nstep4174, loss: 1.9971355199813843\nstep4175, loss: 2.002737045288086\nstep4176, loss: 1.9499205350875854\nstep4177, loss: 1.9107377529144287\nstep4178, loss: 1.9390652179718018\nstep4179, loss: 1.9566107988357544\nstep4180, loss: 2.217996835708618\nstep4181, loss: 2.1332037448883057\nstep4182, loss: 2.23244047164917\nstep4183, loss: 2.1489193439483643\nstep4184, loss: 2.0945868492126465\nstep4185, loss: 2.0789175033569336\nstep4186, loss: 2.034324884414673\nstep4187, loss: 1.94645357131958\nstep4188, loss: 2.0489208698272705\nstep4189, loss: 1.976505994796753\nstep4190, loss: 1.981529712677002\nstep4191, loss: 2.0836095809936523\nstep4192, loss: 2.161813974380493\nstep4193, loss: 2.0996243953704834\nstep4194, loss: 2.018681526184082\nstep4195, loss: 2.043611526489258\nstep4196, loss: 1.9512083530426025\nstep4197, loss: 2.1622891426086426\nstep4198, loss: 1.9452457427978516\nstep4199, loss: 1.9487611055374146\nstep4200, loss: 2.1234841346740723\nstep4201, loss: 1.9866405725479126\nstep4202, loss: 2.094010829925537\nstep4203, loss: 1.914339542388916\nstep4204, loss: 2.0591087341308594\nstep4205, loss: 2.154106616973877\nstep4206, loss: 2.1841673851013184\nstep4207, loss: 2.1920204162597656\nstep4208, loss: 2.088080644607544\nstep4209, loss: 2.2117106914520264\nstep4210, loss: 2.197308301925659\nstep4211, loss: 2.107219934463501\nstep4212, loss: 2.093501091003418\nstep4213, loss: 2.0456809997558594\nstep4214, loss: 2.045222043991089\nstep4215, loss: 2.128438949584961\nstep4216, loss: 2.14202618598938\nstep4217, loss: 2.1271793842315674\nstep4218, loss: 2.0232393741607666\nstep4219, loss: 2.1272616386413574\nstep4220, loss: 2.123244285583496\nstep4221, loss: 2.0840771198272705\nstep4222, loss: 1.9730372428894043\nstep4223, loss: 2.163626194000244\nstep4224, loss: 2.1116693019866943\nstep4225, loss: 2.1405506134033203\nstep4226, loss: 2.331040382385254\nstep4227, loss: 2.2022602558135986\nstep4228, loss: 2.2436087131500244\nstep4229, loss: 2.0572006702423096\nstep4230, loss: 2.218451499938965\nstep4231, loss: 2.1426966190338135\nstep4232, loss: 2.1441328525543213\nstep4233, loss: 2.121858835220337\nstep4234, loss: 2.2166318893432617\nstep4235, loss: 2.175882577896118\nstep4236, loss: 2.2598986625671387\nstep4237, loss: 2.3310022354125977\nstep4238, loss: 2.2121219635009766\nstep4239, loss: 2.168776035308838\nstep4240, loss: 2.058269500732422\nstep4241, loss: 2.025148391723633\nstep4242, loss: 2.2596933841705322\nstep4243, loss: 2.1881446838378906\nstep4244, loss: 2.055375814437866\nstep4245, loss: 1.8744326829910278\nstep4246, loss: 2.0877456665039062\nstep4247, loss: 2.095991373062134\nstep4248, loss: 1.9974026679992676\nstep4249, loss: 1.9321706295013428\nstep4250, loss: 1.9406497478485107\nstep4251, loss: 2.0008091926574707\nstep4252, loss: 1.9735642671585083\nstep4253, loss: 2.359734535217285\nstep4254, loss: 2.2665960788726807\nstep4255, loss: 2.1052355766296387\nstep4256, loss: 2.031435966491699\nstep4257, loss: 1.9491050243377686\nstep4258, loss: 1.9032599925994873\nstep4259, loss: 1.8325302600860596\nstep4260, loss: 1.8541995286941528\nstep4261, loss: 1.9326450824737549\nstep4262, loss: 2.2336368560791016\nstep4263, loss: 2.089043378829956\nstep4264, loss: 2.3299429416656494\nstep4265, loss: 2.1830334663391113\nstep4266, loss: 2.100778102874756\nstep4267, loss: 2.062701940536499\nstep4268, loss: 1.9927587509155273\nstep4269, loss: 1.8962091207504272\nstep4270, loss: 2.0294220447540283\nstep4271, loss: 1.9437470436096191\nstep4272, loss: 1.9749798774719238\nstep4273, loss: 2.087106943130493\nstep4274, loss: 2.1708953380584717\nstep4275, loss: 2.163616895675659\nstep4276, loss: 2.0528364181518555\nstep4277, loss: 2.1184639930725098\nstep4278, loss: 1.9269496202468872\nstep4279, loss: 2.203295946121216\nstep4280, loss: 1.8982442617416382\nstep4281, loss: 1.902242660522461\nstep4282, loss: 2.0804636478424072\nstep4283, loss: 1.9313756227493286\nstep4284, loss: 2.047715187072754\nstep4285, loss: 1.8893040418624878\nstep4286, loss: 2.0224626064300537\nstep4287, loss: 2.1373586654663086\nstep4288, loss: 2.1317861080169678\nstep4289, loss: 2.1390106678009033\nstep4290, loss: 2.0772674083709717\nstep4291, loss: 2.152496814727783\nstep4292, loss: 2.1310794353485107\nstep4293, loss: 2.045844793319702\nstep4294, loss: 2.044968843460083\nstep4295, loss: 1.9963061809539795\nstep4296, loss: 2.002716064453125\nstep4297, loss: 2.097930669784546\nstep4298, loss: 2.114935874938965\nstep4299, loss: 2.0721938610076904\nstep4300, loss: 1.9617253541946411\nstep4301, loss: 2.062030076980591\nstep4302, loss: 2.027888774871826\nstep4303, loss: 1.9885749816894531\nstep4304, loss: 1.890247106552124\nstep4305, loss: 2.1276590824127197\nstep4306, loss: 2.0894737243652344\nstep4307, loss: 2.1150403022766113\nstep4308, loss: 2.236488103866577\nstep4309, loss: 2.1661019325256348\nstep4310, loss: 2.1400582790374756\nstep4311, loss: 1.9327259063720703\nstep4312, loss: 2.1133055686950684\nstep4313, loss: 2.028111696243286\nstep4314, loss: 2.069547176361084\nstep4315, loss: 2.0535709857940674\nstep4316, loss: 2.127720355987549\nstep4317, loss: 2.0755159854888916\nstep4318, loss: 2.1968984603881836\nstep4319, loss: 2.2049505710601807\nstep4320, loss: 2.1135125160217285\nstep4321, loss: 2.0694289207458496\nstep4322, loss: 1.9905064105987549\nstep4323, loss: 1.9213396310806274\nstep4324, loss: 2.1614768505096436\nstep4325, loss: 2.0629777908325195\nstep4326, loss: 1.9374550580978394\nstep4327, loss: 1.7679436206817627\nstep4328, loss: 1.9981348514556885\nstep4329, loss: 1.994488000869751\nstep4330, loss: 1.9289010763168335\nstep4331, loss: 1.873352289199829\nstep4332, loss: 1.8600363731384277\nstep4333, loss: 1.852647066116333\nstep4334, loss: 1.8550186157226562\nstep4335, loss: 2.162505626678467\nstep4336, loss: 2.069746732711792\nstep4337, loss: 1.9306449890136719\nstep4338, loss: 1.8697181940078735\nstep4339, loss: 1.926615595817566\nstep4340, loss: 1.894382357597351\nstep4341, loss: 1.8193342685699463\nstep4342, loss: 1.8369150161743164\nstep4343, loss: 1.8405251502990723\nstep4344, loss: 2.1570792198181152\nstep4345, loss: 1.9848945140838623\nstep4346, loss: 2.1982839107513428\nstep4347, loss: 2.0806868076324463\nstep4348, loss: 2.0261342525482178\nstep4349, loss: 1.995246171951294\nstep4350, loss: 2.0072014331817627\nstep4351, loss: 1.8997101783752441\nstep4352, loss: 2.0477514266967773\nstep4353, loss: 2.0165634155273438\nstep4354, loss: 1.9604294300079346\nstep4355, loss: 2.117899179458618\nstep4356, loss: 2.1470959186553955\nstep4357, loss: 2.1190648078918457\nstep4358, loss: 1.9983192682266235\nstep4359, loss: 1.991212010383606\nstep4360, loss: 1.8495173454284668\nstep4361, loss: 2.1264421939849854\nstep4362, loss: 1.866112470626831\nstep4363, loss: 1.8965394496917725\nstep4364, loss: 2.0398099422454834\nstep4365, loss: 1.8938467502593994\nstep4366, loss: 1.989005446434021\nstep4367, loss: 1.8289445638656616\nstep4368, loss: 1.9653785228729248\nstep4369, loss: 2.0645689964294434\nstep4370, loss: 2.0895895957946777\nstep4371, loss: 2.081178903579712\nstep4372, loss: 1.9874273538589478\nstep4373, loss: 2.1132750511169434\nstep4374, loss: 2.0396013259887695\nstep4375, loss: 2.025006055831909\nstep4376, loss: 2.00323748588562\nstep4377, loss: 1.9388741254806519\nstep4378, loss: 1.9288015365600586\nstep4379, loss: 2.018972635269165\nstep4380, loss: 2.02463698387146\nstep4381, loss: 2.012584924697876\nstep4382, loss: 1.894284725189209\nstep4383, loss: 1.98957359790802\nstep4384, loss: 1.9573192596435547\nstep4385, loss: 1.9304002523422241\nstep4386, loss: 1.843386173248291\nstep4387, loss: 2.0378854274749756\nstep4388, loss: 2.008486270904541\nstep4389, loss: 2.037269115447998\nstep4390, loss: 2.1580514907836914\nstep4391, loss: 2.0827293395996094\nstep4392, loss: 2.103055953979492\nstep4393, loss: 1.9130173921585083\nstep4394, loss: 2.0725419521331787\nstep4395, loss: 1.968266248703003\nstep4396, loss: 1.9840307235717773\nstep4397, loss: 1.9615631103515625\nstep4398, loss: 1.9929113388061523\nstep4399, loss: 2.000819206237793\nstep4400, loss: 2.083343982696533\nstep4401, loss: 2.1676764488220215\nstep4402, loss: 2.0988588333129883\nstep4403, loss: 2.012768268585205\nstep4404, loss: 1.9728410243988037\nstep4405, loss: 1.8563276529312134\nstep4406, loss: 2.0841729640960693\nstep4407, loss: 2.0054194927215576\nstep4408, loss: 1.834887146949768\nstep4409, loss: 1.6913306713104248\nstep4410, loss: 1.9235124588012695\nstep4411, loss: 1.9181184768676758\nstep4412, loss: 1.7856111526489258\nstep4413, loss: 1.758793830871582\nstep4414, loss: 1.8076263666152954\nstep4415, loss: 1.804816722869873\nstep4416, loss: 1.7923290729522705\nstep4417, loss: 2.1517245769500732\nstep4418, loss: 2.039792776107788\nstep4419, loss: 1.8548330068588257\nstep4420, loss: 1.7964822053909302\nstep4421, loss: 1.7956490516662598\nstep4422, loss: 1.7461875677108765\nstep4423, loss: 1.7308486700057983\nstep4424, loss: 1.781354546546936\nstep4425, loss: 1.7960875034332275\nstep4426, loss: 2.0831854343414307\nstep4427, loss: 1.9642351865768433\nstep4428, loss: 2.1895594596862793\nstep4429, loss: 2.0434553623199463\nstep4430, loss: 2.044516086578369\nstep4431, loss: 1.964428424835205\nstep4432, loss: 1.9308487176895142\nstep4433, loss: 1.8015272617340088\nstep4434, loss: 1.975600242614746\nstep4435, loss: 1.921851634979248\nstep4436, loss: 1.9126815795898438\nstep4437, loss: 2.083944082260132\nstep4438, loss: 2.159403085708618\nstep4439, loss: 2.120633602142334\nstep4440, loss: 2.1009716987609863\nstep4441, loss: 2.080469846725464\nstep4442, loss: 1.8926336765289307\nstep4443, loss: 2.0753612518310547\nstep4444, loss: 1.8601113557815552\nstep4445, loss: 1.9211207628250122\nstep4446, loss: 2.021841287612915\nstep4447, loss: 1.8732596635818481\nstep4448, loss: 1.9902095794677734\nstep4449, loss: 1.8255174160003662\nstep4450, loss: 1.9265352487564087\nstep4451, loss: 2.0836355686187744\nstep4452, loss: 2.0799224376678467\nstep4453, loss: 2.099543333053589\nstep4454, loss: 1.9462586641311646\nstep4455, loss: 2.0815420150756836\nstep4456, loss: 2.057398796081543\nstep4457, loss: 2.014082193374634\nstep4458, loss: 1.9688233137130737\nstep4459, loss: 1.8981364965438843\nstep4460, loss: 1.8587806224822998\nstep4461, loss: 1.987964153289795\nstep4462, loss: 1.994096040725708\nstep4463, loss: 1.977425217628479\nstep4464, loss: 1.8529771566390991\nstep4465, loss: 1.9546031951904297\nstep4466, loss: 1.938488483428955\nstep4467, loss: 1.931563138961792\nstep4468, loss: 1.8493553400039673\nstep4469, loss: 2.0095252990722656\nstep4470, loss: 2.0270488262176514\nstep4471, loss: 2.01776123046875\nstep4472, loss: 2.1458044052124023\nstep4473, loss: 2.011512517929077\nstep4474, loss: 2.052920341491699\nstep4475, loss: 1.8623483180999756\nstep4476, loss: 2.0513055324554443\nstep4477, loss: 1.9225138425827026\nstep4478, loss: 1.9513424634933472\nstep4479, loss: 1.9454485177993774\nstep4480, loss: 1.991735816001892\nstep4481, loss: 1.9760143756866455\nstep4482, loss: 2.0597314834594727\nstep4483, loss: 2.143322229385376\nstep4484, loss: 2.019049882888794\nstep4485, loss: 1.947250485420227\nstep4486, loss: 1.8908082246780396\nstep4487, loss: 1.847264051437378\nstep4488, loss: 2.0407612323760986\nstep4489, loss: 1.9933887720108032\nstep4490, loss: 1.8022491931915283\nstep4491, loss: 1.662046194076538\nstep4492, loss: 1.8650296926498413\nstep4493, loss: 1.8800781965255737\nstep4494, loss: 1.751376748085022\nstep4495, loss: 1.7136945724487305\nstep4496, loss: 1.709754228591919\nstep4497, loss: 1.7677640914916992\nstep4498, loss: 1.739598035812378\nstep4499, loss: 2.064478874206543\nstep4500, loss: 1.9269722700119019\nstep4501, loss: 1.8045881986618042\nstep4502, loss: 1.7448811531066895\nstep4503, loss: 1.7359946966171265\nstep4504, loss: 1.7165559530258179\nstep4505, loss: 1.6494709253311157\nstep4506, loss: 1.6858675479888916\nstep4507, loss: 1.6822668313980103\nstep4508, loss: 1.9644030332565308\nstep4509, loss: 1.8560154438018799\nstep4510, loss: 2.0072708129882812\nstep4511, loss: 1.9266821146011353\nstep4512, loss: 1.891460657119751\nstep4513, loss: 1.9030184745788574\nstep4514, loss: 1.8629333972930908\nstep4515, loss: 1.7500293254852295\nstep4516, loss: 1.8642728328704834\nstep4517, loss: 1.8003489971160889\nstep4518, loss: 1.7940846681594849\nstep4519, loss: 1.9275734424591064\nstep4520, loss: 2.028275966644287\nstep4521, loss: 2.0594897270202637\nstep4522, loss: 1.9955742359161377\nstep4523, loss: 2.0208518505096436\nstep4524, loss: 1.8597147464752197\nstep4525, loss: 2.178943157196045\nstep4526, loss: 1.842420220375061\nstep4527, loss: 1.848239779472351\nstep4528, loss: 2.0020389556884766\nstep4529, loss: 1.851840615272522\nstep4530, loss: 1.986433506011963\nstep4531, loss: 1.7726987600326538\nstep4532, loss: 1.93374764919281\nstep4533, loss: 2.0443058013916016\nstep4534, loss: 2.045097827911377\nstep4535, loss: 2.040830373764038\nstep4536, loss: 1.9316898584365845\nstep4537, loss: 2.044055700302124\nstep4538, loss: 2.010078191757202\nstep4539, loss: 1.9765496253967285\nstep4540, loss: 1.9468854665756226\nstep4541, loss: 1.9121315479278564\nstep4542, loss: 1.8420703411102295\nstep4543, loss: 1.950587511062622\nstep4544, loss: 1.9579116106033325\nstep4545, loss: 1.9320679903030396\nstep4546, loss: 1.7704371213912964\nstep4547, loss: 1.868242621421814\nstep4548, loss: 1.862943410873413\nstep4549, loss: 1.8613216876983643\nstep4550, loss: 1.7372478246688843\nstep4551, loss: 1.9478996992111206\nstep4552, loss: 1.9213225841522217\nstep4553, loss: 1.9384543895721436\nstep4554, loss: 2.0765058994293213\nstep4555, loss: 1.9936988353729248\nstep4556, loss: 2.0111920833587646\nstep4557, loss: 1.8255410194396973\nstep4558, loss: 1.9787095785140991\nstep4559, loss: 1.8999583721160889\nstep4560, loss: 1.8668131828308105\nstep4561, loss: 1.8696969747543335\nstep4562, loss: 1.8550041913986206\nstep4563, loss: 1.867351770401001\nstep4564, loss: 1.9605250358581543\nstep4565, loss: 2.018264055252075\nstep4566, loss: 1.9505479335784912\nstep4567, loss: 1.8456051349639893\nstep4568, loss: 1.7961962223052979\nstep4569, loss: 1.724223256111145\nstep4570, loss: 1.9308586120605469\nstep4571, loss: 1.889250636100769\nstep4572, loss: 1.7222572565078735\nstep4573, loss: 1.5654486417770386\nstep4574, loss: 1.805586814880371\nstep4575, loss: 1.7888656854629517\nstep4576, loss: 1.688538670539856\nstep4577, loss: 1.6521552801132202\nstep4578, loss: 1.6596007347106934\nstep4579, loss: 1.704535961151123\nstep4580, loss: 1.6394296884536743\nstep4581, loss: 1.9431993961334229\nstep4582, loss: 1.8089097738265991\nstep4583, loss: 1.7115095853805542\nstep4584, loss: 1.6295952796936035\nstep4585, loss: 1.649666666984558\nstep4586, loss: 1.6619254350662231\nstep4587, loss: 1.59801185131073\nstep4588, loss: 1.6073590517044067\nstep4589, loss: 1.607475757598877\nstep4590, loss: 1.8837565183639526\nstep4591, loss: 1.766137719154358\nstep4592, loss: 1.8809905052185059\nstep4593, loss: 1.8329732418060303\nstep4594, loss: 1.762963891029358\nstep4595, loss: 1.7841376066207886\nstep4596, loss: 1.723509669303894\nstep4597, loss: 1.6303802728652954\nstep4598, loss: 1.7884657382965088\nstep4599, loss: 1.7001779079437256\nstep4600, loss: 1.7049745321273804\nstep4601, loss: 1.8412959575653076\nstep4602, loss: 1.8870041370391846\nstep4603, loss: 1.9012649059295654\nstep4604, loss: 1.8685057163238525\nstep4605, loss: 1.8638739585876465\nstep4606, loss: 1.724871277809143\nstep4607, loss: 1.9475586414337158\nstep4608, loss: 1.7048406600952148\nstep4609, loss: 1.767295479774475\nstep4610, loss: 1.882412075996399\nstep4611, loss: 1.767385721206665\nstep4612, loss: 1.863083839416504\nstep4613, loss: 1.707984447479248\nstep4614, loss: 1.85405695438385\nstep4615, loss: 1.9275282621383667\nstep4616, loss: 1.9764066934585571\nstep4617, loss: 1.9655832052230835\nstep4618, loss: 1.8601245880126953\nstep4619, loss: 1.9419103860855103\nstep4620, loss: 1.9358288049697876\nstep4621, loss: 1.8798736333847046\nstep4622, loss: 1.862820029258728\nstep4623, loss: 1.822249174118042\nstep4624, loss: 1.8243881464004517\nstep4625, loss: 1.871965765953064\nstep4626, loss: 1.9022982120513916\nstep4627, loss: 1.9086174964904785\nstep4628, loss: 1.758501648902893\nstep4629, loss: 1.851575255393982\nstep4630, loss: 1.803524374961853\nstep4631, loss: 1.8132902383804321\nstep4632, loss: 1.6636672019958496\nstep4633, loss: 1.8539925813674927\nstep4634, loss: 1.819538950920105\nstep4635, loss: 1.8669451475143433\nstep4636, loss: 1.9798821210861206\nstep4637, loss: 1.8976584672927856\nstep4638, loss: 1.9095425605773926\nstep4639, loss: 1.7628614902496338\nstep4640, loss: 1.9015780687332153\nstep4641, loss: 1.770944595336914\nstep4642, loss: 1.8146904706954956\nstep4643, loss: 1.8111045360565186\nstep4644, loss: 1.7897945642471313\nstep4645, loss: 1.8041294813156128\nstep4646, loss: 1.9099006652832031\nstep4647, loss: 1.9883369207382202\nstep4648, loss: 1.8865331411361694\nstep4649, loss: 1.8077993392944336\nstep4650, loss: 1.7321093082427979\nstep4651, loss: 1.6529653072357178\nstep4652, loss: 1.868654489517212\nstep4653, loss: 1.8182246685028076\nstep4654, loss: 1.6360019445419312\nstep4655, loss: 1.4903701543807983\nstep4656, loss: 1.7344310283660889\nstep4657, loss: 1.7393922805786133\nstep4658, loss: 1.6269629001617432\nstep4659, loss: 1.5759024620056152\nstep4660, loss: 1.5836318731307983\nstep4661, loss: 1.6510391235351562\nstep4662, loss: 1.5747956037521362\nstep4663, loss: 1.8993991613388062\nstep4664, loss: 1.7910703420639038\nstep4665, loss: 1.6714378595352173\nstep4666, loss: 1.6082611083984375\nstep4667, loss: 1.5728403329849243\nstep4668, loss: 1.5572460889816284\nstep4669, loss: 1.4984062910079956\nstep4670, loss: 1.5467479228973389\nstep4671, loss: 1.5952457189559937\nstep4672, loss: 1.8195381164550781\nstep4673, loss: 1.7414560317993164\nstep4674, loss: 1.8849495649337769\nstep4675, loss: 1.7881840467453003\nstep4676, loss: 1.7332067489624023\nstep4677, loss: 1.721582055091858\nstep4678, loss: 1.6597942113876343\nstep4679, loss: 1.5481674671173096\nstep4680, loss: 1.6806432008743286\nstep4681, loss: 1.6375198364257812\nstep4682, loss: 1.6693310737609863\nstep4683, loss: 1.7979165315628052\nstep4684, loss: 1.869235634803772\nstep4685, loss: 1.8417572975158691\nstep4686, loss: 1.7286014556884766\nstep4687, loss: 1.8083741664886475\nstep4688, loss: 1.621084213256836\nstep4689, loss: 1.8722271919250488\nstep4690, loss: 1.6386985778808594\nstep4691, loss: 1.6627804040908813\nstep4692, loss: 1.799980878829956\nstep4693, loss: 1.6496250629425049\nstep4694, loss: 1.7666674852371216\nstep4695, loss: 1.6118850708007812\nstep4696, loss: 1.7278461456298828\nstep4697, loss: 1.7888535261154175\nstep4698, loss: 1.8617430925369263\nstep4699, loss: 1.8938337564468384\nstep4700, loss: 1.7614988088607788\nstep4701, loss: 1.8700257539749146\nstep4702, loss: 1.8981986045837402\nstep4703, loss: 1.8215043544769287\nstep4704, loss: 1.7823615074157715\nstep4705, loss: 1.742336392402649\nstep4706, loss: 1.7538272142410278\nstep4707, loss: 1.826796293258667\nstep4708, loss: 1.8352218866348267\nstep4709, loss: 1.821604609489441\nstep4710, loss: 1.7067970037460327\nstep4711, loss: 1.7868337631225586\nstep4712, loss: 1.7892684936523438\nstep4713, loss: 1.777561902999878\nstep4714, loss: 1.6153688430786133\nstep4715, loss: 1.830054521560669\nstep4716, loss: 1.8464763164520264\nstep4717, loss: 1.823208212852478\nstep4718, loss: 1.9542431831359863\nstep4719, loss: 1.8439505100250244\nstep4720, loss: 1.8395603895187378\nstep4721, loss: 1.6913880109786987\nstep4722, loss: 1.873536467552185\nstep4723, loss: 1.7716604471206665\nstep4724, loss: 1.7820590734481812\nstep4725, loss: 1.7733529806137085\nstep4726, loss: 1.757528305053711\nstep4727, loss: 1.772032380104065\nstep4728, loss: 1.9080849885940552\nstep4729, loss: 1.939115285873413\nstep4730, loss: 1.840661883354187\nstep4731, loss: 1.7413874864578247\nstep4732, loss: 1.6969184875488281\nstep4733, loss: 1.5897889137268066\nstep4734, loss: 1.804171085357666\nstep4735, loss: 1.7445520162582397\nstep4736, loss: 1.6646462678909302\nstep4737, loss: 1.4859944581985474\nstep4738, loss: 1.7170307636260986\nstep4739, loss: 1.7247092723846436\nstep4740, loss: 1.5619410276412964\nstep4741, loss: 1.5321550369262695\nstep4742, loss: 1.549058437347412\nstep4743, loss: 1.5802323818206787\nstep4744, loss: 1.5398668050765991\nstep4745, loss: 1.830451488494873\nstep4746, loss: 1.7127864360809326\nstep4747, loss: 1.5581367015838623\nstep4748, loss: 1.5309250354766846\nstep4749, loss: 1.5339179039001465\nstep4750, loss: 1.5173457860946655\nstep4751, loss: 1.4786345958709717\nstep4752, loss: 1.5196365118026733\nstep4753, loss: 1.5027772188186646\nstep4754, loss: 1.7729368209838867\nstep4755, loss: 1.6610628366470337\nstep4756, loss: 1.7363208532333374\nstep4757, loss: 1.7324416637420654\nstep4758, loss: 1.677414059638977\nstep4759, loss: 1.697184443473816\nstep4760, loss: 1.6613322496414185\nstep4761, loss: 1.555570363998413\nstep4762, loss: 1.6829253435134888\nstep4763, loss: 1.6122037172317505\nstep4764, loss: 1.6027799844741821\nstep4765, loss: 1.6981816291809082\nstep4766, loss: 1.7726329565048218\nstep4767, loss: 1.743377685546875\nstep4768, loss: 1.6612852811813354\nstep4769, loss: 1.7554455995559692\nstep4770, loss: 1.5698601007461548\nstep4771, loss: 1.8569644689559937\nstep4772, loss: 1.587766170501709\nstep4773, loss: 1.6579554080963135\nstep4774, loss: 1.7658169269561768\nstep4775, loss: 1.6038596630096436\nstep4776, loss: 1.7145366668701172\nstep4777, loss: 1.5679945945739746\nstep4778, loss: 1.6492574214935303\nstep4779, loss: 1.7487529516220093\nstep4780, loss: 1.7879031896591187\nstep4781, loss: 1.8224070072174072\nstep4782, loss: 1.7064687013626099\nstep4783, loss: 1.7758487462997437\nstep4784, loss: 1.7531646490097046\nstep4785, loss: 1.7336347103118896\nstep4786, loss: 1.7555216550827026\nstep4787, loss: 1.6946488618850708\nstep4788, loss: 1.6478474140167236\nstep4789, loss: 1.7536582946777344\nstep4790, loss: 1.7583101987838745\nstep4791, loss: 1.782153606414795\nstep4792, loss: 1.6450670957565308\nstep4793, loss: 1.753402829170227\nstep4794, loss: 1.7256639003753662\nstep4795, loss: 1.7067617177963257\nstep4796, loss: 1.5715869665145874\nstep4797, loss: 1.808964490890503\nstep4798, loss: 1.757488489151001\nstep4799, loss: 1.8151966333389282\nstep4800, loss: 1.9383738040924072\nstep4801, loss: 1.8429996967315674\nstep4802, loss: 1.8741047382354736\nstep4803, loss: 1.6835347414016724\nstep4804, loss: 1.8497987985610962\nstep4805, loss: 1.7123926877975464\nstep4806, loss: 1.7009007930755615\nstep4807, loss: 1.7101558446884155\nstep4808, loss: 1.7878779172897339\nstep4809, loss: 1.7772635221481323\nstep4810, loss: 1.8980704545974731\nstep4811, loss: 1.938445806503296\nstep4812, loss: 1.8038398027420044\nstep4813, loss: 1.7250897884368896\nstep4814, loss: 1.695920467376709\nstep4815, loss: 1.5762598514556885\nstep4816, loss: 1.767926573753357\nstep4817, loss: 1.7264273166656494\nstep4818, loss: 1.5884923934936523\nstep4819, loss: 1.4514727592468262\nstep4820, loss: 1.6301305294036865\nstep4821, loss: 1.623685598373413\nstep4822, loss: 1.5407741069793701\nstep4823, loss: 1.545963168144226\nstep4824, loss: 1.4982800483703613\nstep4825, loss: 1.570792317390442\nstep4826, loss: 1.5193995237350464\nstep4827, loss: 1.8123693466186523\nstep4828, loss: 1.678512454032898\nstep4829, loss: 1.5534430742263794\nstep4830, loss: 1.5342495441436768\nstep4831, loss: 1.5144548416137695\nstep4832, loss: 1.4830883741378784\nstep4833, loss: 1.3976945877075195\nstep4834, loss: 1.4622551202774048\nstep4835, loss: 1.4376729726791382\nstep4836, loss: 1.6966530084609985\nstep4837, loss: 1.5809664726257324\nstep4838, loss: 1.7079288959503174\nstep4839, loss: 1.685156226158142\nstep4840, loss: 1.6206154823303223\nstep4841, loss: 1.6540712118148804\nstep4842, loss: 1.5931047201156616\nstep4843, loss: 1.4996669292449951\nstep4844, loss: 1.648228645324707\nstep4845, loss: 1.551632046699524\nstep4846, loss: 1.559676170349121\nstep4847, loss: 1.6908975839614868\nstep4848, loss: 1.7655645608901978\nstep4849, loss: 1.7640283107757568\nstep4850, loss: 1.681869626045227\nstep4851, loss: 1.7372386455535889\nstep4852, loss: 1.5279103517532349\nstep4853, loss: 1.8143939971923828\nstep4854, loss: 1.537200689315796\nstep4855, loss: 1.5867232084274292\nstep4856, loss: 1.6947346925735474\nstep4857, loss: 1.5874602794647217\nstep4858, loss: 1.6721924543380737\nstep4859, loss: 1.5282511711120605\nstep4860, loss: 1.6368296146392822\nstep4861, loss: 1.7171205282211304\nstep4862, loss: 1.748326063156128\nstep4863, loss: 1.7499053478240967\nstep4864, loss: 1.6345808506011963\nstep4865, loss: 1.7223073244094849\nstep4866, loss: 1.7238863706588745\nstep4867, loss: 1.670508623123169\nstep4868, loss: 1.6716220378875732\nstep4869, loss: 1.6268223524093628\nstep4870, loss: 1.5775599479675293\nstep4871, loss: 1.6577621698379517\nstep4872, loss: 1.6583296060562134\nstep4873, loss: 1.7094743251800537\nstep4874, loss: 1.5470476150512695\nstep4875, loss: 1.668925166130066\nstep4876, loss: 1.6584055423736572\nstep4877, loss: 1.657575011253357\nstep4878, loss: 1.5356370210647583\nstep4879, loss: 1.738707423210144\nstep4880, loss: 1.753123164176941\nstep4881, loss: 1.757243275642395\nstep4882, loss: 1.8367505073547363\nstep4883, loss: 1.7842466831207275\nstep4884, loss: 1.7906005382537842\nstep4885, loss: 1.6325603723526\nstep4886, loss: 1.7952970266342163\nstep4887, loss: 1.6710689067840576\nstep4888, loss: 1.698817253112793\nstep4889, loss: 1.6451202630996704\nstep4890, loss: 1.7688300609588623\nstep4891, loss: 1.7185523509979248\nstep4892, loss: 1.7848566770553589\nstep4893, loss: 1.866149663925171\nstep4894, loss: 1.7708873748779297\nstep4895, loss: 1.7449380159378052\nstep4896, loss: 1.7003893852233887\nstep4897, loss: 1.6268490552902222\nstep4898, loss: 1.754970669746399\nstep4899, loss: 1.7035322189331055\nstep4900, loss: 1.559863567352295\nstep4901, loss: 1.4245448112487793\nstep4902, loss: 1.6026921272277832\nstep4903, loss: 1.5945813655853271\nstep4904, loss: 1.4591624736785889\nstep4905, loss: 1.468134880065918\nstep4906, loss: 1.4244670867919922\nstep4907, loss: 1.5036360025405884\nstep4908, loss: 1.4530390501022339\nstep4909, loss: 1.757284164428711\nstep4910, loss: 1.6484200954437256\nstep4911, loss: 1.5594849586486816\nstep4912, loss: 1.4829542636871338\nstep4913, loss: 1.4840275049209595\nstep4914, loss: 1.4415428638458252\nstep4915, loss: 1.4126719236373901\nstep4916, loss: 1.4757888317108154\nstep4917, loss: 1.4289491176605225\nstep4918, loss: 1.6659202575683594\nstep4919, loss: 1.5323967933654785\nstep4920, loss: 1.6548182964324951\nstep4921, loss: 1.609974980354309\nstep4922, loss: 1.5587722063064575\nstep4923, loss: 1.5653787851333618\nstep4924, loss: 1.5033936500549316\nstep4925, loss: 1.4247167110443115\nstep4926, loss: 1.5823702812194824\nstep4927, loss: 1.5149433612823486\nstep4928, loss: 1.486811637878418\nstep4929, loss: 1.5949745178222656\nstep4930, loss: 1.6773872375488281\nstep4931, loss: 1.6640868186950684\nstep4932, loss: 1.6095722913742065\nstep4933, loss: 1.6452871561050415\nstep4934, loss: 1.4767820835113525\nstep4935, loss: 1.748220682144165\nstep4936, loss: 1.522249698638916\nstep4937, loss: 1.5408340692520142\nstep4938, loss: 1.6047359704971313\nstep4939, loss: 1.5482479333877563\nstep4940, loss: 1.5959417819976807\nstep4941, loss: 1.4587862491607666\nstep4942, loss: 1.5987579822540283\nstep4943, loss: 1.648220419883728\nstep4944, loss: 1.6707568168640137\nstep4945, loss: 1.693424105644226\nstep4946, loss: 1.580678105354309\nstep4947, loss: 1.6746658086776733\nstep4948, loss: 1.6439272165298462\nstep4949, loss: 1.6064563989639282\nstep4950, loss: 1.5940701961517334\nstep4951, loss: 1.5691025257110596\nstep4952, loss: 1.5215834379196167\nstep4953, loss: 1.6026537418365479\nstep4954, loss: 1.58158540725708\nstep4955, loss: 1.5890766382217407\nstep4956, loss: 1.4966766834259033\nstep4957, loss: 1.579464316368103\nstep4958, loss: 1.576737880706787\nstep4959, loss: 1.568979024887085\nstep4960, loss: 1.4063621759414673\nstep4961, loss: 1.6365280151367188\nstep4962, loss: 1.7080647945404053\nstep4963, loss: 1.7396048307418823\nstep4964, loss: 1.8017266988754272\nstep4965, loss: 1.6990853548049927\nstep4966, loss: 1.7522852420806885\nstep4967, loss: 1.5480531454086304\nstep4968, loss: 1.7176369428634644\nstep4969, loss: 1.6081353425979614\nstep4970, loss: 1.615708827972412\nstep4971, loss: 1.6548058986663818\nstep4972, loss: 1.6829924583435059\nstep4973, loss: 1.70115327835083\nstep4974, loss: 1.8077497482299805\nstep4975, loss: 1.8028846979141235\nstep4976, loss: 1.7654144763946533\nstep4977, loss: 1.6672981977462769\nstep4978, loss: 1.5736268758773804\nstep4979, loss: 1.54514479637146\nstep4980, loss: 1.7246941328048706\nstep4981, loss: 1.6896177530288696\nstep4982, loss: 1.62724769115448\nstep4983, loss: 1.4758918285369873\nstep4984, loss: 1.6757186651229858\nstep4985, loss: 1.6468716859817505\nstep4986, loss: 1.459509253501892\nstep4987, loss: 1.459754467010498\nstep4988, loss: 1.3736355304718018\nstep4989, loss: 1.4478949308395386\nstep4990, loss: 1.4292808771133423\nstep4991, loss: 1.6807752847671509\nstep4992, loss: 1.578702688217163\nstep4992, loss: 1.578702688217163\nstep4993, loss: 1.4643861055374146\nstep4994, loss: 1.4536213874816895\nstep4995, loss: 1.4137442111968994\nstep4996, loss: 1.4182379245758057\nstep4997, loss: 1.4009811878204346\nstep4998, loss: 1.442057490348816\nstep4999, loss: 1.395827293395996\nstep5000, loss: 1.6290314197540283\nstep5001, loss: 1.5072021484375\nstep5002, loss: 1.6780459880828857\nstep5003, loss: 1.6211144924163818\nstep5004, loss: 1.5310423374176025\nstep5005, loss: 1.5253232717514038\nstep5006, loss: 1.4615072011947632\nstep5007, loss: 1.3803212642669678\nstep5008, loss: 1.5036377906799316\nstep5009, loss: 1.409485101699829\nstep5010, loss: 1.456437110900879\nstep5011, loss: 1.6050152778625488\nstep5012, loss: 1.6432478427886963\nstep5013, loss: 1.6205028295516968\nstep5014, loss: 1.5508278608322144\nstep5015, loss: 1.5873754024505615\nstep5016, loss: 1.4284553527832031\nstep5017, loss: 1.6717443466186523\nstep5018, loss: 1.4725019931793213\nstep5019, loss: 1.5136288404464722\nstep5020, loss: 1.6188067197799683\nstep5021, loss: 1.5477118492126465\nstep5022, loss: 1.5777950286865234\nstep5023, loss: 1.45026433467865\nstep5024, loss: 1.5436856746673584\nstep5025, loss: 1.6156734228134155\nstep5026, loss: 1.6510677337646484\nstep5027, loss: 1.651362657546997\nstep5028, loss: 1.5561208724975586\nstep5029, loss: 1.6667513847351074\nstep5030, loss: 1.6137248277664185\nstep5031, loss: 1.5631213188171387\nstep5032, loss: 1.5705819129943848\nstep5033, loss: 1.5520488023757935\nstep5034, loss: 1.4580365419387817\nstep5035, loss: 1.5861486196517944\nstep5036, loss: 1.5521390438079834\nstep5037, loss: 1.5752516984939575\nstep5038, loss: 1.4915480613708496\nstep5039, loss: 1.5360465049743652\nstep5040, loss: 1.526753306388855\nstep5041, loss: 1.5095707178115845\nstep5042, loss: 1.3773341178894043\nstep5043, loss: 1.5425212383270264\nstep5044, loss: 1.5675145387649536\nstep5045, loss: 1.6075838804244995\nstep5046, loss: 1.691220998764038\nstep5047, loss: 1.620538353919983\nstep5048, loss: 1.6628367900848389\nstep5049, loss: 1.4988476037979126\nstep5050, loss: 1.707106351852417\nstep5051, loss: 1.5294921398162842\nstep5052, loss: 1.5146417617797852\nstep5053, loss: 1.5506950616836548\nstep5054, loss: 1.5733211040496826\nstep5055, loss: 1.5451934337615967\nstep5056, loss: 1.660611629486084\nstep5057, loss: 1.6857291460037231\nstep5058, loss: 1.6654809713363647\nstep5059, loss: 1.5960572957992554\nstep5060, loss: 1.5620568990707397\nstep5061, loss: 1.4956852197647095\nstep5062, loss: 1.7351529598236084\nstep5063, loss: 1.6484183073043823\nstep5064, loss: 1.5505529642105103\nstep5065, loss: 1.3892548084259033\nstep5066, loss: 1.6202455759048462\nstep5067, loss: 1.5440592765808105\nstep5068, loss: 1.4383349418640137\nstep5069, loss: 1.455533504486084\nstep5070, loss: 1.4131879806518555\nstep5071, loss: 1.4624605178833008\nstep5072, loss: 1.40674889087677\nstep5073, loss: 1.6768052577972412\nstep5074, loss: 1.5365239381790161\nstep5075, loss: 1.4358400106430054\nstep5076, loss: 1.3928508758544922\nstep5077, loss: 1.3971271514892578\nstep5078, loss: 1.3230429887771606\nstep5079, loss: 1.3390132188796997\nstep5080, loss: 1.3811849355697632\nstep5081, loss: 1.3878631591796875\nstep5082, loss: 1.6076743602752686\nstep5083, loss: 1.48040771484375\nstep5084, loss: 1.6303056478500366\nstep5085, loss: 1.5594593286514282\nstep5086, loss: 1.5092750787734985\nstep5087, loss: 1.49028480052948\nstep5088, loss: 1.4561080932617188\nstep5089, loss: 1.3337711095809937\nstep5090, loss: 1.4510788917541504\nstep5091, loss: 1.3833500146865845\nstep5092, loss: 1.4055304527282715\nstep5093, loss: 1.5364582538604736\nstep5094, loss: 1.587084412574768\nstep5095, loss: 1.5285097360610962\nstep5096, loss: 1.5156660079956055\nstep5097, loss: 1.5537254810333252\nstep5098, loss: 1.3849622011184692\nstep5099, loss: 1.6418671607971191\nstep5100, loss: 1.4595905542373657\nstep5101, loss: 1.4602173566818237\nstep5102, loss: 1.554366946220398\nstep5103, loss: 1.4501370191574097\nstep5104, loss: 1.4999088048934937\nstep5105, loss: 1.3830639123916626\nstep5106, loss: 1.4742445945739746\nstep5107, loss: 1.5746936798095703\nstep5108, loss: 1.5913969278335571\nstep5109, loss: 1.596655249595642\nstep5110, loss: 1.5168604850769043\nstep5111, loss: 1.5972365140914917\nstep5112, loss: 1.58484947681427\nstep5113, loss: 1.5553126335144043\nstep5114, loss: 1.5580072402954102\nstep5115, loss: 1.5085595846176147\nstep5116, loss: 1.4845868349075317\nstep5117, loss: 1.4974380731582642\nstep5118, loss: 1.4794824123382568\nstep5119, loss: 1.4980279207229614\nstep5120, loss: 1.416257619857788\nstep5121, loss: 1.512434482574463\nstep5122, loss: 1.5038847923278809\nstep5123, loss: 1.534299373626709\nstep5124, loss: 1.3836567401885986\nstep5125, loss: 1.5490750074386597\nstep5126, loss: 1.5696403980255127\nstep5127, loss: 1.5719671249389648\nstep5128, loss: 1.6078732013702393\nstep5129, loss: 1.5467591285705566\nstep5130, loss: 1.571505069732666\nstep5131, loss: 1.4322571754455566\nstep5132, loss: 1.600190281867981\nstep5133, loss: 1.464534044265747\nstep5134, loss: 1.4477589130401611\nstep5135, loss: 1.5107721090316772\nstep5136, loss: 1.5214353799819946\nstep5137, loss: 1.5260050296783447\nstep5138, loss: 1.6374666690826416\nstep5139, loss: 1.670841097831726\nstep5140, loss: 1.5640231370925903\nstep5141, loss: 1.5182889699935913\nstep5142, loss: 1.4183413982391357\nstep5143, loss: 1.3925211429595947\nstep5144, loss: 1.5395715236663818\nstep5145, loss: 1.5221772193908691\nstep5146, loss: 1.471337080001831\nstep5147, loss: 1.3319791555404663\nstep5148, loss: 1.566473364830017\nstep5149, loss: 1.5249817371368408\nstep5150, loss: 1.4364452362060547\nstep5151, loss: 1.3942396640777588\nstep5152, loss: 1.317516803741455\nstep5153, loss: 1.3824340105056763\nstep5154, loss: 1.3952677249908447\nstep5155, loss: 1.6322638988494873\nstep5156, loss: 1.550987720489502\nstep5157, loss: 1.487512230873108\nstep5158, loss: 1.4062397480010986\nstep5159, loss: 1.3770172595977783\nstep5160, loss: 1.3167705535888672\nstep5161, loss: 1.2709734439849854\nstep5162, loss: 1.3114768266677856\nstep5163, loss: 1.277633786201477\nstep5164, loss: 1.5278005599975586\nstep5165, loss: 1.4511970281600952\nstep5166, loss: 1.5867869853973389\nstep5167, loss: 1.5557982921600342\nstep5168, loss: 1.4871861934661865\nstep5169, loss: 1.5365068912506104\nstep5170, loss: 1.4423775672912598\nstep5171, loss: 1.3499284982681274\nstep5172, loss: 1.4414336681365967\nstep5173, loss: 1.3554383516311646\nstep5174, loss: 1.358083963394165\nstep5175, loss: 1.4655342102050781\nstep5176, loss: 1.508866548538208\nstep5177, loss: 1.4763140678405762\nstep5178, loss: 1.4273402690887451\nstep5179, loss: 1.5110944509506226\nstep5180, loss: 1.3427510261535645\nstep5181, loss: 1.5818026065826416\nstep5182, loss: 1.355912208557129\nstep5183, loss: 1.40024995803833\nstep5184, loss: 1.4913661479949951\nstep5185, loss: 1.3796387910842896\nstep5186, loss: 1.4362472295761108\nstep5187, loss: 1.342849850654602\nstep5188, loss: 1.4223055839538574\nstep5189, loss: 1.5618277788162231\nstep5190, loss: 1.5877816677093506\nstep5191, loss: 1.527978539466858\nstep5192, loss: 1.4593335390090942\nstep5193, loss: 1.549342155456543\nstep5194, loss: 1.5243313312530518\nstep5195, loss: 1.5211495161056519\nstep5196, loss: 1.4743013381958008\nstep5197, loss: 1.4686745405197144\nstep5198, loss: 1.4606943130493164\nstep5199, loss: 1.5393422842025757\nstep5200, loss: 1.4916160106658936\nstep5201, loss: 1.5204567909240723\nstep5202, loss: 1.4020262956619263\nstep5203, loss: 1.5354853868484497\nstep5204, loss: 1.450701355934143\nstep5205, loss: 1.442710280418396\nstep5206, loss: 1.3488738536834717\nstep5207, loss: 1.4995858669281006\nstep5208, loss: 1.5328762531280518\nstep5209, loss: 1.5452617406845093\nstep5210, loss: 1.6143953800201416\nstep5211, loss: 1.5447345972061157\nstep5212, loss: 1.6054911613464355\nstep5213, loss: 1.42432701587677\nstep5214, loss: 1.5673103332519531\nstep5215, loss: 1.4760459661483765\nstep5216, loss: 1.4261884689331055\nstep5217, loss: 1.4235655069351196\nstep5218, loss: 1.4348136186599731\nstep5219, loss: 1.4362783432006836\nstep5220, loss: 1.54632568359375\nstep5221, loss: 1.5964922904968262\nstep5222, loss: 1.5434707403182983\nstep5223, loss: 1.4966503381729126\nstep5224, loss: 1.4348167181015015\nstep5225, loss: 1.3406411409378052\nstep5226, loss: 1.5040719509124756\nstep5227, loss: 1.49014413356781\nstep5228, loss: 1.3587652444839478\nstep5229, loss: 1.2454566955566406\nstep5230, loss: 1.4378217458724976\nstep5231, loss: 1.4058622121810913\nstep5232, loss: 1.3103266954421997\nstep5233, loss: 1.3006030321121216\nstep5234, loss: 1.2870910167694092\nstep5235, loss: 1.3371659517288208\nstep5236, loss: 1.3330186605453491\nstep5237, loss: 1.5368359088897705\nstep5238, loss: 1.4566683769226074\nstep5239, loss: 1.3784230947494507\nstep5240, loss: 1.274287462234497\nstep5241, loss: 1.2483348846435547\nstep5242, loss: 1.228887677192688\nstep5243, loss: 1.2657557725906372\nstep5244, loss: 1.2990552186965942\nstep5245, loss: 1.303156852722168\nstep5246, loss: 1.5202971696853638\nstep5247, loss: 1.3949733972549438\nstep5248, loss: 1.550349473953247\nstep5249, loss: 1.4288265705108643\nstep5250, loss: 1.3924864530563354\nstep5251, loss: 1.409263253211975\nstep5252, loss: 1.3636929988861084\nstep5253, loss: 1.2918721437454224\nstep5254, loss: 1.3991488218307495\nstep5255, loss: 1.3108265399932861\nstep5256, loss: 1.3633065223693848\nstep5257, loss: 1.4615750312805176\nstep5258, loss: 1.4770652055740356\nstep5259, loss: 1.4609458446502686\nstep5260, loss: 1.4146943092346191\nstep5261, loss: 1.4353324174880981\nstep5262, loss: 1.3035639524459839\nstep5263, loss: 1.5113333463668823\nstep5264, loss: 1.3118846416473389\nstep5265, loss: 1.379335641860962\nstep5266, loss: 1.4613525867462158\nstep5267, loss: 1.3460705280303955\nstep5268, loss: 1.3688373565673828\nstep5269, loss: 1.2890373468399048\nstep5270, loss: 1.3734161853790283\nstep5271, loss: 1.5010309219360352\nstep5272, loss: 1.5348652601242065\nstep5273, loss: 1.5074031352996826\nstep5274, loss: 1.4036957025527954\nstep5275, loss: 1.4724708795547485\nstep5276, loss: 1.4601994752883911\nstep5277, loss: 1.4574711322784424\nstep5278, loss: 1.412304401397705\nstep5279, loss: 1.3924376964569092\nstep5280, loss: 1.377967119216919\nstep5281, loss: 1.4782122373580933\nstep5282, loss: 1.426072597503662\nstep5283, loss: 1.5045092105865479\nstep5284, loss: 1.3626823425292969\nstep5285, loss: 1.4456793069839478\nstep5286, loss: 1.441293716430664\nstep5287, loss: 1.4070730209350586\nstep5288, loss: 1.3329527378082275\nstep5289, loss: 1.460581660270691\nstep5290, loss: 1.4551798105239868\nstep5291, loss: 1.490935206413269\nstep5292, loss: 1.537553310394287\nstep5293, loss: 1.4597795009613037\nstep5294, loss: 1.5170491933822632\nstep5295, loss: 1.39673912525177\nstep5296, loss: 1.5288259983062744\nstep5297, loss: 1.4352396726608276\nstep5298, loss: 1.4283168315887451\nstep5299, loss: 1.415134072303772\nstep5300, loss: 1.4121720790863037\nstep5301, loss: 1.4018818140029907\nstep5302, loss: 1.470529556274414\nstep5303, loss: 1.5196208953857422\nstep5304, loss: 1.4568506479263306\nstep5305, loss: 1.3790706396102905\nstep5306, loss: 1.368895411491394\nstep5307, loss: 1.2915655374526978\nstep5308, loss: 1.4622230529785156\nstep5309, loss: 1.4224979877471924\nstep5310, loss: 1.3189321756362915\nstep5311, loss: 1.2140541076660156\nstep5312, loss: 1.3953899145126343\nstep5313, loss: 1.3319029808044434\nstep5314, loss: 1.2811774015426636\nstep5315, loss: 1.2406638860702515\nstep5316, loss: 1.1960259675979614\nstep5317, loss: 1.2346606254577637\nstep5318, loss: 1.2459924221038818\nstep5319, loss: 1.467119812965393\nstep5320, loss: 1.4207763671875\nstep5321, loss: 1.355776071548462\nstep5322, loss: 1.2663862705230713\nstep5323, loss: 1.2331023216247559\nstep5324, loss: 1.2229458093643188\nstep5325, loss: 1.1762950420379639\nstep5326, loss: 1.2153973579406738\nstep5327, loss: 1.200084924697876\nstep5328, loss: 1.4334661960601807\nstep5329, loss: 1.3595247268676758\nstep5330, loss: 1.4565798044204712\nstep5331, loss: 1.4351612329483032\nstep5332, loss: 1.3471063375473022\nstep5333, loss: 1.3794206380844116\nstep5334, loss: 1.3227132558822632\nstep5335, loss: 1.2004337310791016\nstep5336, loss: 1.3240772485733032\nstep5337, loss: 1.2718175649642944\nstep5338, loss: 1.287562608718872\nstep5339, loss: 1.413533329963684\nstep5340, loss: 1.426923394203186\nstep5341, loss: 1.4043854475021362\nstep5342, loss: 1.3861384391784668\nstep5343, loss: 1.4028524160385132\nstep5344, loss: 1.2990427017211914\nstep5345, loss: 1.4628527164459229\nstep5346, loss: 1.2777289152145386\nstep5347, loss: 1.304693341255188\nstep5348, loss: 1.380258321762085\nstep5349, loss: 1.2921212911605835\nstep5350, loss: 1.3326148986816406\nstep5351, loss: 1.2323611974716187\nstep5352, loss: 1.3179094791412354\nstep5353, loss: 1.4114934206008911\nstep5354, loss: 1.3907179832458496\nstep5355, loss: 1.4114696979522705\nstep5356, loss: 1.3087661266326904\nstep5357, loss: 1.4407340288162231\nstep5358, loss: 1.3749167919158936\nstep5359, loss: 1.3467273712158203\nstep5360, loss: 1.3252142667770386\nstep5361, loss: 1.30158531665802\nstep5362, loss: 1.2804343700408936\nstep5363, loss: 1.3363763093948364\nstep5364, loss: 1.337359070777893\nstep5365, loss: 1.3808902502059937\nstep5366, loss: 1.3028857707977295\nstep5367, loss: 1.3898522853851318\nstep5368, loss: 1.34678316116333\nstep5369, loss: 1.3407429456710815\nstep5370, loss: 1.2715129852294922\nstep5371, loss: 1.3721628189086914\nstep5372, loss: 1.3998520374298096\nstep5373, loss: 1.4175103902816772\nstep5374, loss: 1.4831798076629639\nstep5375, loss: 1.3723424673080444\nstep5376, loss: 1.3990057706832886\nstep5377, loss: 1.2972992658615112\nstep5378, loss: 1.4506696462631226\nstep5379, loss: 1.335932731628418\nstep5380, loss: 1.344016671180725\nstep5381, loss: 1.3354756832122803\nstep5382, loss: 1.353962779045105\nstep5383, loss: 1.376073956489563\nstep5384, loss: 1.4940053224563599\nstep5385, loss: 1.5057480335235596\nstep5386, loss: 1.4001342058181763\nstep5387, loss: 1.3220241069793701\nstep5388, loss: 1.294594407081604\nstep5389, loss: 1.221793532371521\nstep5390, loss: 1.3960133790969849\nstep5391, loss: 1.3670536279678345\nstep5392, loss: 1.257278323173523\nstep5393, loss: 1.1677682399749756\nstep5394, loss: 1.3903727531433105\nstep5395, loss: 1.2864670753479004\nstep5396, loss: 1.2249599695205688\nstep5397, loss: 1.213786005973816\nstep5398, loss: 1.1328495740890503\nstep5399, loss: 1.1914290189743042\nstep5400, loss: 1.2027539014816284\nstep5401, loss: 1.3629775047302246\nstep5402, loss: 1.3229469060897827\nstep5403, loss: 1.2345620393753052\nstep5404, loss: 1.1429600715637207\nstep5405, loss: 1.1944659948349\nstep5406, loss: 1.1641945838928223\nstep5407, loss: 1.1701276302337646\nstep5408, loss: 1.1812288761138916\nstep5409, loss: 1.138091802597046\nstep5410, loss: 1.365027666091919\nstep5411, loss: 1.2555640935897827\nstep5412, loss: 1.3431545495986938\nstep5413, loss: 1.322662115097046\nstep5414, loss: 1.2764939069747925\nstep5415, loss: 1.2833442687988281\nstep5416, loss: 1.2515255212783813\nstep5417, loss: 1.1604424715042114\nstep5418, loss: 1.2641836404800415\nstep5419, loss: 1.236877202987671\nstep5420, loss: 1.219743013381958\nstep5421, loss: 1.3366713523864746\nstep5422, loss: 1.3335721492767334\nstep5423, loss: 1.3335942029953003\nstep5424, loss: 1.3130204677581787\nstep5425, loss: 1.3567962646484375\nstep5426, loss: 1.2294002771377563\nstep5427, loss: 1.4284586906433105\nstep5428, loss: 1.2178070545196533\nstep5429, loss: 1.189652442932129\nstep5430, loss: 1.2894536256790161\nstep5431, loss: 1.2044265270233154\nstep5432, loss: 1.293400526046753\nstep5433, loss: 1.1732150316238403\nstep5434, loss: 1.2372897863388062\nstep5435, loss: 1.3007798194885254\nstep5436, loss: 1.312002420425415\nstep5437, loss: 1.326372504234314\nstep5438, loss: 1.2528780698776245\nstep5439, loss: 1.354346513748169\nstep5440, loss: 1.2934162616729736\nstep5441, loss: 1.3042917251586914\nstep5442, loss: 1.2573413848876953\nstep5443, loss: 1.254676103591919\nstep5444, loss: 1.2053488492965698\nstep5445, loss: 1.2982549667358398\nstep5446, loss: 1.232591152191162\nstep5447, loss: 1.2995144128799438\nstep5448, loss: 1.2232967615127563\nstep5449, loss: 1.2825182676315308\nstep5450, loss: 1.2472096681594849\nstep5451, loss: 1.3124516010284424\nstep5452, loss: 1.204702615737915\nstep5453, loss: 1.3507146835327148\nstep5454, loss: 1.3852858543395996\nstep5455, loss: 1.392484188079834\nstep5456, loss: 1.4285398721694946\nstep5457, loss: 1.3597145080566406\nstep5458, loss: 1.3800667524337769\nstep5459, loss: 1.2421774864196777\nstep5460, loss: 1.3658264875411987\nstep5461, loss: 1.2949949502944946\nstep5462, loss: 1.2550678253173828\nstep5463, loss: 1.3063174486160278\nstep5464, loss: 1.3062382936477661\nstep5465, loss: 1.3016225099563599\nstep5466, loss: 1.3875412940979004\nstep5467, loss: 1.4179432392120361\nstep5468, loss: 1.3437269926071167\nstep5469, loss: 1.3105827569961548\nstep5470, loss: 1.2377216815948486\nstep5471, loss: 1.170649528503418\nstep5472, loss: 1.315508246421814\nstep5473, loss: 1.2865768671035767\nstep5474, loss: 1.1924903392791748\nstep5475, loss: 1.075766921043396\nstep5476, loss: 1.273024320602417\nstep5477, loss: 1.2365829944610596\nstep5478, loss: 1.126302719116211\nstep5479, loss: 1.172180414199829\nstep5480, loss: 1.0779682397842407\nstep5481, loss: 1.1339516639709473\nstep5482, loss: 1.1655932664871216\nstep5483, loss: 1.3388303518295288\nstep5484, loss: 1.2852964401245117\nstep5485, loss: 1.19316565990448\nstep5486, loss: 1.1156383752822876\nstep5487, loss: 1.1113251447677612\nstep5488, loss: 1.0443737506866455\nstep5489, loss: 1.0626344680786133\nstep5490, loss: 1.0643516778945923\nstep5491, loss: 1.0814967155456543\nstep5492, loss: 1.2995556592941284\nstep5493, loss: 1.2229565382003784\nstep5494, loss: 1.3084657192230225\nstep5495, loss: 1.2579848766326904\nstep5496, loss: 1.2477353811264038\nstep5497, loss: 1.2431408166885376\nstep5498, loss: 1.1730579137802124\nstep5499, loss: 1.096247673034668\nstep5500, loss: 1.1846094131469727\nstep5501, loss: 1.132400631904602\nstep5502, loss: 1.114783525466919\nstep5503, loss: 1.229019284248352\nstep5504, loss: 1.2471349239349365\nstep5505, loss: 1.276616096496582\nstep5506, loss: 1.2230759859085083\nstep5507, loss: 1.2786191701889038\nstep5508, loss: 1.1327617168426514\nstep5509, loss: 1.307819128036499\nstep5510, loss: 1.157952904701233\nstep5511, loss: 1.1425232887268066\nstep5512, loss: 1.2211319208145142\nstep5513, loss: 1.158006191253662\nstep5514, loss: 1.2149560451507568\nstep5515, loss: 1.1056547164916992\nstep5516, loss: 1.1374074220657349\nstep5517, loss: 1.268262505531311\nstep5518, loss: 1.2940510511398315\nstep5519, loss: 1.3037596940994263\nstep5520, loss: 1.1709301471710205\nstep5521, loss: 1.2763216495513916\nstep5522, loss: 1.237985610961914\nstep5523, loss: 1.2000006437301636\nstep5524, loss: 1.1941297054290771\nstep5525, loss: 1.186987042427063\nstep5526, loss: 1.1470253467559814\nstep5527, loss: 1.2294330596923828\nstep5528, loss: 1.2069600820541382\nstep5529, loss: 1.236901044845581\nstep5530, loss: 1.1832876205444336\nstep5531, loss: 1.2077149152755737\nstep5532, loss: 1.2120060920715332\nstep5533, loss: 1.1983840465545654\nstep5534, loss: 1.1046473979949951\nstep5535, loss: 1.2269108295440674\nstep5536, loss: 1.2456539869308472\nstep5537, loss: 1.2994145154953003\nstep5538, loss: 1.3490791320800781\nstep5539, loss: 1.3163366317749023\nstep5540, loss: 1.3513625860214233\nstep5541, loss: 1.2064627408981323\nstep5542, loss: 1.3228667974472046\nstep5543, loss: 1.218471646308899\nstep5544, loss: 1.17848539352417\nstep5545, loss: 1.179995059967041\nstep5546, loss: 1.1734381914138794\nstep5547, loss: 1.1923681497573853\nstep5548, loss: 1.259986400604248\nstep5549, loss: 1.301554560661316\nstep5550, loss: 1.2709022760391235\nstep5551, loss: 1.223417043685913\nstep5552, loss: 1.1779165267944336\nstep5553, loss: 1.0921250581741333\nstep5554, loss: 1.2786120176315308\nstep5555, loss: 1.2530596256256104\nstep5556, loss: 1.1278395652770996\nstep5557, loss: 1.0265902280807495\nstep5558, loss: 1.189295768737793\nstep5559, loss: 1.1765284538269043\nstep5560, loss: 1.0149197578430176\nstep5561, loss: 1.0433741807937622\nstep5562, loss: 1.0049729347229004\nstep5563, loss: 1.045057773590088\nstep5564, loss: 1.0914239883422852\nstep5565, loss: 1.2173724174499512\nstep5566, loss: 1.1695829629898071\nstep5567, loss: 1.1124578714370728\nstep5568, loss: 1.0399476289749146\nstep5569, loss: 1.0374763011932373\nstep5570, loss: 0.9969528913497925\nstep5571, loss: 0.9926645159721375\nstep5572, loss: 1.0454856157302856\nstep5573, loss: 0.9921995401382446\nstep5574, loss: 1.2129395008087158\nstep5575, loss: 1.1131864786148071\nstep5576, loss: 1.2294726371765137\nstep5577, loss: 1.1928882598876953\nstep5578, loss: 1.1458557844161987\nstep5579, loss: 1.1430693864822388\nstep5580, loss: 1.0991528034210205\nstep5581, loss: 1.0682982206344604\nstep5582, loss: 1.1301279067993164\nstep5583, loss: 1.087302565574646\nstep5584, loss: 1.1083478927612305\nstep5585, loss: 1.1590189933776855\nstep5586, loss: 1.1912097930908203\nstep5587, loss: 1.1811854839324951\nstep5588, loss: 1.147019624710083\nstep5589, loss: 1.1783387660980225\nstep5590, loss: 1.0447592735290527\nstep5591, loss: 1.2401572465896606\nstep5592, loss: 1.0620918273925781\nstep5593, loss: 1.0769027471542358\nstep5594, loss: 1.139674186706543\nstep5595, loss: 1.089809536933899\nstep5596, loss: 1.1351404190063477\nstep5597, loss: 1.0657868385314941\nstep5598, loss: 1.0889060497283936\nstep5599, loss: 1.1973865032196045\nstep5600, loss: 1.166264533996582\nstep5601, loss: 1.2049353122711182\nstep5602, loss: 1.135822057723999\nstep5603, loss: 1.191430687904358\nstep5604, loss: 1.1620020866394043\nstep5605, loss: 1.1440149545669556\nstep5606, loss: 1.1138511896133423\nstep5607, loss: 1.1522783041000366\nstep5608, loss: 1.0511245727539062\nstep5609, loss: 1.094741940498352\nstep5610, loss: 1.0915942192077637\nstep5611, loss: 1.138654112815857\nstep5612, loss: 1.0562992095947266\nstep5613, loss: 1.1127294301986694\nstep5614, loss: 1.1252827644348145\nstep5615, loss: 1.125423789024353\nstep5616, loss: 1.0162347555160522\nstep5617, loss: 1.160019040107727\nstep5618, loss: 1.1508411169052124\nstep5619, loss: 1.179722785949707\nstep5620, loss: 1.2092067003250122\nstep5621, loss: 1.1706901788711548\nstep5622, loss: 1.2155683040618896\nstep5623, loss: 1.1098146438598633\nstep5624, loss: 1.2367926836013794\nstep5625, loss: 1.1517839431762695\nstep5626, loss: 1.1269567012786865\nstep5627, loss: 1.14141047000885\nstep5628, loss: 1.1305288076400757\nstep5629, loss: 1.117789626121521\nstep5630, loss: 1.215819001197815\nstep5631, loss: 1.2245876789093018\nstep5632, loss: 1.166714072227478\nstep5633, loss: 1.1109504699707031\nstep5634, loss: 1.0654263496398926\nstep5635, loss: 1.0090771913528442\nstep5636, loss: 1.1975702047348022\nstep5637, loss: 1.1798336505889893\nstep5638, loss: 1.066280722618103\nstep5639, loss: 0.9873616695404053\nstep5640, loss: 1.1527689695358276\nstep5641, loss: 1.1451016664505005\nstep5642, loss: 0.950842559337616\nstep5643, loss: 0.9914924502372742\nstep5644, loss: 0.9601115584373474\nstep5645, loss: 0.9776278734207153\nstep5646, loss: 1.0041208267211914\nstep5647, loss: 1.1108342409133911\nstep5648, loss: 1.04840087890625\nstep5649, loss: 0.9946193695068359\nstep5650, loss: 0.9647679328918457\nstep5651, loss: 0.9527708292007446\nstep5652, loss: 0.9182407855987549\nstep5653, loss: 0.9198803901672363\nstep5654, loss: 0.9639859199523926\nstep5655, loss: 0.9411655068397522\nstep5656, loss: 1.1263071298599243\nstep5657, loss: 1.00160551071167\nstep5658, loss: 1.1327815055847168\nstep5659, loss: 1.1169565916061401\nstep5660, loss: 1.0471856594085693\nstep5661, loss: 1.052668809890747\nstep5662, loss: 1.0228567123413086\nstep5663, loss: 0.976071834564209\nstep5664, loss: 1.0539878606796265\nstep5665, loss: 0.9876639246940613\nstep5666, loss: 1.021554946899414\nstep5667, loss: 1.1037535667419434\nstep5668, loss: 1.1227000951766968\nstep5669, loss: 1.0989081859588623\nstep5670, loss: 1.0990184545516968\nstep5671, loss: 1.102372407913208\nstep5672, loss: 0.9499880075454712\nstep5673, loss: 1.1378501653671265\nstep5674, loss: 1.0031507015228271\nstep5675, loss: 0.9980059266090393\nstep5676, loss: 1.0806434154510498\nstep5677, loss: 1.0168570280075073\nstep5678, loss: 1.0593016147613525\nstep5679, loss: 1.015800952911377\nstep5680, loss: 1.0206902027130127\nstep5681, loss: 1.1032246351242065\nstep5682, loss: 1.1159865856170654\nstep5683, loss: 1.1582398414611816\nstep5684, loss: 1.0549947023391724\nstep5685, loss: 1.1124736070632935\nstep5686, loss: 1.092008352279663\nstep5687, loss: 1.0872001647949219\nstep5688, loss: 1.043779969215393\nstep5689, loss: 1.076846718788147\nstep5690, loss: 1.0096014738082886\nstep5691, loss: 1.1097058057785034\nstep5692, loss: 1.0584725141525269\nstep5693, loss: 1.0980397462844849\nstep5694, loss: 0.9777035713195801\nstep5695, loss: 1.0721267461776733\nstep5696, loss: 1.0664901733398438\nstep5697, loss: 1.0753706693649292\nstep5698, loss: 0.9924336075782776\nstep5699, loss: 1.1437418460845947\nstep5700, loss: 1.1279704570770264\nstep5701, loss: 1.1389802694320679\nstep5702, loss: 1.1671910285949707\nstep5703, loss: 1.1039828062057495\nstep5704, loss: 1.1362110376358032\nstep5705, loss: 1.0084701776504517\nstep5706, loss: 1.1697053909301758\nstep5707, loss: 1.1025831699371338\nstep5708, loss: 1.0813194513320923\nstep5709, loss: 1.097330927848816\nstep5710, loss: 1.1030054092407227\nstep5711, loss: 1.0780266523361206\nstep5712, loss: 1.1811864376068115\nstep5713, loss: 1.188807487487793\nstep5714, loss: 1.1093531847000122\nstep5715, loss: 1.0044374465942383\nstep5716, loss: 1.0272279977798462\nstep5717, loss: 0.9623442888259888\nstep5718, loss: 1.1408289670944214\nstep5719, loss: 1.1393259763717651\nstep5720, loss: 1.0055663585662842\nstep5721, loss: 0.928111732006073\nstep5722, loss: 1.0778236389160156\nstep5723, loss: 1.0585147142410278\nstep5724, loss: 0.9283112287521362\nstep5725, loss: 0.9455069899559021\nstep5726, loss: 0.940885066986084\nstep5727, loss: 0.9897267818450928\nstep5728, loss: 0.9814187288284302\nstep5729, loss: 1.1193584203720093\nstep5730, loss: 1.0760680437088013\nstep5731, loss: 1.005111813545227\nstep5732, loss: 0.9243476986885071\nstep5733, loss: 0.9088791012763977\nstep5734, loss: 0.875930905342102\nstep5735, loss: 0.8766303062438965\nstep5736, loss: 0.9213200807571411\nstep5737, loss: 0.871212363243103\nstep5738, loss: 1.0615283250808716\nstep5739, loss: 0.9744786024093628\nstep5740, loss: 1.038877248764038\nstep5741, loss: 1.0342366695404053\nstep5742, loss: 1.0007312297821045\nstep5743, loss: 0.9743555784225464\nstep5744, loss: 0.9506995677947998\nstep5745, loss: 0.9142314791679382\nstep5746, loss: 1.0168201923370361\nstep5747, loss: 0.9340905547142029\nstep5748, loss: 0.9724715948104858\nstep5749, loss: 1.0621460676193237\nstep5750, loss: 1.063501000404358\nstep5751, loss: 1.0271517038345337\nstep5752, loss: 1.014965534210205\nstep5753, loss: 1.0846766233444214\nstep5754, loss: 0.9605596661567688\nstep5755, loss: 1.1370575428009033\nstep5756, loss: 0.9780121445655823\nstep5757, loss: 1.0143760442733765\nstep5758, loss: 1.0204814672470093\nstep5759, loss: 0.9824673533439636\nstep5760, loss: 1.0356502532958984\nstep5761, loss: 0.9566245675086975\nstep5762, loss: 0.9523124694824219\nstep5763, loss: 1.0151844024658203\nstep5764, loss: 1.0972306728363037\nstep5765, loss: 1.119591474533081\nstep5766, loss: 1.031421422958374\nstep5767, loss: 1.0904247760772705\nstep5768, loss: 1.0281440019607544\nstep5769, loss: 1.036770224571228\nstep5770, loss: 0.987108051776886\nstep5771, loss: 1.017983317375183\nstep5772, loss: 0.9672166705131531\nstep5773, loss: 1.036012053489685\nstep5774, loss: 1.021653175354004\nstep5775, loss: 1.0233378410339355\nstep5776, loss: 0.9541483521461487\nstep5777, loss: 1.0288808345794678\nstep5778, loss: 1.057676076889038\nstep5779, loss: 1.0319721698760986\nstep5780, loss: 0.9196693301200867\nstep5781, loss: 1.0687565803527832\nstep5782, loss: 1.0461061000823975\nstep5783, loss: 1.0672975778579712\nstep5784, loss: 1.0985914468765259\nstep5785, loss: 1.0436800718307495\nstep5786, loss: 1.0976570844650269\nstep5787, loss: 0.9722990989685059\nstep5788, loss: 1.09807288646698\nstep5789, loss: 0.9891497492790222\nstep5790, loss: 0.9562586545944214\nstep5791, loss: 0.9694538712501526\nstep5792, loss: 0.9682265520095825\nstep5793, loss: 1.0137242078781128\nstep5794, loss: 1.1058743000030518\nstep5795, loss: 1.130147933959961\nstep5796, loss: 1.0529634952545166\nstep5797, loss: 0.9899282455444336\nstep5798, loss: 1.0059868097305298\nstep5799, loss: 0.9003577828407288\nstep5800, loss: 1.0438921451568604\nstep5801, loss: 1.0397456884384155\nstep5802, loss: 0.9153684377670288\nstep5803, loss: 0.8494126200675964\nstep5804, loss: 1.0086824893951416\nstep5805, loss: 1.0190037488937378\nstep5806, loss: 0.8865809440612793\nstep5807, loss: 0.9014902114868164\nstep5808, loss: 0.8693339824676514\nstep5809, loss: 0.9208161234855652\nstep5810, loss: 0.9035566449165344\nstep5811, loss: 1.049633264541626\nstep5812, loss: 1.015778660774231\nstep5813, loss: 0.9861325025558472\nstep5814, loss: 0.9179685115814209\nstep5815, loss: 0.9336545467376709\nstep5816, loss: 0.8751132488250732\nstep5817, loss: 0.8679199814796448\nstep5818, loss: 0.8689346313476562\nstep5819, loss: 0.8382291197776794\nstep5820, loss: 0.968834638595581\nstep5821, loss: 0.9053038358688354\nstep5822, loss: 0.9808320999145508\nstep5823, loss: 1.0001477003097534\nstep5824, loss: 0.9459326863288879\nstep5825, loss: 0.9125679135322571\nstep5826, loss: 0.927524983882904\nstep5827, loss: 0.8651102185249329\nstep5828, loss: 0.9406341910362244\nstep5829, loss: 0.8853333592414856\nstep5830, loss: 0.8840588927268982\nstep5831, loss: 0.9748051762580872\nstep5832, loss: 1.008442997932434\nstep5833, loss: 1.0217251777648926\nstep5834, loss: 0.9885112047195435\nstep5835, loss: 1.0185093879699707\nstep5836, loss: 0.8916153311729431\nstep5837, loss: 1.0544776916503906\nstep5838, loss: 0.9138954281806946\nstep5839, loss: 0.9679084420204163\nstep5840, loss: 0.9667653441429138\nstep5841, loss: 0.9426884055137634\nstep5842, loss: 0.9827827215194702\nstep5843, loss: 0.9392797946929932\nstep5844, loss: 0.913769543170929\nstep5845, loss: 0.9651190638542175\nstep5846, loss: 1.0201009511947632\nstep5847, loss: 1.0095292329788208\nstep5848, loss: 0.9620672464370728\nstep5849, loss: 1.019574522972107\nstep5850, loss: 0.9880254864692688\nstep5851, loss: 1.0033756494522095\nstep5852, loss: 0.9289003014564514\nstep5853, loss: 0.9989408850669861\nstep5854, loss: 0.9244336485862732\nstep5855, loss: 0.9709445834159851\nstep5856, loss: 0.9560783505439758\nstep5857, loss: 0.9336426854133606\nstep5858, loss: 0.9104764461517334\nstep5859, loss: 0.946199893951416\nstep5860, loss: 0.9664624333381653\nstep5861, loss: 1.0034615993499756\nstep5862, loss: 0.8706172704696655\nstep5863, loss: 1.0122311115264893\nstep5864, loss: 1.0446176528930664\nstep5865, loss: 1.0548408031463623\nstep5866, loss: 1.019836664199829\nstep5867, loss: 0.9888067841529846\nstep5868, loss: 0.9993984699249268\nstep5869, loss: 1.0160279273986816\nstep5870, loss: 1.0781834125518799\nstep5871, loss: 0.989924430847168\nstep5872, loss: 0.9571924805641174\nstep5873, loss: 0.966298520565033\nstep5874, loss: 0.9741122126579285\nstep5875, loss: 0.9698184728622437\nstep5876, loss: 1.0393086671829224\nstep5877, loss: 1.0138806104660034\nstep5878, loss: 0.960099458694458\nstep5879, loss: 0.9386743903160095\nstep5880, loss: 0.9426535367965698\nstep5881, loss: 0.859719455242157\nstep5882, loss: 1.0181392431259155\nstep5883, loss: 1.0286359786987305\nstep5884, loss: 0.9333454370498657\nstep5885, loss: 0.8530057668685913\nstep5886, loss: 0.972451388835907\nstep5887, loss: 0.9862937927246094\nstep5888, loss: 0.8090524673461914\nstep5889, loss: 0.8427879810333252\nstep5890, loss: 0.8484960198402405\nstep5891, loss: 0.9057075381278992\nstep5892, loss: 0.9095685482025146\nstep5893, loss: 1.0225872993469238\nstep5894, loss: 1.0208079814910889\nstep5895, loss: 0.9052363634109497\nstep5896, loss: 0.8876571655273438\nstep5897, loss: 0.8463153839111328\nstep5898, loss: 0.8195730447769165\nstep5899, loss: 0.8316517472267151\nstep5900, loss: 0.8827524185180664\nstep5901, loss: 0.8555812239646912\nstep5902, loss: 0.9847647547721863\nstep5903, loss: 0.9307489395141602\nstep5904, loss: 0.9948157668113708\nstep5905, loss: 0.9489665031433105\nstep5906, loss: 0.8924328684806824\nstep5907, loss: 0.8717705011367798\nstep5908, loss: 0.879789412021637\nstep5909, loss: 0.8593479990959167\nstep5910, loss: 0.8987261056900024\nstep5911, loss: 0.8659014105796814\nstep5912, loss: 0.865812361240387\nstep5913, loss: 0.9255796670913696\nstep5914, loss: 0.9579815864562988\nstep5915, loss: 0.9464604258537292\nstep5916, loss: 0.9304323792457581\nstep5917, loss: 0.9546131491661072\nstep5918, loss: 0.8147011995315552\nstep5919, loss: 1.0016367435455322\nstep5920, loss: 0.8846945762634277\nstep5921, loss: 0.9038915634155273\nstep5922, loss: 0.8737888932228088\nstep5923, loss: 0.8860452175140381\nstep5924, loss: 0.9302739500999451\nstep5925, loss: 0.875899612903595\nstep5926, loss: 0.8939878344535828\nstep5927, loss: 0.9576988220214844\nstep5928, loss: 0.9704428911209106\nstep5929, loss: 0.9375718235969543\nstep5930, loss: 0.9251508116722107\nstep5931, loss: 0.934740424156189\nstep5932, loss: 0.94099360704422\nstep5933, loss: 0.9023326635360718\nstep5934, loss: 0.8795886635780334\nstep5935, loss: 0.9465658664703369\nstep5936, loss: 0.8918173313140869\nstep5937, loss: 0.9614232182502747\nstep5938, loss: 0.941180408000946\nstep5939, loss: 0.9536860585212708\nstep5940, loss: 0.871478259563446\nstep5941, loss: 0.8990968465805054\nstep5942, loss: 0.9124804139137268\nstep5943, loss: 0.9093952775001526\nstep5944, loss: 0.828136146068573\nstep5945, loss: 0.9385042786598206\nstep5946, loss: 0.9603171944618225\nstep5947, loss: 0.9714663028717041\nstep5948, loss: 0.994359016418457\nstep5949, loss: 0.9348042607307434\nstep5950, loss: 0.9418128132820129\nstep5951, loss: 0.8625858426094055\nstep5952, loss: 0.9998292922973633\nstep5953, loss: 0.9106278419494629\nstep5954, loss: 0.9100964665412903\nstep5955, loss: 0.8664449453353882\nstep5956, loss: 0.9108972549438477\nstep5957, loss: 0.879831850528717\nstep5958, loss: 0.9675924181938171\nstep5959, loss: 0.9672858119010925\nstep5960, loss: 0.8931537866592407\nstep5961, loss: 0.8556822538375854\nstep5962, loss: 0.9021965861320496\nstep5963, loss: 0.7894140481948853\nstep5964, loss: 0.964794397354126\nstep5965, loss: 0.9673354625701904\nstep5966, loss: 0.8707869052886963\nstep5967, loss: 0.776814877986908\nstep5968, loss: 0.9652983546257019\nstep5969, loss: 0.9292067885398865\nstep5970, loss: 0.8393110036849976\nstep5971, loss: 0.8123263716697693\nstep5972, loss: 0.7714316248893738\nstep5973, loss: 0.8338251709938049\nstep5974, loss: 0.8254413604736328\nstep5975, loss: 0.9648757576942444\nstep5976, loss: 0.957313060760498\nstep5977, loss: 0.9240349531173706\nstep5978, loss: 0.8755898475646973\nstep5979, loss: 0.8439390063285828\nstep5980, loss: 0.7583190202713013\nstep5981, loss: 0.7668224573135376\nstep5982, loss: 0.8054229021072388\nstep5983, loss: 0.7625171542167664\nstep5984, loss: 0.9550602436065674\nstep5985, loss: 0.9064354300498962\nstep5986, loss: 1.0323772430419922\nstep5987, loss: 0.9840850830078125\nstep5988, loss: 0.9097790718078613\nstep5989, loss: 0.8848671317100525\nstep5990, loss: 0.8528928160667419\nstep5991, loss: 0.7767019867897034\nstep5992, loss: 0.8720897436141968\nstep5993, loss: 0.8236649632453918\nstep5994, loss: 0.8575714230537415\nstep5995, loss: 0.919580340385437\nstep5996, loss: 0.9431520700454712\nstep5997, loss: 0.9705533981323242\nstep5998, loss: 0.9244258999824524\nstep5999, loss: 0.9350350499153137\nstep6000, loss: 0.8283286690711975\nstep6001, loss: 0.9654792547225952\nstep6002, loss: 0.8576979637145996\nstep6003, loss: 0.8640313148498535\nstep6004, loss: 0.8702837228775024\nstep6005, loss: 0.8473221063613892\nstep6006, loss: 0.9145219326019287\nstep6007, loss: 0.8623432517051697\nstep6008, loss: 0.9024691581726074\nstep6009, loss: 0.9269851446151733\nstep6010, loss: 0.925727903842926\nstep6011, loss: 0.9042623043060303\nstep6012, loss: 0.8685161471366882\nstep6013, loss: 0.9152828454971313\nstep6014, loss: 0.8737781643867493\nstep6015, loss: 0.8798568844795227\nstep6016, loss: 0.8513832688331604\nstep6017, loss: 0.9085688591003418\nstep6018, loss: 0.8411819934844971\nstep6019, loss: 0.8857327699661255\nstep6020, loss: 0.8944231271743774\nstep6021, loss: 0.8918294906616211\nstep6022, loss: 0.8172546625137329\nstep6023, loss: 0.8466076254844666\nstep6024, loss: 0.8693370223045349\nstep6025, loss: 0.9012104272842407\nstep6026, loss: 0.792321503162384\nstep6027, loss: 0.9075082540512085\nstep6028, loss: 0.8957106471061707\nstep6029, loss: 0.9436088800430298\nstep6030, loss: 0.9107908010482788\nstep6031, loss: 0.9083119630813599\nstep6032, loss: 0.8902063369750977\nstep6033, loss: 0.8460705280303955\nstep6034, loss: 0.9547815322875977\nstep6035, loss: 0.8622004985809326\nstep6036, loss: 0.819608747959137\nstep6037, loss: 0.8518402576446533\nstep6038, loss: 0.86089026927948\nstep6039, loss: 0.8706216216087341\nstep6040, loss: 0.9451500773429871\nstep6041, loss: 0.956398069858551\nstep6042, loss: 0.835507333278656\nstep6043, loss: 0.8117709755897522\nstep6044, loss: 0.8092882633209229\nstep6045, loss: 0.7470329999923706\nstep6046, loss: 0.8880816698074341\nstep6047, loss: 0.8548572063446045\nstep6048, loss: 0.7977210879325867\nstep6049, loss: 0.753664493560791\nstep6050, loss: 0.87601637840271\nstep6051, loss: 0.855455219745636\nstep6052, loss: 0.7744911909103394\nstep6053, loss: 0.8073235154151917\nstep6054, loss: 0.7558578252792358\nstep6055, loss: 0.7954435348510742\nstep6056, loss: 0.7599208950996399\nstep6057, loss: 0.8939170241355896\nstep6058, loss: 0.9207972288131714\nstep6059, loss: 0.8737335205078125\nstep6060, loss: 0.7977895140647888\nstep6061, loss: 0.7460778951644897\nstep6062, loss: 0.7291098833084106\nstep6063, loss: 0.7842295169830322\nstep6064, loss: 0.8081494569778442\nstep6065, loss: 0.7676751017570496\nstep6066, loss: 0.9659274220466614\nstep6067, loss: 0.9038791656494141\nstep6068, loss: 0.9579012393951416\nstep6069, loss: 0.885820209980011\nstep6070, loss: 0.8351294994354248\nstep6071, loss: 0.7918919324874878\nstep6072, loss: 0.8503888249397278\nstep6073, loss: 0.7673525810241699\nstep6074, loss: 0.8597658276557922\nstep6075, loss: 0.8613700866699219\nstep6076, loss: 0.8393845558166504\nstep6077, loss: 0.8598268628120422\nstep6078, loss: 0.888368546962738\nstep6079, loss: 0.853325366973877\nstep6080, loss: 0.8195849657058716\nstep6081, loss: 0.9032984375953674\nstep6082, loss: 0.8031629920005798\nstep6083, loss: 0.9633049964904785\nstep6084, loss: 0.8674641251564026\nstep6085, loss: 0.8509244918823242\nstep6086, loss: 0.8327481746673584\nstep6087, loss: 0.8227543234825134\nstep6088, loss: 0.8437512516975403\nstep6089, loss: 0.8012773394584656\nstep6090, loss: 0.8292132019996643\nstep6091, loss: 0.858761191368103\nstep6092, loss: 0.899999737739563\nstep6093, loss: 0.9656038284301758\nstep6094, loss: 0.8557134866714478\nstep6095, loss: 0.8966104388237\nstep6096, loss: 0.8541704416275024\nstep6097, loss: 0.8414080142974854\nstep6098, loss: 0.781204342842102\nstep6099, loss: 0.845592200756073\nstep6100, loss: 0.811196506023407\nstep6101, loss: 0.8569345474243164\nstep6102, loss: 0.8309245109558105\nstep6103, loss: 0.8458712697029114\nstep6104, loss: 0.7768500447273254\nstep6105, loss: 0.8226191401481628\nstep6106, loss: 0.8406600952148438\nstep6107, loss: 0.8485435843467712\nstep6108, loss: 0.7275410294532776\nstep6109, loss: 0.8787069320678711\nstep6110, loss: 0.8771143555641174\nstep6111, loss: 0.9099741578102112\nstep6112, loss: 0.8972381353378296\nstep6113, loss: 0.8595751523971558\nstep6114, loss: 0.8599300980567932\nstep6115, loss: 0.7773361206054688\nstep6116, loss: 0.909822404384613\nstep6117, loss: 0.821959912776947\nstep6118, loss: 0.7999952435493469\nstep6119, loss: 0.8113029599189758\nstep6120, loss: 0.8215601444244385\nstep6121, loss: 0.8257851600646973\nstep6122, loss: 0.8941528797149658\nstep6123, loss: 0.9236817955970764\nstep6124, loss: 0.8002904653549194\nstep6125, loss: 0.7644646763801575\nstep6126, loss: 0.8127444386482239\nstep6127, loss: 0.6879092454910278\nstep6128, loss: 0.8168398141860962\nstep6129, loss: 0.7869660258293152\nstep6130, loss: 0.7253741025924683\nstep6131, loss: 0.6867274045944214\nstep6132, loss: 0.8340286612510681\nstep6133, loss: 0.799807608127594\nstep6134, loss: 0.7322017550468445\nstep6135, loss: 0.7266398668289185\nstep6136, loss: 0.6778333187103271\nstep6137, loss: 0.7352871298789978\nstep6138, loss: 0.744098424911499\nstep6139, loss: 0.8456538319587708\nstep6140, loss: 0.8797636032104492\nstep6141, loss: 0.8396188616752625\nstep6142, loss: 0.8052367568016052\nstep6143, loss: 0.7305256128311157\nstep6144, loss: 0.7151604294776917\nstep6145, loss: 0.6892899870872498\nstep6146, loss: 0.7047753930091858\nstep6147, loss: 0.6909750699996948\nstep6148, loss: 0.8713048696517944\nstep6149, loss: 0.7902328968048096\nstep6150, loss: 0.8981897234916687\nstep6151, loss: 0.8289521336555481\nstep6152, loss: 0.7822408676147461\nstep6153, loss: 0.7775143384933472\nstep6154, loss: 0.7788671851158142\nstep6155, loss: 0.7154988646507263\nstep6156, loss: 0.7820965051651001\nstep6157, loss: 0.7602173686027527\nstep6158, loss: 0.768904983997345\nstep6159, loss: 0.838424563407898\nstep6160, loss: 0.8566597104072571\nstep6161, loss: 0.8413089513778687\nstep6162, loss: 0.8528223633766174\nstep6163, loss: 0.8556607961654663\nstep6164, loss: 0.7780734896659851\nstep6165, loss: 0.8860759139060974\nstep6166, loss: 0.769282341003418\nstep6167, loss: 0.7563081383705139\nstep6168, loss: 0.776187002658844\nstep6169, loss: 0.7718265056610107\nstep6170, loss: 0.7729234099388123\nstep6171, loss: 0.7408778667449951\nstep6172, loss: 0.738707959651947\nstep6173, loss: 0.8111030459403992\nstep6174, loss: 0.8429718017578125\nstep6175, loss: 0.8775847554206848\nstep6176, loss: 0.7791054248809814\nstep6177, loss: 0.8503566980361938\nstep6178, loss: 0.7841115593910217\nstep6179, loss: 0.8162490725517273\nstep6180, loss: 0.7832927107810974\nstep6181, loss: 0.7897520065307617\nstep6182, loss: 0.7510627508163452\nstep6183, loss: 0.7958908677101135\nstep6184, loss: 0.8044008016586304\nstep6185, loss: 0.7731384634971619\nstep6186, loss: 0.7155921459197998\nstep6187, loss: 0.7236621975898743\nstep6188, loss: 0.7627260684967041\nstep6189, loss: 0.7914111614227295\nstep6190, loss: 0.6843193769454956\nstep6191, loss: 0.8242056965827942\nstep6192, loss: 0.7896727919578552\nstep6193, loss: 0.8429408073425293\nstep6194, loss: 0.8388431072235107\nstep6195, loss: 0.8186060190200806\nstep6196, loss: 0.8265963196754456\nstep6197, loss: 0.7654832005500793\nstep6198, loss: 0.8589922189712524\nstep6199, loss: 0.7499973177909851\nstep6200, loss: 0.7182824611663818\nstep6201, loss: 0.7031365633010864\nstep6202, loss: 0.7351076006889343\nstep6203, loss: 0.7772948741912842\nstep6204, loss: 0.8207659721374512\nstep6205, loss: 0.8405197262763977\nstep6206, loss: 0.7715793251991272\nstep6207, loss: 0.7055063247680664\nstep6208, loss: 0.7157202363014221\nstep6209, loss: 0.6589443683624268\nstep6210, loss: 0.7705115079879761\nstep6211, loss: 0.7649691700935364\nstep6212, loss: 0.6799843311309814\nstep6213, loss: 0.6556529402732849\nstep6214, loss: 0.7786567807197571\nstep6215, loss: 0.7294553518295288\nstep6216, loss: 0.641213595867157\nstep6217, loss: 0.6356690526008606\nstep6218, loss: 0.6315363049507141\nstep6219, loss: 0.6668890118598938\nstep6220, loss: 0.6940587162971497\nstep6221, loss: 0.7628272175788879\nstep6222, loss: 0.8124401569366455\nstep6223, loss: 0.7581482529640198\nstep6224, loss: 0.7049599885940552\nstep6225, loss: 0.6700625419616699\nstep6226, loss: 0.607913076877594\nstep6227, loss: 0.61543208360672\nstep6228, loss: 0.6578236222267151\nstep6229, loss: 0.627325713634491\nstep6230, loss: 0.7544723153114319\nstep6231, loss: 0.7052978873252869\nstep6232, loss: 0.8485085964202881\nstep6233, loss: 0.7481120228767395\nstep6234, loss: 0.6825335025787354\nstep6235, loss: 0.7029085159301758\nstep6236, loss: 0.7193337678909302\nstep6237, loss: 0.648634672164917\nstep6238, loss: 0.7241039276123047\nstep6239, loss: 0.7118349075317383\nstep6240, loss: 0.7091923952102661\nstep6241, loss: 0.7585540413856506\nstep6242, loss: 0.7550771832466125\nstep6243, loss: 0.8250991702079773\nstep6244, loss: 0.7813252210617065\nstep6245, loss: 0.8097612857818604\nstep6246, loss: 0.7259092926979065\nstep6247, loss: 0.8548617959022522\nstep6248, loss: 0.7441090941429138\nstep6249, loss: 0.769319474697113\nstep6250, loss: 0.7675784230232239\nstep6251, loss: 0.7349377274513245\nstep6252, loss: 0.7451199293136597\nstep6253, loss: 0.7115415930747986\nstep6254, loss: 0.7533589005470276\nstep6255, loss: 0.7704647183418274\nstep6256, loss: 0.7928240895271301\nstep6257, loss: 0.8044664859771729\nstep6258, loss: 0.7227011322975159\nstep6259, loss: 0.7768234610557556\nstep6260, loss: 0.7583343386650085\nstep6261, loss: 0.7152062058448792\nstep6262, loss: 0.6927716732025146\nstep6263, loss: 0.7404224276542664\nstep6264, loss: 0.6945019960403442\nstep6265, loss: 0.734402060508728\nstep6266, loss: 0.7318031787872314\nstep6267, loss: 0.7217592000961304\nstep6268, loss: 0.6729079484939575\nstep6269, loss: 0.7069162726402283\nstep6270, loss: 0.7054198980331421\nstep6271, loss: 0.7401761412620544\nstep6272, loss: 0.6558641791343689\nstep6273, loss: 0.7588764429092407\nstep6274, loss: 0.740222692489624\nstep6275, loss: 0.7830842733383179\nstep6276, loss: 0.7729843258857727\nstep6277, loss: 0.7551871538162231\nstep6278, loss: 0.7526631951332092\nstep6279, loss: 0.7074329257011414\nstep6280, loss: 0.7934185266494751\nstep6281, loss: 0.7457666993141174\nstep6282, loss: 0.7000572085380554\nstep6283, loss: 0.6835185885429382\nstep6284, loss: 0.7123326063156128\nstep6285, loss: 0.7009145021438599\nstep6286, loss: 0.7550623416900635\nstep6287, loss: 0.7795383930206299\nstep6288, loss: 0.7018730044364929\nstep6289, loss: 0.6456823945045471\nstep6290, loss: 0.6954214572906494\nstep6291, loss: 0.6375811696052551\nstep6292, loss: 0.7566541433334351\nstep6293, loss: 0.7561903595924377\nstep6294, loss: 0.6665054559707642\nstep6295, loss: 0.6116954684257507\nstep6296, loss: 0.7389250993728638\nstep6297, loss: 0.6931967735290527\nstep6298, loss: 0.5885441303253174\nstep6299, loss: 0.6075543165206909\nstep6300, loss: 0.5745960474014282\nstep6301, loss: 0.6214020848274231\nstep6302, loss: 0.6595296263694763\nstep6303, loss: 0.7035782337188721\nstep6304, loss: 0.7289996147155762\nstep6305, loss: 0.6758617758750916\nstep6306, loss: 0.6684952974319458\nstep6307, loss: 0.6083642244338989\nstep6308, loss: 0.5751672387123108\nstep6309, loss: 0.5920212268829346\nstep6310, loss: 0.6471858024597168\nstep6311, loss: 0.5663264393806458\nstep6312, loss: 0.6955776810646057\nstep6313, loss: 0.6141571402549744\nstep6314, loss: 0.7561272978782654\nstep6315, loss: 0.6711636185646057\nstep6316, loss: 0.6332927346229553\nstep6317, loss: 0.6302822232246399\nstep6318, loss: 0.6581329107284546\nstep6319, loss: 0.596224308013916\nstep6320, loss: 0.6199911236763\nstep6321, loss: 0.6223838329315186\nstep6322, loss: 0.6193212270736694\nstep6323, loss: 0.6925711631774902\nstep6324, loss: 0.6680681705474854\nstep6325, loss: 0.7171053290367126\nstep6326, loss: 0.6950116753578186\nstep6327, loss: 0.7656872272491455\nstep6328, loss: 0.6708284616470337\nstep6329, loss: 0.7740087509155273\nstep6330, loss: 0.6681935787200928\nstep6331, loss: 0.6462590098381042\nstep6332, loss: 0.6591140031814575\nstep6333, loss: 0.6140981316566467\nstep6334, loss: 0.6769302487373352\nstep6335, loss: 0.6161410808563232\nstep6336, loss: 0.673905074596405\nstep6337, loss: 0.7043322920799255\nstep6338, loss: 0.7272500395774841\nstep6339, loss: 0.7617530226707458\nstep6340, loss: 0.699553906917572\nstep6341, loss: 0.7321227192878723\nstep6342, loss: 0.6766298413276672\nstep6343, loss: 0.6531877517700195\nstep6344, loss: 0.6443994045257568\nstep6345, loss: 0.6700133085250854\nstep6346, loss: 0.6534536480903625\nstep6347, loss: 0.6863222122192383\nstep6348, loss: 0.6842101812362671\nstep6349, loss: 0.6714168787002563\nstep6350, loss: 0.6429308652877808\nstep6351, loss: 0.662223756313324\nstep6352, loss: 0.6683139801025391\nstep6353, loss: 0.7051065564155579\nstep6354, loss: 0.6023503541946411\nstep6355, loss: 0.7087971568107605\nstep6356, loss: 0.7381664514541626\nstep6357, loss: 0.717283308506012\nstep6358, loss: 0.7258161306381226\nstep6359, loss: 0.6741596460342407\nstep6360, loss: 0.7037597894668579\nstep6361, loss: 0.6199873685836792\nstep6362, loss: 0.7486070394515991\nstep6363, loss: 0.6544850468635559\nstep6364, loss: 0.6240527033805847\nstep6365, loss: 0.6473037004470825\nstep6366, loss: 0.6881552934646606\nstep6367, loss: 0.6917909979820251\nstep6368, loss: 0.733690619468689\nstep6369, loss: 0.7097136974334717\nstep6370, loss: 0.6361390352249146\nstep6371, loss: 0.5744392275810242\nstep6372, loss: 0.6133820414543152\nstep6373, loss: 0.546886146068573\nstep6374, loss: 0.7060726881027222\nstep6375, loss: 0.6678640246391296\nstep6376, loss: 0.6012545824050903\nstep6377, loss: 0.5712158679962158\nstep6378, loss: 0.6859654784202576\nstep6379, loss: 0.6385472416877747\nstep6380, loss: 0.543962299823761\nstep6381, loss: 0.5276088118553162\nstep6382, loss: 0.539372980594635\nstep6383, loss: 0.5480522513389587\nstep6384, loss: 0.5683639049530029\nstep6385, loss: 0.6738982200622559\nstep6386, loss: 0.7053094506263733\nstep6387, loss: 0.6883144378662109\nstep6388, loss: 0.6251072883605957\nstep6389, loss: 0.5420642495155334\nstep6390, loss: 0.5068491697311401\nstep6391, loss: 0.5271081924438477\nstep6392, loss: 0.5270397663116455\nstep6393, loss: 0.5046568512916565\nstep6394, loss: 0.6212949156761169\nstep6395, loss: 0.5667459964752197\nstep6396, loss: 0.6932939291000366\nstep6397, loss: 0.6340878009796143\nstep6398, loss: 0.5912446975708008\nstep6399, loss: 0.5570331811904907\nstep6400, loss: 0.5712406039237976\nstep6401, loss: 0.5386114716529846\nstep6402, loss: 0.5429196953773499\nstep6403, loss: 0.5422521233558655\nstep6404, loss: 0.5689289569854736\nstep6405, loss: 0.6430918574333191\nstep6406, loss: 0.5934635400772095\nstep6407, loss: 0.6190949082374573\nstep6408, loss: 0.6045311689376831\nstep6409, loss: 0.6259458661079407\nstep6410, loss: 0.5674423575401306\nstep6411, loss: 0.6595906019210815\nstep6412, loss: 0.5770413875579834\nstep6413, loss: 0.59113609790802\nstep6414, loss: 0.5970008969306946\nstep6415, loss: 0.5636575222015381\nstep6416, loss: 0.5590287446975708\nstep6417, loss: 0.5030118823051453\nstep6418, loss: 0.5418473482131958\nstep6419, loss: 0.5953965187072754\nstep6420, loss: 0.6079351305961609\nstep6421, loss: 0.6133759617805481\nstep6422, loss: 0.5890465378761292\nstep6423, loss: 0.6182160973548889\nstep6424, loss: 0.5864976644515991\nstep6425, loss: 0.6049472093582153\nstep6426, loss: 0.5796650052070618\nstep6427, loss: 0.5999829769134521\nstep6428, loss: 0.5571043491363525\nstep6429, loss: 0.6034919023513794\nstep6430, loss: 0.5857749581336975\nstep6431, loss: 0.5860868096351624\nstep6432, loss: 0.5615139007568359\nstep6433, loss: 0.5643117427825928\nstep6434, loss: 0.5821691751480103\nstep6435, loss: 0.6046193838119507\nstep6436, loss: 0.5434799790382385\nstep6437, loss: 0.612271785736084\nstep6438, loss: 0.6748727560043335\nstep6439, loss: 0.6418895721435547\nstep6440, loss: 0.6572549343109131\nstep6441, loss: 0.6324334144592285\nstep6442, loss: 0.6436631679534912\nstep6443, loss: 0.550610363483429\nstep6444, loss: 0.6470736861228943\nstep6445, loss: 0.5834633111953735\nstep6446, loss: 0.5592939853668213\nstep6447, loss: 0.5789620280265808\nstep6448, loss: 0.6217039823532104\nstep6449, loss: 0.6075738668441772\nstep6450, loss: 0.6364142298698425\nstep6451, loss: 0.6580838561058044\nstep6452, loss: 0.584953784942627\nstep6453, loss: 0.5315813422203064\nstep6454, loss: 0.5713288187980652\nstep6455, loss: 0.5145272612571716\nstep6456, loss: 0.6294934749603271\nstep6457, loss: 0.5829229354858398\nstep6458, loss: 0.5627729296684265\nstep6459, loss: 0.5127534866333008\nstep6460, loss: 0.6280757784843445\nstep6461, loss: 0.586990237236023\nstep6462, loss: 0.5312225222587585\nstep6463, loss: 0.527154803276062\nstep6464, loss: 0.49922022223472595\nstep6465, loss: 0.5074278116226196\nstep6466, loss: 0.5026492476463318\nstep6467, loss: 0.5935098528862\nstep6468, loss: 0.6134970784187317\nstep6469, loss: 0.5575316548347473\nstep6470, loss: 0.5361064076423645\nstep6471, loss: 0.4847022294998169\nstep6472, loss: 0.48798370361328125\nstep6473, loss: 0.5109478235244751\nstep6474, loss: 0.5108123421669006\nstep6475, loss: 0.47447094321250916\nstep6476, loss: 0.5629632472991943\nstep6477, loss: 0.5363112688064575\nstep6478, loss: 0.6364701390266418\nstep6479, loss: 0.5634281039237976\nstep6480, loss: 0.5291623473167419\nstep6481, loss: 0.5465049147605896\nstep6482, loss: 0.5362892746925354\nstep6483, loss: 0.533467173576355\nstep6484, loss: 0.49856650829315186\nstep6485, loss: 0.4837290346622467\nstep6486, loss: 0.4870564341545105\nstep6487, loss: 0.5592610239982605\nstep6488, loss: 0.5119050741195679\nstep6489, loss: 0.548978328704834\nstep6490, loss: 0.5255524516105652\nstep6491, loss: 0.5981378555297852\nstep6492, loss: 0.4905551075935364\nstep6493, loss: 0.6105942130088806\nstep6494, loss: 0.528519868850708\nstep6495, loss: 0.5524057745933533\nstep6496, loss: 0.5206255912780762\nstep6497, loss: 0.4916948080062866\nstep6498, loss: 0.5028026700019836\nstep6499, loss: 0.47061020135879517\nstep6500, loss: 0.464962363243103\nstep6501, loss: 0.5140515565872192\nstep6502, loss: 0.5370525121688843\nstep6503, loss: 0.5301521420478821\nstep6504, loss: 0.5013031959533691\nstep6505, loss: 0.5374693870544434\nstep6506, loss: 0.500877857208252\nstep6507, loss: 0.4978983402252197\nstep6508, loss: 0.4770405888557434\nstep6509, loss: 0.5004122257232666\nstep6510, loss: 0.489113986492157\nstep6511, loss: 0.5327286720275879\nstep6512, loss: 0.5163778066635132\nstep6513, loss: 0.5308414101600647\nstep6514, loss: 0.5138406753540039\nstep6515, loss: 0.52094566822052\nstep6516, loss: 0.5264386534690857\nstep6517, loss: 0.5317453145980835\nstep6518, loss: 0.48369988799095154\nstep6519, loss: 0.5381250977516174\nstep6520, loss: 0.5428865551948547\nstep6521, loss: 0.5437939763069153\nstep6522, loss: 0.5522274971008301\nstep6523, loss: 0.5498442053794861\nstep6524, loss: 0.564005434513092\nstep6525, loss: 0.5285617709159851\nstep6526, loss: 0.5801296234130859\nstep6527, loss: 0.5183639526367188\nstep6528, loss: 0.48555707931518555\nstep6529, loss: 0.5010600090026855\nstep6530, loss: 0.5235320925712585\nstep6531, loss: 0.5411722660064697\nstep6532, loss: 0.5582388043403625\nstep6533, loss: 0.5746809244155884\nstep6534, loss: 0.5526407361030579\nstep6535, loss: 0.4912961721420288\nstep6536, loss: 0.5100206136703491\nstep6537, loss: 0.45635461807250977\nstep6538, loss: 0.557855486869812\nstep6539, loss: 0.5010787844657898\nstep6540, loss: 0.45230725407600403\nstep6541, loss: 0.45968401432037354\nstep6542, loss: 0.5481096506118774\nstep6543, loss: 0.5003203749656677\nstep6544, loss: 0.4421658217906952\nstep6545, loss: 0.4428598880767822\nstep6546, loss: 0.4372374713420868\nstep6547, loss: 0.48334193229675293\nstep6548, loss: 0.4924784004688263\nstep6549, loss: 0.581254243850708\nstep6550, loss: 0.5486733317375183\nstep6551, loss: 0.5039097666740417\nstep6552, loss: 0.490287721157074\nstep6553, loss: 0.4479006230831146\nstep6554, loss: 0.4117019772529602\nstep6555, loss: 0.4317030906677246\nstep6556, loss: 0.4310592710971832\nstep6557, loss: 0.4248095750808716\nstep6558, loss: 0.5039995908737183\nstep6559, loss: 0.4715842008590698\nstep6560, loss: 0.5442577600479126\nstep6561, loss: 0.49152448773384094\nstep6562, loss: 0.4814291000366211\nstep6563, loss: 0.4784265160560608\nstep6564, loss: 0.4511546194553375\nstep6565, loss: 0.43110552430152893\nstep6566, loss: 0.4412425756454468\nstep6567, loss: 0.44646406173706055\nstep6568, loss: 0.4667629599571228\nstep6569, loss: 0.4836002588272095\nstep6570, loss: 0.47578248381614685\nstep6571, loss: 0.5136000514030457\nstep6572, loss: 0.5084731578826904\nstep6573, loss: 0.504504919052124\nstep6574, loss: 0.4420466423034668\nstep6575, loss: 0.4993439018726349\nstep6576, loss: 0.4529668688774109\nstep6577, loss: 0.43907901644706726\nstep6578, loss: 0.44342389702796936\nstep6579, loss: 0.42702218890190125\nstep6580, loss: 0.46978291869163513\nstep6581, loss: 0.4403371214866638\nstep6582, loss: 0.41066983342170715\nstep6583, loss: 0.4342593848705292\nstep6584, loss: 0.45141366124153137\nstep6585, loss: 0.4556572735309601\nstep6586, loss: 0.42197951674461365\nstep6587, loss: 0.4510754942893982\nstep6588, loss: 0.4099443554878235\nstep6589, loss: 0.4151928722858429\nstep6590, loss: 0.39084261655807495\nstep6591, loss: 0.4407027065753937\nstep6592, loss: 0.4220653474330902\nstep6593, loss: 0.4344409108161926\nstep6594, loss: 0.4241524636745453\nstep6595, loss: 0.4392329752445221\nstep6596, loss: 0.4063027501106262\nstep6597, loss: 0.4318968653678894\nstep6598, loss: 0.43145081400871277\nstep6599, loss: 0.466248482465744\nstep6600, loss: 0.4116602838039398\nstep6601, loss: 0.47568628191947937\nstep6602, loss: 0.48914676904678345\nstep6603, loss: 0.48627379536628723\nstep6604, loss: 0.4420135021209717\nstep6605, loss: 0.45302310585975647\nstep6606, loss: 0.4486371576786041\nstep6607, loss: 0.419906884431839\nstep6608, loss: 0.4875553250312805\nstep6609, loss: 0.45547574758529663\nstep6610, loss: 0.41153737902641296\nstep6611, loss: 0.4209863543510437\nstep6612, loss: 0.4680222272872925\nstep6613, loss: 0.47257673740386963\nstep6614, loss: 0.4853183627128601\nstep6615, loss: 0.5014592409133911\nstep6616, loss: 0.45513519644737244\nstep6617, loss: 0.4236198961734772\nstep6618, loss: 0.4105696380138397\nstep6619, loss: 0.3867194950580597\nstep6620, loss: 0.4884435534477234\nstep6621, loss: 0.42544180154800415\nstep6622, loss: 0.395106703042984\nstep6623, loss: 0.3900533616542816\nstep6624, loss: 0.4687401354312897\nstep6625, loss: 0.42658182978630066\nstep6626, loss: 0.38622525334358215\nstep6627, loss: 0.36106032133102417\nstep6628, loss: 0.3647952377796173\nstep6629, loss: 0.4193006455898285\nstep6630, loss: 0.41181066632270813\nstep6631, loss: 0.460764616727829\nstep6632, loss: 0.5093311667442322\nstep6633, loss: 0.4937148690223694\nstep6634, loss: 0.4464341104030609\nstep6635, loss: 0.4137355387210846\nstep6636, loss: 0.36135557293891907\nstep6637, loss: 0.3708896338939667\nstep6638, loss: 0.3930979073047638\nstep6639, loss: 0.3622937798500061\nstep6640, loss: 0.4106869101524353\nstep6641, loss: 0.39010074734687805\nstep6642, loss: 0.4707593023777008\nstep6643, loss: 0.459220290184021\nstep6644, loss: 0.42502695322036743\nstep6645, loss: 0.41693294048309326\nstep6646, loss: 0.44013386964797974\nstep6647, loss: 0.3821156322956085\nstep6648, loss: 0.38349950313568115\nstep6649, loss: 0.38910022377967834\nstep6650, loss: 0.3766458034515381\nstep6651, loss: 0.3972548246383667\nstep6652, loss: 0.37903136014938354\nstep6653, loss: 0.42691659927368164\nstep6654, loss: 0.43058398365974426\nstep6655, loss: 0.46538421511650085\nstep6656, loss: 0.40888020396232605\nstep6657, loss: 0.4828120470046997\nstep6658, loss: 0.42372679710388184\nstep6659, loss: 0.4245646297931671\nstep6660, loss: 0.40818893909454346\nstep6661, loss: 0.37227267026901245\nstep6662, loss: 0.38293692469596863\nstep6663, loss: 0.36918601393699646\nstep6664, loss: 0.3423483967781067\nstep6665, loss: 0.3656153976917267\nstep6666, loss: 0.3943711221218109\nstep6667, loss: 0.4169231355190277\nstep6668, loss: 0.37482455372810364\nstep6669, loss: 0.3992704153060913\nstep6670, loss: 0.3717448115348816\nstep6671, loss: 0.38670727610588074\nstep6672, loss: 0.3552285432815552\nstep6673, loss: 0.38664984703063965\nstep6674, loss: 0.3627442419528961\nstep6675, loss: 0.36958956718444824\nstep6676, loss: 0.36531952023506165\nstep6677, loss: 0.3903879225254059\nstep6678, loss: 0.34389424324035645\nstep6679, loss: 0.37018993496894836\nstep6680, loss: 0.3730176091194153\nstep6681, loss: 0.37382587790489197\nstep6682, loss: 0.3397681415081024\nstep6683, loss: 0.37983930110931396\nstep6684, loss: 0.41337957978248596\nstep6685, loss: 0.3915879726409912\nstep6686, loss: 0.3839196264743805\nstep6687, loss: 0.36494043469429016\nstep6688, loss: 0.39545977115631104\nstep6689, loss: 0.3623960018157959\nstep6690, loss: 0.4076903164386749\nstep6691, loss: 0.35528820753097534\nstep6692, loss: 0.3311612010002136\nstep6693, loss: 0.34887877106666565\nstep6694, loss: 0.3880789279937744\nstep6695, loss: 0.40145450830459595\nstep6696, loss: 0.40673381090164185\nstep6697, loss: 0.43021729588508606\nstep6698, loss: 0.3903263211250305\nstep6699, loss: 0.35924383997917175\nstep6700, loss: 0.3434146046638489\nstep6701, loss: 0.32435333728790283\nstep6702, loss: 0.3871830403804779\nstep6703, loss: 0.37753233313560486\nstep6704, loss: 0.3261636197566986\nstep6705, loss: 0.3144708573818207\nstep6706, loss: 0.39490020275115967\nstep6707, loss: 0.3561939299106598\nstep6708, loss: 0.3248733878135681\nstep6709, loss: 0.3363085389137268\nstep6710, loss: 0.3034213185310364\nstep6711, loss: 0.3308620750904083\nstep6712, loss: 0.3367432653903961\nstep6713, loss: 0.42208877205848694\nstep6714, loss: 0.417483389377594\nstep6715, loss: 0.4234165549278259\nstep6716, loss: 0.36365368962287903\nstep6717, loss: 0.35772842168807983\nstep6718, loss: 0.34362003207206726\nstep6719, loss: 0.34176769852638245\nstep6720, loss: 0.36516016721725464\nstep6721, loss: 0.3249266445636749\nstep6722, loss: 0.4028722941875458\nstep6723, loss: 0.3583148121833801\nstep6724, loss: 0.4455660283565521\nstep6725, loss: 0.39980548620224\nstep6726, loss: 0.38900026679039\nstep6727, loss: 0.39296719431877136\nstep6728, loss: 0.3689213991165161\nstep6729, loss: 0.3258000612258911\nstep6730, loss: 0.3560416102409363\nstep6731, loss: 0.3390660583972931\nstep6732, loss: 0.35265663266181946\nstep6733, loss: 0.36419469118118286\nstep6734, loss: 0.36040598154067993\nstep6735, loss: 0.38792186975479126\nstep6736, loss: 0.39067479968070984\nstep6737, loss: 0.3754056990146637\nstep6738, loss: 0.34415507316589355\nstep6739, loss: 0.4041211009025574\nstep6740, loss: 0.3522610664367676\nstep6741, loss: 0.38957011699676514\nstep6742, loss: 0.3657338321208954\nstep6743, loss: 0.3604527413845062\nstep6744, loss: 0.3411392867565155\nstep6745, loss: 0.31848210096359253\nstep6746, loss: 0.3059082627296448\nstep6747, loss: 0.33699700236320496\nstep6748, loss: 0.3504118323326111\nstep6749, loss: 0.35180866718292236\nstep6750, loss: 0.3376774191856384\nstep6751, loss: 0.33966678380966187\nstep6752, loss: 0.32749003171920776\nstep6753, loss: 0.3326159119606018\nstep6754, loss: 0.3189069926738739\nstep6755, loss: 0.3579549193382263\nstep6756, loss: 0.3289938271045685\nstep6757, loss: 0.33677923679351807\nstep6758, loss: 0.3331061601638794\nstep6759, loss: 0.34179842472076416\nstep6760, loss: 0.31392335891723633\nstep6761, loss: 0.3213518559932709\nstep6762, loss: 0.3464151620864868\nstep6763, loss: 0.33922213315963745\nstep6764, loss: 0.2976684272289276\nstep6765, loss: 0.3487097918987274\nstep6766, loss: 0.34208178520202637\nstep6767, loss: 0.33199456334114075\nstep6768, loss: 0.34923839569091797\nstep6769, loss: 0.34687089920043945\nstep6770, loss: 0.34977835416793823\nstep6771, loss: 0.32518988847732544\nstep6772, loss: 0.34820982813835144\nstep6773, loss: 0.316771000623703\nstep6774, loss: 0.2844217121601105\nstep6775, loss: 0.30841198563575745\nstep6776, loss: 0.3255856931209564\nstep6777, loss: 0.3278275430202484\nstep6778, loss: 0.34030255675315857\nstep6779, loss: 0.35665223002433777\nstep6780, loss: 0.35439231991767883\nstep6781, loss: 0.31254565715789795\nstep6782, loss: 0.30767983198165894\nstep6783, loss: 0.28910133242607117\nstep6784, loss: 0.32216736674308777\nstep6785, loss: 0.34279540181159973\nstep6786, loss: 0.29788950085639954\nstep6787, loss: 0.2935980558395386\nstep6788, loss: 0.3600772023200989\nstep6789, loss: 0.30399182438850403\nstep6790, loss: 0.2687259018421173\nstep6791, loss: 0.27958711981773376\nstep6792, loss: 0.2786806523799896\nstep6793, loss: 0.296394020318985\nstep6794, loss: 0.2949032783508301\nstep6795, loss: 0.3595293164253235\nstep6796, loss: 0.35168927907943726\nstep6797, loss: 0.34594783186912537\nstep6798, loss: 0.31430140137672424\nstep6799, loss: 0.3103382885456085\nstep6800, loss: 0.304198682308197\nstep6801, loss: 0.3155777156352997\nstep6802, loss: 0.31668543815612793\nstep6803, loss: 0.29258617758750916\nstep6804, loss: 0.34383293986320496\nstep6805, loss: 0.3302968144416809\nstep6806, loss: 0.3752708435058594\nstep6807, loss: 0.348653107881546\nstep6808, loss: 0.3450057804584503\nstep6809, loss: 0.336434543132782\nstep6810, loss: 0.32759228348731995\nstep6811, loss: 0.31604117155075073\nstep6812, loss: 0.29336312413215637\nstep6813, loss: 0.3126503825187683\nstep6814, loss: 0.2992928922176361\nstep6815, loss: 0.3535313308238983\nstep6816, loss: 0.3354502320289612\nstep6817, loss: 0.3683965504169464\nstep6818, loss: 0.3528718650341034\nstep6819, loss: 0.36598703265190125\nstep6820, loss: 0.3404596745967865\nstep6821, loss: 0.3539952337741852\nstep6822, loss: 0.3387012481689453\nstep6823, loss: 0.3231936991214752\nstep6824, loss: 0.3128715753555298\nstep6825, loss: 0.3148579001426697\nstep6826, loss: 0.3068576753139496\nstep6827, loss: 0.32597532868385315\nstep6828, loss: 0.31197819113731384\nstep6829, loss: 0.31759709119796753\nstep6830, loss: 0.3371349275112152\nstep6831, loss: 0.32607218623161316\nstep6832, loss: 0.32425686717033386\nstep6833, loss: 0.3278641998767853\nstep6834, loss: 0.27629896998405457\nstep6835, loss: 0.2781009078025818\nstep6836, loss: 0.2756657302379608\nstep6837, loss: 0.3021748661994934\nstep6838, loss: 0.30304911732673645\nstep6839, loss: 0.3180370032787323\nstep6840, loss: 0.32189035415649414\nstep6841, loss: 0.3083471953868866\nstep6842, loss: 0.29458892345428467\nstep6843, loss: 0.28787052631378174\nstep6844, loss: 0.3161473572254181\nstep6845, loss: 0.2975150942802429\nstep6846, loss: 0.280927836894989\nstep6847, loss: 0.2939232885837555\nstep6848, loss: 0.32880547642707825\nstep6849, loss: 0.30706194043159485\nstep6850, loss: 0.28863289952278137\nstep6851, loss: 0.3120579421520233\nstep6852, loss: 0.2962261736392975\nstep6853, loss: 0.30084583163261414\nstep6854, loss: 0.31896933913230896\nstep6855, loss: 0.2837718427181244\nstep6856, loss: 0.26932090520858765\nstep6857, loss: 0.27835527062416077\nstep6858, loss: 0.29508140683174133\nstep6859, loss: 0.28977495431900024\nstep6860, loss: 0.29213425517082214\nstep6861, loss: 0.2888792157173157\nstep6862, loss: 0.29529839754104614\nstep6863, loss: 0.2671712338924408\nstep6864, loss: 0.2753211259841919\nstep6865, loss: 0.2676183879375458\nstep6866, loss: 0.27860620617866516\nstep6867, loss: 0.2819259464740753\nstep6868, loss: 0.2831900715827942\nstep6869, loss: 0.2591352164745331\nstep6870, loss: 0.3064329922199249\nstep6871, loss: 0.2618202865123749\nstep6872, loss: 0.23775078356266022\nstep6873, loss: 0.24167685210704803\nstep6874, loss: 0.23485137522220612\nstep6875, loss: 0.2475149929523468\nstep6876, loss: 0.2536153197288513\nstep6877, loss: 0.29811742901802063\nstep6878, loss: 0.2890731394290924\nstep6879, loss: 0.27855366468429565\nstep6880, loss: 0.24151858687400818\nstep6881, loss: 0.24695834517478943\nstep6882, loss: 0.24619758129119873\nstep6883, loss: 0.28002774715423584\nstep6884, loss: 0.25348034501075745\nstep6885, loss: 0.2515828311443329\nstep6886, loss: 0.32920148968696594\nstep6887, loss: 0.3076295256614685\nstep6888, loss: 0.3446437418460846\nstep6889, loss: 0.332083135843277\nstep6890, loss: 0.32681846618652344\nstep6891, loss: 0.30374959111213684\nstep6892, loss: 0.315971702337265\nstep6893, loss: 0.265951544046402\nstep6894, loss: 0.2695409655570984\nstep6895, loss: 0.2814983129501343\nstep6896, loss: 0.2887960374355316\nstep6897, loss: 0.3153008222579956\nstep6898, loss: 0.3118363916873932\nstep6899, loss: 0.34889867901802063\nstep6900, loss: 0.36654219031333923\nstep6901, loss: 0.3583914041519165\nstep6902, loss: 0.29214608669281006\nstep6903, loss: 0.3490864336490631\nstep6904, loss: 0.3153761327266693\nstep6905, loss: 0.28345081210136414\nstep6906, loss: 0.27770107984542847\nstep6907, loss: 0.2734385132789612\nstep6908, loss: 0.28538861870765686\nstep6909, loss: 0.28588616847991943\nstep6910, loss: 0.28697502613067627\nstep6911, loss: 0.31642505526542664\nstep6912, loss: 0.313678503036499\nstep6913, loss: 0.3086816668510437\nstep6914, loss: 0.3363535702228546\nstep6915, loss: 0.3313275873661041\nstep6916, loss: 0.2522306740283966\nstep6917, loss: 0.2875524163246155\nstep6918, loss: 0.2609778642654419\nstep6919, loss: 0.30679646134376526\nstep6920, loss: 0.2767173647880554\nstep6921, loss: 0.29962292313575745\nstep6922, loss: 0.29565927386283875\nstep6923, loss: 0.2922946512699127\nstep6924, loss: 0.26767152547836304\nstep6925, loss: 0.2694484293460846\nstep6926, loss: 0.30253493785858154\nstep6927, loss: 0.27944278717041016\nstep6928, loss: 0.26601332426071167\nstep6929, loss: 0.29935312271118164\nstep6930, loss: 0.3404780328273773\nstep6931, loss: 0.3139728605747223\nstep6932, loss: 0.3164275884628296\nstep6933, loss: 0.30088984966278076\nstep6934, loss: 0.3020980954170227\nstep6935, loss: 0.30295389890670776\nstep6936, loss: 0.29857662320137024\nstep6937, loss: 0.2731110453605652\nstep6938, loss: 0.24279741942882538\nstep6939, loss: 0.26500529050827026\nstep6940, loss: 0.25602495670318604\nstep6941, loss: 0.27018287777900696\nstep6942, loss: 0.28185975551605225\nstep6943, loss: 0.26669999957084656\nstep6944, loss: 0.26729434728622437\nstep6945, loss: 0.23850853741168976\nstep6946, loss: 0.24163606762886047\nstep6947, loss: 0.231516033411026\nstep6948, loss: 0.24446246027946472\nstep6949, loss: 0.25539302825927734\nstep6950, loss: 0.2343788743019104\nstep6951, loss: 0.2471664547920227\nstep6952, loss: 0.29801493883132935\nstep6953, loss: 0.2639424204826355\nstep6954, loss: 0.21432870626449585\nstep6955, loss: 0.2111002504825592\nstep6956, loss: 0.20230130851268768\nstep6957, loss: 0.23340100049972534\nstep6958, loss: 0.24310480058193207\nstep6959, loss: 0.27914756536483765\nstep6960, loss: 0.2563568353652954\nstep6961, loss: 0.24150744080543518\nstep6962, loss: 0.21160997450351715\nstep6963, loss: 0.21097341179847717\nstep6964, loss: 0.20900225639343262\nstep6965, loss: 0.23261195421218872\nstep6966, loss: 0.2140296846628189\nstep6967, loss: 0.20700736343860626\nstep6968, loss: 0.27055689692497253\nstep6969, loss: 0.27479737997055054\nstep6970, loss: 0.3093273341655731\nstep6971, loss: 0.28647512197494507\nstep6972, loss: 0.298834890127182\nstep6973, loss: 0.27135324478149414\nstep6974, loss: 0.29361510276794434\nstep6975, loss: 0.2566554844379425\nstep6976, loss: 0.25951117277145386\nstep6977, loss: 0.26061272621154785\nstep6978, loss: 0.2493826150894165\nstep6979, loss: 0.27193203568458557\nstep6980, loss: 0.2700718641281128\nstep6981, loss: 0.30814671516418457\nstep6982, loss: 0.3254082202911377\nstep6983, loss: 0.32117292284965515\nstep6984, loss: 0.29891160130500793\nstep6985, loss: 0.3560405671596527\nstep6986, loss: 0.30146336555480957\nstep6987, loss: 0.2956327497959137\nstep6988, loss: 0.29256680607795715\nstep6989, loss: 0.2729629874229431\nstep6990, loss: 0.27202871441841125\nstep6991, loss: 0.25607576966285706\nstep6992, loss: 0.27179357409477234\nstep6993, loss: 0.2592719793319702\nstep6994, loss: 0.28724205493927\nstep6995, loss: 0.2744695246219635\nstep6996, loss: 0.3007228672504425\nstep6997, loss: 0.2730097472667694\nstep6998, loss: 0.24505364894866943\nstep6999, loss: 0.25614309310913086\nstep7000, loss: 0.25071054697036743\nstep7001, loss: 0.285141259431839\nstep7002, loss: 0.3065861463546753\nstep7003, loss: 0.29246756434440613\nstep7004, loss: 0.3075772225856781\nstep7005, loss: 0.2950570285320282\nstep7006, loss: 0.2635888457298279\nstep7007, loss: 0.267259418964386\nstep7008, loss: 0.2736877202987671\nstep7009, loss: 0.25724703073501587\nstep7010, loss: 0.2601255774497986\nstep7011, loss: 0.2798728048801422\nstep7012, loss: 0.31872889399528503\nstep7013, loss: 0.29541119933128357\nstep7014, loss: 0.2878322899341583\nstep7015, loss: 0.27556002140045166\nstep7016, loss: 0.2933483123779297\nstep7017, loss: 0.3005513846874237\nstep7018, loss: 0.29314368963241577\nstep7019, loss: 0.28360214829444885\nstep7020, loss: 0.2532476484775543\nstep7021, loss: 0.25731122493743896\nstep7022, loss: 0.25291338562965393\nstep7023, loss: 0.24753864109516144\nstep7024, loss: 0.2464640736579895\nstep7025, loss: 0.25241711735725403\nstep7026, loss: 0.24804139137268066\nstep7027, loss: 0.23158037662506104\nstep7028, loss: 0.24424785375595093\nstep7029, loss: 0.23173317313194275\nstep7030, loss: 0.2543083727359772\nstep7031, loss: 0.24265426397323608\nstep7032, loss: 0.22849661111831665\nstep7033, loss: 0.24993257224559784\nstep7034, loss: 0.27696692943573\nstep7035, loss: 0.23812559247016907\nstep7036, loss: 0.20246590673923492\nstep7037, loss: 0.23142875730991364\nstep7038, loss: 0.20135542750358582\nstep7039, loss: 0.21809552609920502\nstep7040, loss: 0.20891369879245758\nstep7041, loss: 0.23798994719982147\nstep7042, loss: 0.2381274402141571\nstep7043, loss: 0.2284080684185028\nstep7044, loss: 0.20150452852249146\nstep7045, loss: 0.2158322036266327\nstep7046, loss: 0.19980281591415405\nstep7047, loss: 0.19475717842578888\nstep7048, loss: 0.19633843004703522\nstep7049, loss: 0.18990924954414368\nstep7050, loss: 0.25088533759117126\nstep7051, loss: 0.22344297170639038\nstep7052, loss: 0.2709804177284241\nstep7053, loss: 0.24003545939922333\nstep7054, loss: 0.23509736359119415\nstep7055, loss: 0.22180545330047607\nstep7056, loss: 0.23440352082252502\nstep7057, loss: 0.22311079502105713\nstep7058, loss: 0.231271430850029\nstep7059, loss: 0.22431766986846924\nstep7060, loss: 0.22780519723892212\nstep7061, loss: 0.24443143606185913\nstep7062, loss: 0.24411538243293762\nstep7063, loss: 0.28828004002571106\nstep7064, loss: 0.29738613963127136\nstep7065, loss: 0.2767735719680786\nstep7066, loss: 0.24752190709114075\nstep7067, loss: 0.29275912046432495\nstep7068, loss: 0.2738516330718994\nstep7069, loss: 0.2795117199420929\nstep7070, loss: 0.2985151708126068\nstep7071, loss: 0.282710999250412\nstep7072, loss: 0.26759809255599976\nstep7073, loss: 0.24374845623970032\nstep7074, loss: 0.24992622435092926\nstep7075, loss: 0.26104700565338135\nstep7076, loss: 0.2697267234325409\nstep7077, loss: 0.2731233239173889\nstep7078, loss: 0.26532164216041565\nstep7079, loss: 0.26844069361686707\nstep7080, loss: 0.25460684299468994\nstep7081, loss: 0.26706135272979736\nstep7082, loss: 0.23874452710151672\nstep7083, loss: 0.25786101818084717\nstep7084, loss: 0.2872139811515808\nstep7085, loss: 0.2986339330673218\nstep7086, loss: 0.30761492252349854\nstep7087, loss: 0.2898891568183899\nstep7088, loss: 0.26287612318992615\nstep7089, loss: 0.26288697123527527\nstep7090, loss: 0.2768995761871338\nstep7091, loss: 0.26256829500198364\nstep7092, loss: 0.2601932883262634\nstep7093, loss: 0.29651159048080444\nstep7094, loss: 0.316839337348938\nstep7095, loss: 0.2975206673145294\nstep7096, loss: 0.30229100584983826\nstep7097, loss: 0.26001617312431335\nstep7098, loss: 0.2862909734249115\nstep7099, loss: 0.30536291003227234\nstep7100, loss: 0.26854807138442993\nstep7101, loss: 0.2546570301055908\nstep7102, loss: 0.23735494911670685\nstep7103, loss: 0.2670230567455292\nstep7104, loss: 0.3011705279350281\nstep7105, loss: 0.28435084223747253\nstep7106, loss: 0.2916097939014435\nstep7107, loss: 0.2762978672981262\nstep7108, loss: 0.24163611233234406\nstep7109, loss: 0.217027485370636\nstep7110, loss: 0.23409432172775269\nstep7111, loss: 0.2085636854171753\nstep7112, loss: 0.25524812936782837\nstep7113, loss: 0.2245294153690338\nstep7114, loss: 0.2502099871635437\nstep7115, loss: 0.24863176047801971\nstep7116, loss: 0.2847679853439331\nstep7117, loss: 0.24607188999652863\nstep7118, loss: 0.22610566020011902\nstep7119, loss: 0.22104965150356293\nstep7120, loss: 0.21210692822933197\nstep7121, loss: 0.20496658980846405\nstep7122, loss: 0.21591797471046448\nstep7123, loss: 0.23964934051036835\nstep7124, loss: 0.2640002965927124\nstep7125, loss: 0.2486310750246048\nstep7126, loss: 0.20457586646080017\nstep7127, loss: 0.19797153770923615\nstep7128, loss: 0.19583149254322052\nstep7129, loss: 0.19549253582954407\nstep7130, loss: 0.21508289873600006\nstep7131, loss: 0.18737435340881348\nstep7132, loss: 0.238261878490448\nstep7133, loss: 0.22526390850543976\nstep7134, loss: 0.2604137659072876\nstep7135, loss: 0.23048987984657288\nstep7136, loss: 0.22645102441310883\nstep7137, loss: 0.22246849536895752\nstep7138, loss: 0.22010929882526398\nstep7139, loss: 0.18549960851669312\nstep7140, loss: 0.20405830442905426\nstep7141, loss: 0.20067067444324493\nstep7142, loss: 0.19095933437347412\nstep7143, loss: 0.22844699025154114\nstep7144, loss: 0.21984416246414185\nstep7145, loss: 0.2704722285270691\nstep7146, loss: 0.2876761257648468\nstep7147, loss: 0.28759846091270447\nstep7148, loss: 0.2619057297706604\nstep7149, loss: 0.2650867700576782\nstep7150, loss: 0.24989454448223114\nstep7151, loss: 0.2434268444776535\nstep7152, loss: 0.24376262724399567\nstep7153, loss: 0.21695846319198608\nstep7154, loss: 0.2549828886985779\nstep7155, loss: 0.22553589940071106\nstep7156, loss: 0.23070935904979706\nstep7157, loss: 0.26854732632637024\nstep7158, loss: 0.2967321574687958\nstep7159, loss: 0.2750661075115204\nstep7160, loss: 0.28686657547950745\nstep7161, loss: 0.2542862892150879\nstep7162, loss: 0.24660587310791016\nstep7163, loss: 0.2262679934501648\nstep7164, loss: 0.22503560781478882\nstep7165, loss: 0.2575373649597168\nstep7166, loss: 0.27535271644592285\nstep7167, loss: 0.271695613861084\nstep7168, loss: 0.28443223237991333\nstep7169, loss: 0.2987962067127228\nstep7170, loss: 0.24655812978744507\nstep7171, loss: 0.2690695524215698\nstep7172, loss: 0.30002743005752563\nstep7173, loss: 0.24770815670490265\nstep7174, loss: 0.25008389353752136\nstep7175, loss: 0.2938103973865509\nstep7176, loss: 0.3317612111568451\nstep7177, loss: 0.31611567735671997\nstep7178, loss: 0.3238687515258789\nstep7179, loss: 0.29557564854621887\nstep7180, loss: 0.31271597743034363\nstep7181, loss: 0.2933690845966339\nstep7182, loss: 0.28075045347213745\nstep7183, loss: 0.26169028878211975\nstep7184, loss: 0.252174973487854\nstep7185, loss: 0.27955716848373413\nstep7186, loss: 0.31354665756225586\nstep7187, loss: 0.2907593250274658\nstep7188, loss: 0.32705530524253845\nstep7189, loss: 0.2945438325405121\nstep7190, loss: 0.278827041387558\nstep7191, loss: 0.237000972032547\nstep7192, loss: 0.26887455582618713\nstep7193, loss: 0.22138184309005737\nstep7194, loss: 0.2678943872451782\nstep7195, loss: 0.22495873272418976\nstep7196, loss: 0.23440079391002655\nstep7197, loss: 0.23168179392814636\nstep7198, loss: 0.2608817517757416\nstep7199, loss: 0.2605799734592438\nstep7200, loss: 0.23337692022323608\nstep7201, loss: 0.21051421761512756\nstep7202, loss: 0.21330899000167847\nstep7203, loss: 0.2201693058013916\nstep7204, loss: 0.22740033268928528\nstep7205, loss: 0.24228307604789734\nstep7206, loss: 0.2581492066383362\nstep7207, loss: 0.24881532788276672\nstep7208, loss: 0.21211855113506317\nstep7209, loss: 0.1883559376001358\nstep7210, loss: 0.18357086181640625\nstep7211, loss: 0.19789370894432068\nstep7212, loss: 0.1992122381925583\nstep7213, loss: 0.17198525369167328\nstep7214, loss: 0.22016935050487518\nstep7215, loss: 0.23143476247787476\nstep7216, loss: 0.25333622097969055\nstep7217, loss: 0.25302863121032715\nstep7218, loss: 0.24005192518234253\nstep7219, loss: 0.23680691421031952\nstep7220, loss: 0.21964526176452637\nstep7221, loss: 0.17066006362438202\nstep7222, loss: 0.19595064222812653\nstep7223, loss: 0.1996077001094818\nstep7224, loss: 0.19124574959278107\nstep7225, loss: 0.20629407465457916\nstep7226, loss: 0.21237872540950775\nstep7227, loss: 0.23686647415161133\nstep7228, loss: 0.22015520930290222\nstep7229, loss: 0.23911283910274506\nstep7230, loss: 0.22060009837150574\nstep7231, loss: 0.25055885314941406\nstep7232, loss: 0.2299967259168625\nstep7233, loss: 0.22529537975788116\nstep7234, loss: 0.2468123733997345\nstep7235, loss: 0.23206248879432678\nstep7236, loss: 0.2215619832277298\nstep7237, loss: 0.19338633120059967\nstep7238, loss: 0.2256733924150467\nstep7239, loss: 0.21876145899295807\nstep7240, loss: 0.2311684787273407\nstep7241, loss: 0.2384774386882782\nstep7242, loss: 0.22662261128425598\nstep7243, loss: 0.23088638484477997\nstep7244, loss: 0.2227150797843933\nstep7245, loss: 0.22059209644794464\nstep7246, loss: 0.21774424612522125\nstep7247, loss: 0.2364254891872406\nstep7248, loss: 0.2343973070383072\nstep7249, loss: 0.2467840611934662\nstep7250, loss: 0.24245135486125946\nstep7251, loss: 0.268056184053421\nstep7252, loss: 0.22710755467414856\nstep7253, loss: 0.22608494758605957\nstep7254, loss: 0.2525387406349182\nstep7255, loss: 0.24488729238510132\nstep7256, loss: 0.21990922093391418\nstep7257, loss: 0.291242778301239\nstep7258, loss: 0.33301055431365967\nstep7259, loss: 0.335027813911438\nstep7260, loss: 0.35021916031837463\nstep7261, loss: 0.33295270800590515\nstep7262, loss: 0.28013288974761963\nstep7263, loss: 0.2635926604270935\nstep7264, loss: 0.270164430141449\nstep7265, loss: 0.258300244808197\nstep7266, loss: 0.2518428862094879\nstep7267, loss: 0.2748831808567047\nstep7268, loss: 0.3033044934272766\nstep7269, loss: 0.2990778088569641\nstep7270, loss: 0.3531161844730377\nstep7271, loss: 0.32742950320243835\nstep7272, loss: 0.2741917669773102\nstep7273, loss: 0.26753509044647217\nstep7274, loss: 0.23966428637504578\nstep7275, loss: 0.22374121844768524\nstep7276, loss: 0.27967914938926697\nstep7277, loss: 0.27607259154319763\nstep7278, loss: 0.2310100495815277\nstep7279, loss: 0.284117728471756\nstep7280, loss: 0.30591246485710144\nstep7281, loss: 0.2849491536617279\nstep7282, loss: 0.25088053941726685\nstep7283, loss: 0.2142939567565918\nstep7284, loss: 0.24088266491889954\nstep7285, loss: 0.2148752361536026\nstep7286, loss: 0.2432909607887268\nstep7287, loss: 0.27878525853157043\nstep7288, loss: 0.2890932559967041\nstep7289, loss: 0.2530900537967682\nstep7290, loss: 0.2175954431295395\nstep7291, loss: 0.2096066027879715\nstep7292, loss: 0.19731463491916656\nstep7293, loss: 0.20447763800621033\nstep7294, loss: 0.2180178463459015\nstep7295, loss: 0.19246022403240204\nstep7296, loss: 0.2389337122440338\nstep7297, loss: 0.22218477725982666\nstep7298, loss: 0.26113852858543396\nstep7299, loss: 0.23921222984790802\nstep7300, loss: 0.24498790502548218\nstep7301, loss: 0.21996982395648956\nstep7302, loss: 0.2151145040988922\nstep7303, loss: 0.19370974600315094\nstep7304, loss: 0.22996321320533752\nstep7305, loss: 0.1917915940284729\nstep7306, loss: 0.18829597532749176\nstep7307, loss: 0.19872444868087769\nstep7308, loss: 0.19181612133979797\nstep7309, loss: 0.24529415369033813\nstep7310, loss: 0.2349492460489273\nstep7311, loss: 0.2449088841676712\nstep7312, loss: 0.2055637389421463\nstep7313, loss: 0.2344658523797989\nstep7314, loss: 0.22557222843170166\nstep7315, loss: 0.22034111618995667\nstep7316, loss: 0.2097376435995102\nstep7317, loss: 0.18584129214286804\nstep7318, loss: 0.19095028936862946\nstep7319, loss: 0.1809104084968567\nstep7320, loss: 0.21794742345809937\nstep7321, loss: 0.223502978682518\nstep7322, loss: 0.2200830578804016\nstep7323, loss: 0.22267575562000275\nstep7324, loss: 0.23228836059570312\nstep7325, loss: 0.21617946028709412\nstep7326, loss: 0.19869664311408997\nstep7327, loss: 0.1840498447418213\nstep7328, loss: 0.18710190057754517\nstep7329, loss: 0.21720561385154724\nstep7330, loss: 0.22443048655986786\nstep7331, loss: 0.20959612727165222\nstep7332, loss: 0.21895459294319153\nstep7333, loss: 0.25603681802749634\nstep7334, loss: 0.2472701072692871\nstep7335, loss: 0.21311019361019135\nstep7336, loss: 0.23995669186115265\nstep7337, loss: 0.24622079730033875\nstep7338, loss: 0.21462512016296387\nstep7339, loss: 0.22453877329826355\nstep7340, loss: 0.24653306603431702\nstep7341, loss: 0.2562767565250397\nstep7342, loss: 0.2417442351579666\nstep7343, loss: 0.2598496973514557\nstep7344, loss: 0.2727125883102417\nstep7345, loss: 0.29091840982437134\nstep7346, loss: 0.28716251254081726\nstep7347, loss: 0.2647457420825958\nstep7348, loss: 0.2579513192176819\nstep7349, loss: 0.2574622631072998\nstep7350, loss: 0.34200626611709595\nstep7351, loss: 0.3223351836204529\nstep7352, loss: 0.3088882565498352\nstep7353, loss: 0.2780240774154663\nstep7354, loss: 0.25895190238952637\nstep7355, loss: 0.26494646072387695\nstep7356, loss: 0.2734314799308777\nstep7357, loss: 0.25531336665153503\nstep7358, loss: 0.30525505542755127\nstep7359, loss: 0.31929823756217957\nstep7360, loss: 0.3099853992462158\nstep7361, loss: 0.2632133662700653\nstep7362, loss: 0.3016483783721924\nstep7363, loss: 0.25315549969673157\nstep7364, loss: 0.24460211396217346\nstep7365, loss: 0.23605401813983917\nstep7366, loss: 0.2431558072566986\nstep7367, loss: 0.244918555021286\nstep7368, loss: 0.2258196920156479\nstep7369, loss: 0.2681870460510254\nstep7370, loss: 0.27599433064460754\nstep7371, loss: 0.2805725634098053\nstep7372, loss: 0.2177080363035202\nstep7373, loss: 0.21525125205516815\nstep7374, loss: 0.22546352446079254\nstep7375, loss: 0.20785483717918396\nstep7376, loss: 0.2200983762741089\nstep7377, loss: 0.2108166664838791\nstep7378, loss: 0.2394077181816101\nstep7379, loss: 0.22384759783744812\nstep7380, loss: 0.25316593050956726\nstep7381, loss: 0.2277364581823349\nstep7382, loss: 0.229649618268013\nstep7383, loss: 0.2099728286266327\nstep7384, loss: 0.23254242539405823\nstep7385, loss: 0.19675704836845398\nstep7386, loss: 0.2128830999135971\nstep7387, loss: 0.19302961230278015\nstep7388, loss: 0.19924889504909515\nstep7389, loss: 0.1972169280052185\nstep7390, loss: 0.21082909405231476\nstep7391, loss: 0.25177982449531555\nstep7392, loss: 0.21909070014953613\nstep7393, loss: 0.23604348301887512\nstep7394, loss: 0.19395874440670013\nstep7395, loss: 0.23365595936775208\nstep7396, loss: 0.20298628509044647\nstep7397, loss: 0.19873838126659393\nstep7398, loss: 0.19558358192443848\nstep7399, loss: 0.19574177265167236\nstep7400, loss: 0.19481945037841797\nstep7401, loss: 0.18992896378040314\nstep7402, loss: 0.1964942067861557\nstep7403, loss: 0.20571313798427582\nstep7404, loss: 0.2220524102449417\nstep7405, loss: 0.21566468477249146\nstep7406, loss: 0.20089620351791382\nstep7407, loss: 0.21285289525985718\nstep7408, loss: 0.19320109486579895\nstep7409, loss: 0.18693555891513824\nstep7410, loss: 0.17820756137371063\nstep7411, loss: 0.2003423124551773\nstep7412, loss: 0.19115541875362396\nstep7413, loss: 0.18410123884677887\nstep7414, loss: 0.1941465437412262\nstep7415, loss: 0.2123555839061737\nstep7416, loss: 0.20889529585838318\nstep7417, loss: 0.18337862193584442\nstep7418, loss: 0.19615426659584045\nstep7419, loss: 0.20966945588588715\nstep7420, loss: 0.1970548778772354\nstep7421, loss: 0.21311578154563904\nstep7422, loss: 0.2428749054670334\nstep7423, loss: 0.24997150897979736\nstep7424, loss: 0.2605692446231842\nstep7425, loss: 0.2219569832086563\nstep7426, loss: 0.2273498773574829\nstep7427, loss: 0.22930915653705597\nstep7428, loss: 0.2149789184331894\nstep7429, loss: 0.19610241055488586\nstep7430, loss: 0.20144835114479065\nstep7431, loss: 0.2079898715019226\nstep7432, loss: 0.2647799849510193\nstep7433, loss: 0.29023805260658264\nstep7434, loss: 0.3142898380756378\nstep7435, loss: 0.2952897846698761\nstep7436, loss: 0.2851150929927826\nstep7437, loss: 0.24804069101810455\nstep7438, loss: 0.2597617506980896\nstep7439, loss: 0.2306472659111023\nstep7440, loss: 0.270801842212677\nstep7441, loss: 0.26739421486854553\nstep7442, loss: 0.2791547179222107\nstep7443, loss: 0.24491780996322632\nstep7444, loss: 0.3182089924812317\nstep7445, loss: 0.2754010558128357\nstep7446, loss: 0.28953817486763\nstep7447, loss: 0.23847195506095886\nstep7448, loss: 0.2561909258365631\nstep7449, loss: 0.24617119133472443\nstep7450, loss: 0.2391478717327118\nstep7451, loss: 0.2846762537956238\nstep7452, loss: 0.3260299861431122\nstep7453, loss: 0.2698427736759186\nstep7454, loss: 0.24589848518371582\nstep7455, loss: 0.21420961618423462\nstep7456, loss: 0.21491871774196625\nstep7457, loss: 0.22591811418533325\nstep7458, loss: 0.21402284502983093\nstep7459, loss: 0.19321119785308838\nstep7460, loss: 0.21257837116718292\nstep7461, loss: 0.20193423330783844\nstep7462, loss: 0.22994303703308105\nstep7463, loss: 0.20933124423027039\nstep7464, loss: 0.23832836747169495\nstep7465, loss: 0.22165371477603912\nstep7466, loss: 0.23290136456489563\nstep7467, loss: 0.20405778288841248\nstep7468, loss: 0.22412064671516418\nstep7469, loss: 0.19199131429195404\nstep7470, loss: 0.17158906161785126\nstep7471, loss: 0.20761685073375702\nstep7472, loss: 0.21051999926567078\nstep7473, loss: 0.23359443247318268\nstep7474, loss: 0.21505527198314667\nstep7475, loss: 0.2409735769033432\nstep7476, loss: 0.20348241925239563\nstep7477, loss: 0.2397732138633728\nstep7478, loss: 0.1914178431034088\nstep7479, loss: 0.20148736238479614\nstep7480, loss: 0.16997218132019043\nstep7481, loss: 0.18220123648643494\nstep7482, loss: 0.19462671875953674\nstep7483, loss: 0.18480351567268372\nstep7484, loss: 0.197775736451149\nstep7485, loss: 0.20400848984718323\nstep7486, loss: 0.22437593340873718\nstep7487, loss: 0.22102577984333038\nstep7488, loss: 0.19616805016994476\nstep7489, loss: 0.19778932631015778\nstep7490, loss: 0.17332425713539124\nstep7491, loss: 0.1779548078775406\nstep7492, loss: 0.1748826652765274\nstep7493, loss: 0.18684612214565277\nstep7494, loss: 0.16869889199733734\nstep7495, loss: 0.19209985435009003\nstep7496, loss: 0.1949337124824524\nstep7497, loss: 0.18570563197135925\nstep7498, loss: 0.18945184350013733\nstep7499, loss: 0.1788228303194046\nstep7500, loss: 0.20294101536273956\nstep7501, loss: 0.19137537479400635\nstep7502, loss: 0.18282181024551392\nstep7503, loss: 0.1583668738603592\nstep7504, loss: 0.2042769342660904\nstep7505, loss: 0.19010286033153534\nstep7506, loss: 0.1892932802438736\nstep7507, loss: 0.19445918500423431\nstep7508, loss: 0.22223997116088867\nstep7509, loss: 0.20091430842876434\nstep7510, loss: 0.21141071617603302\nstep7511, loss: 0.20777469873428345\nstep7512, loss: 0.1801723688840866\nstep7513, loss: 0.17968276143074036\nstep7514, loss: 0.22398804128170013\nstep7515, loss: 0.19821302592754364\nstep7516, loss: 0.22024139761924744\nstep7517, loss: 0.20689252018928528\nstep7518, loss: 0.22242723405361176\nstep7519, loss: 0.20124970376491547\nstep7520, loss: 0.19587968289852142\nstep7521, loss: 0.21636147797107697\nstep7522, loss: 0.2712215781211853\nstep7523, loss: 0.25240927934646606\nstep7524, loss: 0.25877878069877625\nstep7525, loss: 0.2524210512638092\nstep7526, loss: 0.3003949522972107\nstep7527, loss: 0.23778796195983887\nstep7528, loss: 0.24699606001377106\nstep7529, loss: 0.23840798437595367\nstep7530, loss: 0.2000453770160675\nstep7531, loss: 0.20994654297828674\nstep7532, loss: 0.20000796020030975\nstep7533, loss: 0.2547115087509155\nstep7534, loss: 0.33399054408073425\nstep7535, loss: 0.3338378965854645\nstep7536, loss: 0.25064224004745483\nstep7537, loss: 0.2319820672273636\nstep7538, loss: 0.23571470379829407\nstep7539, loss: 0.21387746930122375\nstep7540, loss: 0.21398214995861053\nstep7541, loss: 0.19978821277618408\nstep7542, loss: 0.20001143217086792\nstep7543, loss: 0.19879315793514252\nstep7544, loss: 0.22911855578422546\nstep7545, loss: 0.21084393560886383\nstep7546, loss: 0.21709927916526794\nstep7547, loss: 0.2104523777961731\nstep7548, loss: 0.21654382348060608\nstep7549, loss: 0.17844140529632568\nstep7550, loss: 0.1952153593301773\nstep7551, loss: 0.1728399097919464\nstep7552, loss: 0.18214178085327148\nstep7553, loss: 0.20929798483848572\nstep7554, loss: 0.1962398737668991\nstep7555, loss: 0.23300176858901978\nstep7556, loss: 0.19918623566627502\nstep7557, loss: 0.22873671352863312\nstep7558, loss: 0.1860894113779068\nstep7559, loss: 0.2226002812385559\nstep7560, loss: 0.1948346495628357\nstep7561, loss: 0.20195363461971283\nstep7562, loss: 0.1763860583305359\nstep7563, loss: 0.17128287255764008\nstep7564, loss: 0.16747920215129852\nstep7565, loss: 0.16004493832588196\nstep7566, loss: 0.17774881422519684\nstep7567, loss: 0.18710766732692719\nstep7568, loss: 0.21312165260314941\nstep7569, loss: 0.2031666487455368\nstep7570, loss: 0.19770844280719757\nstep7571, loss: 0.19490481913089752\nstep7572, loss: 0.20175127685070038\nstep7573, loss: 0.19625964760780334\nstep7574, loss: 0.17856921255588531\nstep7575, loss: 0.1967749148607254\nstep7576, loss: 0.16840475797653198\nstep7577, loss: 0.18295231461524963\nstep7578, loss: 0.16669005155563354\nstep7579, loss: 0.1802770346403122\nstep7580, loss: 0.17601026594638824\nstep7581, loss: 0.17049482464790344\nstep7582, loss: 0.17629794776439667\nstep7583, loss: 0.17093123495578766\nstep7584, loss: 0.17098131775856018\nstep7585, loss: 0.18655213713645935\nstep7586, loss: 0.20266678929328918\nstep7587, loss: 0.17099720239639282\nstep7588, loss: 0.20394404232501984\nstep7589, loss: 0.18047013878822327\nstep7590, loss: 0.19236084818840027\nstep7591, loss: 0.15801042318344116\nstep7592, loss: 0.18215534090995789\nstep7593, loss: 0.160119891166687\nstep7594, loss: 0.15636643767356873\nstep7595, loss: 0.15799206495285034\nstep7596, loss: 0.21637089550495148\nstep7597, loss: 0.2134861946105957\nstep7598, loss: 0.18865250051021576\nstep7599, loss: 0.18794114887714386\nstep7600, loss: 0.20587316155433655\nstep7601, loss: 0.17871375381946564\nstep7602, loss: 0.175046905875206\nstep7603, loss: 0.18066243827342987\nstep7604, loss: 0.18654099106788635\nstep7605, loss: 0.17533859610557556\nstep7606, loss: 0.19166062772274017\nstep7607, loss: 0.2040477842092514\nstep7608, loss: 0.2590613067150116\nstep7609, loss: 0.21348096430301666\nstep7610, loss: 0.22437737882137299\nstep7611, loss: 0.2292395681142807\nstep7612, loss: 0.2027159184217453\nstep7613, loss: 0.222305566072464\nstep7614, loss: 0.18134301900863647\nstep7615, loss: 0.21547281742095947\nstep7616, loss: 0.26822564005851746\nstep7617, loss: 0.2530239522457123\nstep7618, loss: 0.20446032285690308\nstep7619, loss: 0.20204675197601318\nstep7620, loss: 0.17901581525802612\nstep7621, loss: 0.1945493221282959\nstep7622, loss: 0.21756449341773987\nstep7623, loss: 0.20144203305244446\nstep7624, loss: 0.2087411880493164\nstep7625, loss: 0.21365201473236084\nstep7626, loss: 0.2391548454761505\nstep7627, loss: 0.22682975232601166\nstep7628, loss: 0.19908329844474792\nstep7629, loss: 0.1828038990497589\nstep7630, loss: 0.19426104426383972\nstep7631, loss: 0.15901264548301697\nstep7632, loss: 0.1767716109752655\nstep7633, loss: 0.15126672387123108\nstep7634, loss: 0.1618523895740509\nstep7635, loss: 0.18314409255981445\nstep7636, loss: 0.1613037884235382\nstep7637, loss: 0.21564006805419922\nstep7638, loss: 0.17721711099147797\nstep7639, loss: 0.19262254238128662\nstep7640, loss: 0.17887386679649353\nstep7641, loss: 0.2196085900068283\nstep7642, loss: 0.205795019865036\nstep7643, loss: 0.15770694613456726\nstep7644, loss: 0.16706836223602295\nstep7645, loss: 0.1710784137248993\nstep7646, loss: 0.15894873440265656\nstep7647, loss: 0.13433071970939636\nstep7648, loss: 0.1516798436641693\nstep7649, loss: 0.1696019470691681\nstep7650, loss: 0.19042742252349854\nstep7651, loss: 0.17855772376060486\nstep7652, loss: 0.20826536417007446\nstep7653, loss: 0.1839430332183838\nstep7654, loss: 0.1647939532995224\nstep7655, loss: 0.18402978777885437\nstep7656, loss: 0.16901831328868866\nstep7657, loss: 0.1811937391757965\nstep7658, loss: 0.17091800272464752\nstep7659, loss: 0.18360598385334015\nstep7660, loss: 0.1775992214679718\nstep7661, loss: 0.20250096917152405\nstep7662, loss: 0.17728830873966217\nstep7663, loss: 0.16607673466205597\nstep7664, loss: 0.168357715010643\nstep7665, loss: 0.15651801228523254\nstep7666, loss: 0.15225853025913239\nstep7667, loss: 0.1553959846496582\nstep7668, loss: 0.1787157952785492\nstep7669, loss: 0.17094211280345917\nstep7670, loss: 0.1749275028705597\nstep7671, loss: 0.1786649525165558\nstep7672, loss: 0.16508126258850098\nstep7673, loss: 0.15336687862873077\nstep7674, loss: 0.17119963467121124\nstep7675, loss: 0.16508261859416962\nstep7676, loss: 0.14965812861919403\nstep7677, loss: 0.14369381964206696\nstep7678, loss: 0.15701614320278168\nstep7679, loss: 0.16937944293022156\nstep7680, loss: 0.17873822152614594\nstep7681, loss: 0.1606614589691162\nstep7682, loss: 0.16108007729053497\nstep7683, loss: 0.15746375918388367\nstep7684, loss: 0.16024717688560486\nstep7685, loss: 0.1400468796491623\nstep7686, loss: 0.15453267097473145\nstep7687, loss: 0.14554336667060852\nstep7688, loss: 0.15145477652549744\nstep7689, loss: 0.14649635553359985\nstep7690, loss: 0.19996888935565948\nstep7691, loss: 0.149685800075531\nstep7692, loss: 0.1639026701450348\nstep7693, loss: 0.16692785918712616\nstep7694, loss: 0.1389082372188568\nstep7695, loss: 0.17646096646785736\nstep7696, loss: 0.1373218446969986\nstep7697, loss: 0.1616542488336563\nstep7698, loss: 0.20689022541046143\nstep7699, loss: 0.21606019139289856\nstep7700, loss: 0.17306189239025116\nstep7701, loss: 0.1868220716714859\nstep7702, loss: 0.15470103919506073\nstep7703, loss: 0.15294797718524933\nstep7704, loss: 0.15491478145122528\nstep7705, loss: 0.13460886478424072\nstep7706, loss: 0.1712133288383484\nstep7707, loss: 0.16768960654735565\nstep7708, loss: 0.19485200941562653\nstep7709, loss: 0.1908828616142273\nstep7710, loss: 0.1936783343553543\nstep7711, loss: 0.16088716685771942\nstep7712, loss: 0.19203202426433563\nstep7713, loss: 0.16554278135299683\nstep7714, loss: 0.15893866121768951\nstep7715, loss: 0.15666106343269348\nstep7716, loss: 0.1499090939760208\nstep7717, loss: 0.1670839935541153\nstep7718, loss: 0.14583566784858704\nstep7719, loss: 0.18615612387657166\nstep7720, loss: 0.14335346221923828\nstep7721, loss: 0.1511603146791458\nstep7722, loss: 0.1422431766986847\nstep7723, loss: 0.17056341469287872\nstep7724, loss: 0.15892340242862701\nstep7725, loss: 0.1516803354024887\nstep7726, loss: 0.14497214555740356\nstep7727, loss: 0.14357246458530426\nstep7728, loss: 0.15470005571842194\nstep7729, loss: 0.12819159030914307\nstep7730, loss: 0.14640174806118011\nstep7731, loss: 0.15637269616127014\nstep7732, loss: 0.16759496927261353\nstep7733, loss: 0.15331649780273438\nstep7734, loss: 0.15412402153015137\nstep7735, loss: 0.15114034712314606\nstep7736, loss: 0.1498638093471527\nstep7737, loss: 0.1552363485097885\nstep7738, loss: 0.14245668053627014\nstep7739, loss: 0.14096032083034515\nstep7740, loss: 0.13094982504844666\nstep7741, loss: 0.14656849205493927\nstep7742, loss: 0.13259397447109222\nstep7743, loss: 0.14775821566581726\nstep7744, loss: 0.14436174929141998\nstep7745, loss: 0.1455468237400055\nstep7746, loss: 0.14220695197582245\nstep7747, loss: 0.15457050502300262\nstep7748, loss: 0.13159430027008057\nstep7749, loss: 0.13551093637943268\nstep7750, loss: 0.16955554485321045\nstep7751, loss: 0.14838628470897675\nstep7752, loss: 0.14463640749454498\nstep7753, loss: 0.16906148195266724\nstep7754, loss: 0.15972167253494263\nstep7755, loss: 0.1434357464313507\nstep7756, loss: 0.15294361114501953\nstep7757, loss: 0.1370527744293213\nstep7758, loss: 0.12164594978094101\nstep7759, loss: 0.12391573190689087\nstep7760, loss: 0.14862273633480072\nstep7761, loss: 0.16314154863357544\nstep7762, loss: 0.16105225682258606\nstep7763, loss: 0.17233799397945404\nstep7764, loss: 0.13741998374462128\nstep7765, loss: 0.14589284360408783\nstep7766, loss: 0.13755132257938385\nstep7767, loss: 0.11461757868528366\nstep7768, loss: 0.121172234416008\nstep7769, loss: 0.11827360093593597\nstep7770, loss: 0.12848182022571564\nstep7771, loss: 0.12561751902103424\nstep7772, loss: 0.15401318669319153\nstep7773, loss: 0.13109233975410461\nstep7774, loss: 0.1237950325012207\nstep7775, loss: 0.1327732503414154\nstep7776, loss: 0.11960116773843765\nstep7777, loss: 0.13140913844108582\nstep7778, loss: 0.0935630202293396\nstep7779, loss: 0.12215109169483185\nstep7780, loss: 0.18982240557670593\nstep7781, loss: 0.1877145767211914\nstep7782, loss: 0.13229496777057648\nstep7783, loss: 0.11445307731628418\nstep7784, loss: 0.1096579059958458\nstep7785, loss: 0.1164625808596611\nstep7786, loss: 0.12882855534553528\nstep7787, loss: 0.10726170986890793\nstep7788, loss: 0.13014262914657593\nstep7789, loss: 0.12521393597126007\nstep7790, loss: 0.1489611268043518\nstep7791, loss: 0.15964171290397644\nstep7792, loss: 0.15622355043888092\nstep7793, loss: 0.14601781964302063\nstep7794, loss: 0.14411373436450958\nstep7795, loss: 0.13217045366764069\nstep7796, loss: 0.12605582177639008\nstep7797, loss: 0.1359269917011261\nstep7798, loss: 0.12245307117700577\nstep7799, loss: 0.13647353649139404\nstep7800, loss: 0.1227702796459198\nstep7801, loss: 0.1459447145462036\nstep7802, loss: 0.12697537243366241\nstep7803, loss: 0.14702683687210083\nstep7804, loss: 0.14276891946792603\nstep7805, loss: 0.1562277227640152\nstep7806, loss: 0.1359388381242752\nstep7807, loss: 0.12702380120754242\nstep7808, loss: 0.11447400599718094\nstep7809, loss: 0.110664963722229\nstep7810, loss: 0.11555705964565277\nstep7811, loss: 0.1169213131070137\nstep7812, loss: 0.1206885278224945\nstep7813, loss: 0.14295628666877747\nstep7814, loss: 0.15231545269489288\nstep7815, loss: 0.15456417202949524\nstep7816, loss: 0.1589260995388031\nstep7817, loss: 0.16366204619407654\nstep7818, loss: 0.13883619010448456\nstep7819, loss: 0.14176586270332336\nstep7820, loss: 0.1226704940199852\nstep7821, loss: 0.1264297217130661\nstep7822, loss: 0.11388089507818222\nstep7823, loss: 0.12604007124900818\nstep7824, loss: 0.10295218229293823\nstep7825, loss: 0.1119961068034172\nstep7826, loss: 0.12396082282066345\nstep7827, loss: 0.11539620161056519\nstep7828, loss: 0.09796364605426788\nstep7829, loss: 0.12335588037967682\nstep7830, loss: 0.10782360285520554\nstep7831, loss: 0.1027139276266098\nstep7832, loss: 0.1340942531824112\nstep7833, loss: 0.12982258200645447\nstep7834, loss: 0.12363138794898987\nstep7835, loss: 0.12835711240768433\nstep7836, loss: 0.13880066573619843\nstep7837, loss: 0.1257646530866623\nstep7838, loss: 0.13775435090065002\nstep7839, loss: 0.11859834939241409\nstep7840, loss: 0.10437250882387161\nstep7841, loss: 0.10864865779876709\nstep7842, loss: 0.12795579433441162\nstep7843, loss: 0.13988903164863586\nstep7844, loss: 0.1259119212627411\nstep7845, loss: 0.11943769454956055\nstep7846, loss: 0.1127677634358406\nstep7847, loss: 0.10648093372583389\nstep7848, loss: 0.10979364812374115\nstep7849, loss: 0.10089991986751556\nstep7850, loss: 0.1107088252902031\nstep7851, loss: 0.11356984823942184\nstep7852, loss: 0.12354730814695358\nstep7853, loss: 0.10318365693092346\nstep7854, loss: 0.14546602964401245\nstep7855, loss: 0.11864367127418518\nstep7856, loss: 0.09989464282989502\nstep7857, loss: 0.10584912449121475\nstep7858, loss: 0.10233891755342484\nstep7859, loss: 0.10837223380804062\nstep7860, loss: 0.10231442749500275\nstep7861, loss: 0.11052422225475311\nstep7862, loss: 0.1343119889497757\nstep7863, loss: 0.12675794959068298\nstep7864, loss: 0.11803744733333588\nstep7865, loss: 0.09840934723615646\nstep7866, loss: 0.08901147544384003\nstep7867, loss: 0.08288529515266418\nstep7868, loss: 0.09362751990556717\nstep7869, loss: 0.08308601379394531\nstep7870, loss: 0.11177676916122437\nstep7871, loss: 0.09616878628730774\nstep7872, loss: 0.11367814987897873\nstep7873, loss: 0.14079415798187256\nstep7874, loss: 0.13848015666007996\nstep7875, loss: 0.12621203064918518\nstep7876, loss: 0.11547376215457916\nstep7877, loss: 0.10831999033689499\nstep7878, loss: 0.10438690334558487\nstep7879, loss: 0.104631207883358\nstep7880, loss: 0.09729912877082825\nstep7881, loss: 0.10753816366195679\nstep7882, loss: 0.0979139506816864\nstep7883, loss: 0.12767311930656433\nstep7884, loss: 0.10255121439695358\nstep7885, loss: 0.11779160052537918\nstep7886, loss: 0.09452535957098007\nstep7887, loss: 0.12690789997577667\nstep7888, loss: 0.10508967190980911\nstep7889, loss: 0.10148762911558151\nstep7890, loss: 0.10655362904071808\nstep7891, loss: 0.10130497813224792\nstep7892, loss: 0.09079684317111969\nstep7893, loss: 0.08999283611774445\nstep7894, loss: 0.09897979348897934\nstep7895, loss: 0.09672869741916656\nstep7896, loss: 0.12015589326620102\nstep7897, loss: 0.10410571843385696\nstep7898, loss: 0.10532505810260773\nstep7899, loss: 0.11753629148006439\nstep7900, loss: 0.10315138101577759\nstep7901, loss: 0.10208005458116531\nstep7902, loss: 0.09209258109331131\nstep7903, loss: 0.11586549133062363\nstep7904, loss: 0.09233596175909042\nstep7905, loss: 0.11190629005432129\nstep7906, loss: 0.09838991612195969\nstep7907, loss: 0.09293209761381149\nstep7908, loss: 0.09495586156845093\nstep7909, loss: 0.09113488346338272\nstep7910, loss: 0.08789192140102386\nstep7911, loss: 0.08657665550708771\nstep7912, loss: 0.08386783301830292\nstep7913, loss: 0.08277134597301483\nstep7914, loss: 0.10793852806091309\nstep7915, loss: 0.08670154213905334\nstep7916, loss: 0.08647358417510986\nstep7917, loss: 0.09566904604434967\nstep7918, loss: 0.09107791632413864\nstep7919, loss: 0.09866752475500107\nstep7920, loss: 0.10000848025083542\nstep7921, loss: 0.09152936935424805\nstep7922, loss: 0.08961409330368042\nstep7923, loss: 0.08129995316267014\nstep7924, loss: 0.10463613271713257\nstep7925, loss: 0.11045348644256592\nstep7926, loss: 0.1190510243177414\nstep7927, loss: 0.09939773380756378\nstep7928, loss: 0.10240799188613892\nstep7929, loss: 0.09408973157405853\nstep7930, loss: 0.09869802743196487\nstep7931, loss: 0.08564145117998123\nstep7932, loss: 0.08957269042730331\nstep7933, loss: 0.07354744523763657\nstep7934, loss: 0.08859844505786896\nstep7935, loss: 0.07340380549430847\nstep7936, loss: 0.08708975464105606\nstep7937, loss: 0.07500331848859787\nstep7938, loss: 0.07720503211021423\nstep7939, loss: 0.07218893617391586\nstep7940, loss: 0.07439693808555603\nstep7941, loss: 0.09508934617042542\nstep7942, loss: 0.08629919588565826\nstep7943, loss: 0.093964584171772\nstep7944, loss: 0.10545925796031952\nstep7945, loss: 0.09973655641078949\nstep7946, loss: 0.08364712446928024\nstep7947, loss: 0.08405791968107224\nstep7948, loss: 0.06984881311655045\nstep7949, loss: 0.07757331430912018\nstep7950, loss: 0.07167597860097885\nstep7951, loss: 0.0638175755739212\nstep7952, loss: 0.08534181118011475\nstep7953, loss: 0.08145725727081299\nstep7954, loss: 0.0946195125579834\nstep7955, loss: 0.09340811520814896\nstep7956, loss: 0.09700053185224533\nstep7957, loss: 0.09848432242870331\nstep7958, loss: 0.07665450125932693\nstep7959, loss: 0.07463225722312927\nstep7960, loss: 0.08772066980600357\nstep7961, loss: 0.0813901424407959\nstep7962, loss: 0.07894831150770187\nstep7963, loss: 0.09535156935453415\nstep7964, loss: 0.08097441494464874\nstep7965, loss: 0.08535566180944443\nstep7966, loss: 0.08092773705720901\nstep7967, loss: 0.09245415031909943\nstep7968, loss: 0.07981180399656296\nstep7969, loss: 0.09283339232206345\nstep7970, loss: 0.09468958526849747\nstep7971, loss: 0.09329996258020401\nstep7972, loss: 0.07108242809772491\nstep7973, loss: 0.07551810145378113\nstep7974, loss: 0.0682884007692337\nstep7975, loss: 0.0803069993853569\nstep7976, loss: 0.08162285387516022\nstep7977, loss: 0.07914279401302338\nstep7978, loss: 0.08830013126134872\nstep7979, loss: 0.09029340744018555\nstep7980, loss: 0.08417896926403046\nstep7981, loss: 0.08447791635990143\nstep7982, loss: 0.09504653513431549\nstep7983, loss: 0.07783346623182297\nstep7984, loss: 0.07066017389297485\nstep7985, loss: 0.08510088175535202\nstep7986, loss: 0.07944770902395248\nstep7987, loss: 0.07555486261844635\nstep7988, loss: 0.07122116535902023\nstep7989, loss: 0.06886934489011765\nstep7990, loss: 0.0713265910744667\nstep7991, loss: 0.07573439180850983\nstep7992, loss: 0.0813044011592865\nstep7993, loss: 0.06654144823551178\nstep7994, loss: 0.06963110715150833\nstep7995, loss: 0.06580530107021332\nstep7996, loss: 0.0840914323925972\nstep7997, loss: 0.07925476878881454\nstep7998, loss: 0.06524329632520676\nstep7999, loss: 0.07119787484407425\nstep8000, loss: 0.0652313232421875\nstep8001, loss: 0.07182145863771439\nstep8002, loss: 0.06799448281526566\nstep8003, loss: 0.06809984892606735\nstep8004, loss: 0.058696821331977844\nstep8005, loss: 0.05386676266789436\nstep8006, loss: 0.06709377467632294\nstep8007, loss: 0.07733362168073654\nstep8008, loss: 0.07448757439851761\nstep8009, loss: 0.07242913544178009\nstep8010, loss: 0.09109717607498169\nstep8011, loss: 0.08752429485321045\nstep8012, loss: 0.07532405108213425\nstep8013, loss: 0.07277596741914749\nstep8014, loss: 0.06822708994150162\nstep8015, loss: 0.07642891258001328\nstep8016, loss: 0.07715220749378204\nstep8017, loss: 0.06187714636325836\nstep8018, loss: 0.07263516634702682\nstep8019, loss: 0.05960811674594879\nstep8020, loss: 0.07689884305000305\nstep8021, loss: 0.06101551651954651\nstep8022, loss: 0.0589553564786911\nstep8023, loss: 0.07023844122886658\nstep8024, loss: 0.0629824846982956\nstep8025, loss: 0.060308780521154404\nstep8026, loss: 0.06092829257249832\nstep8027, loss: 0.07707186043262482\nstep8028, loss: 0.06356850266456604\nstep8029, loss: 0.0725112333893776\nstep8030, loss: 0.05815034732222557\nstep8031, loss: 0.056582119315862656\nstep8032, loss: 0.05988530069589615\nstep8033, loss: 0.0467325896024704\nstep8034, loss: 0.07372020184993744\nstep8035, loss: 0.07573501765727997\nstep8036, loss: 0.07302961498498917\nstep8037, loss: 0.06354528665542603\nstep8038, loss: 0.08156587928533554\nstep8039, loss: 0.07348672300577164\nstep8040, loss: 0.07000017166137695\nstep8041, loss: 0.06301365792751312\nstep8042, loss: 0.056569989770650864\nstep8043, loss: 0.05798593908548355\nstep8044, loss: 0.055518049746751785\nstep8045, loss: 0.0686204731464386\nstep8046, loss: 0.06411759555339813\nstep8047, loss: 0.067728191614151\nstep8048, loss: 0.06856339424848557\nstep8049, loss: 0.08780180662870407\nstep8050, loss: 0.07775458693504333\nstep8051, loss: 0.08541373163461685\nstep8052, loss: 0.06651731580495834\nstep8053, loss: 0.07353997975587845\nstep8054, loss: 0.07437557727098465\nstep8055, loss: 0.06700380146503448\nstep8056, loss: 0.05712525546550751\nstep8057, loss: 0.06287660449743271\nstep8058, loss: 0.06355462223291397\nstep8059, loss: 0.05636719986796379\nstep8060, loss: 0.07051148265600204\nstep8061, loss: 0.07113712280988693\nstep8062, loss: 0.0750228613615036\nstep8063, loss: 0.07640621811151505\nstep8064, loss: 0.07533741742372513\nstep8065, loss: 0.06501053273677826\nstep8066, loss: 0.06468062847852707\nstep8067, loss: 0.06998784095048904\nstep8068, loss: 0.06178276613354683\nstep8069, loss: 0.0647086501121521\nstep8070, loss: 0.06210622936487198\nstep8071, loss: 0.06279554218053818\nstep8072, loss: 0.06902991980314255\nstep8073, loss: 0.06288501620292664\nstep8074, loss: 0.06417231261730194\nstep8075, loss: 0.05399569869041443\nstep8076, loss: 0.047852884978055954\nstep8077, loss: 0.04945547133684158\nstep8078, loss: 0.0794455036520958\nstep8079, loss: 0.07218173146247864\nstep8080, loss: 0.061559680849313736\nstep8081, loss: 0.06433536112308502\nstep8082, loss: 0.06566935777664185\nstep8083, loss: 0.08765384554862976\nstep8084, loss: 0.07397151738405228\nstep8085, loss: 0.05091383680701256\nstep8086, loss: 0.05129748582839966\nstep8087, loss: 0.0532890260219574\nstep8088, loss: 0.048022568225860596\nstep8089, loss: 0.05056441202759743\nstep8090, loss: 0.04601641371846199\nstep8091, loss: 0.04963011294603348\nstep8092, loss: 0.05834781378507614\nstep8093, loss: 0.05208530277013779\nstep8094, loss: 0.04861147329211235\nstep8095, loss: 0.05230999365448952\nstep8096, loss: 0.0436151847243309\nstep8097, loss: 0.04939765855669975\nstep8098, loss: 0.062145624309778214\nstep8099, loss: 0.045439213514328\nstep8100, loss: 0.061158519238233566\nstep8101, loss: 0.04891291633248329\nstep8102, loss: 0.050572942942380905\nstep8103, loss: 0.05553412809967995\nstep8104, loss: 0.042371124029159546\nstep8105, loss: 0.050218332558870316\nstep8106, loss: 0.041948750615119934\nstep8107, loss: 0.04053087905049324\nstep8108, loss: 0.03911137208342552\nstep8109, loss: 0.048513900488615036\nstep8110, loss: 0.044375624507665634\nstep8111, loss: 0.04852946847677231\nstep8112, loss: 0.03847717493772507\nstep8113, loss: 0.036433145403862\nstep8114, loss: 0.041676949709653854\nstep8115, loss: 0.03408141806721687\nstep8116, loss: 0.052242204546928406\nstep8117, loss: 0.05276196449995041\nstep8118, loss: 0.056458428502082825\nstep8119, loss: 0.04826320707798004\nstep8120, loss: 0.05286291241645813\nstep8121, loss: 0.057441893965005875\nstep8122, loss: 0.04290840029716492\nstep8123, loss: 0.05422712489962578\nstep8124, loss: 0.04525524005293846\nstep8125, loss: 0.05143486335873604\nstep8126, loss: 0.03922148421406746\nstep8127, loss: 0.04340051859617233\nstep8128, loss: 0.042327702045440674\nstep8129, loss: 0.04827553406357765\nstep8130, loss: 0.04477394372224808\nstep8131, loss: 0.05977845937013626\nstep8132, loss: 0.047968991100788116\nstep8133, loss: 0.05680597946047783\nstep8134, loss: 0.05123564973473549\nstep8135, loss: 0.05797011777758598\nstep8136, loss: 0.05743792653083801\nstep8137, loss: 0.07374001294374466\nstep8138, loss: 0.06576889008283615\nstep8139, loss: 0.06129232794046402\nstep8140, loss: 0.05018109083175659\nstep8141, loss: 0.04623684287071228\nstep8142, loss: 0.05762452259659767\nstep8143, loss: 0.050208598375320435\nstep8144, loss: 0.05200633779168129\nstep8145, loss: 0.055227603763341904\nstep8146, loss: 0.05920352786779404\nstep8147, loss: 0.044013023376464844\nstep8148, loss: 0.04728283733129501\nstep8149, loss: 0.05208365246653557\nstep8150, loss: 0.05480792373418808\nstep8151, loss: 0.06315024197101593\nstep8152, loss: 0.06099899113178253\nstep8153, loss: 0.06845517456531525\nstep8154, loss: 0.0579383410513401\nstep8155, loss: 0.05483650788664818\nstep8156, loss: 0.05690924823284149\nstep8157, loss: 0.05795681104063988\nstep8158, loss: 0.04794082045555115\nstep8159, loss: 0.04039498791098595\nstep8160, loss: 0.05788877233862877\nstep8161, loss: 0.054863542318344116\nstep8162, loss: 0.041446901857852936\nstep8163, loss: 0.05295122414827347\nstep8164, loss: 0.060970574617385864\nstep8165, loss: 0.09053925424814224\nstep8166, loss: 0.06371572613716125\nstep8167, loss: 0.042817436158657074\nstep8168, loss: 0.052456703037023544\nstep8169, loss: 0.05193613842129707\nstep8170, loss: 0.04972708970308304\nstep8171, loss: 0.05117403715848923\nstep8172, loss: 0.05197729915380478\nstep8173, loss: 0.056981585919857025\nstep8174, loss: 0.06073487177491188\nstep8175, loss: 0.045069366693496704\nstep8176, loss: 0.042109642177820206\nstep8177, loss: 0.04485092684626579\nstep8178, loss: 0.034891389310359955\nstep8179, loss: 0.036788586527109146\nstep8180, loss: 0.045204631984233856\nstep8181, loss: 0.041873570531606674\nstep8182, loss: 0.04281860589981079\nstep8183, loss: 0.03581751137971878\nstep8184, loss: 0.03312981501221657\nstep8185, loss: 0.039459627121686935\nstep8186, loss: 0.038272347301244736\nstep8187, loss: 0.045293617993593216\nstep8188, loss: 0.036718450486660004\nstep8189, loss: 0.03528674691915512\nstep8190, loss: 0.03048846125602722\nstep8191, loss: 0.0365876629948616\nstep8192, loss: 0.03436100110411644\nstep8193, loss: 0.03647616505622864\nstep8194, loss: 0.03555484861135483\nstep8195, loss: 0.038189809769392014\nstep8196, loss: 0.030491363257169724\nstep8197, loss: 0.025889117270708084\nstep8198, loss: 0.04043865576386452\nstep8199, loss: 0.03480612114071846\nstep8200, loss: 0.03710247576236725\nstep8201, loss: 0.03232666477560997\nstep8202, loss: 0.035710856318473816\nstep8203, loss: 0.04516669362783432\nstep8204, loss: 0.03354690596461296\nstep8205, loss: 0.035634517669677734\nstep8206, loss: 0.037187546491622925\nstep8207, loss: 0.04591938853263855\nstep8208, loss: 0.025682350620627403\nstep8209, loss: 0.03309696167707443\nstep8210, loss: 0.03206987306475639\nstep8211, loss: 0.0318264365196228\nstep8212, loss: 0.03645198792219162\nstep8213, loss: 0.039316028356552124\nstep8214, loss: 0.031259264796972275\nstep8215, loss: 0.04068112000823021\nstep8216, loss: 0.036344777792692184\nstep8217, loss: 0.03743654489517212\nstep8218, loss: 0.03804374486207962\nstep8219, loss: 0.05564463883638382\nstep8220, loss: 0.04819967597723007\nstep8221, loss: 0.04827223718166351\nstep8222, loss: 0.04372020810842514\nstep8223, loss: 0.04362694174051285\nstep8224, loss: 0.04754674434661865\nstep8225, loss: 0.04215152934193611\nstep8226, loss: 0.04758811742067337\nstep8227, loss: 0.05048034340143204\nstep8228, loss: 0.051888443529605865\nstep8229, loss: 0.03756989538669586\nstep8230, loss: 0.03516359254717827\nstep8231, loss: 0.03917444869875908\nstep8232, loss: 0.03824309632182121\nstep8233, loss: 0.03991948440670967\nstep8234, loss: 0.04444609954953194\nstep8235, loss: 0.04773833975195885\nstep8236, loss: 0.03951335325837135\nstep8237, loss: 0.04662350192666054\nstep8238, loss: 0.03977719321846962\nstep8239, loss: 0.04479173198342323\nstep8240, loss: 0.04060077667236328\nstep8241, loss: 0.039998531341552734\nstep8242, loss: 0.062056321650743484\nstep8243, loss: 0.05778151750564575\nstep8244, loss: 0.0424586720764637\nstep8245, loss: 0.0561039112508297\nstep8246, loss: 0.08389550447463989\nstep8247, loss: 0.0862223356962204\nstep8248, loss: 0.04939153417944908\nstep8249, loss: 0.048614632338285446\nstep8250, loss: 0.035666629672050476\nstep8251, loss: 0.04402020573616028\nstep8252, loss: 0.034117214381694794\nstep8253, loss: 0.040373966097831726\nstep8254, loss: 0.03272204101085663\nstep8255, loss: 0.041887067258358\nstep8256, loss: 0.04745379090309143\nstep8257, loss: 0.04148411378264427\nstep8258, loss: 0.0424465648829937\nstep8259, loss: 0.03912452235817909\nstep8260, loss: 0.033599574118852615\nstep8261, loss: 0.03590508922934532\nstep8262, loss: 0.03918182849884033\nstep8263, loss: 0.032860759645700455\nstep8264, loss: 0.044358983635902405\nstep8265, loss: 0.03503746539354324\nstep8266, loss: 0.027492733672261238\nstep8267, loss: 0.033946823328733444\nstep8268, loss: 0.032149944454431534\nstep8269, loss: 0.031092079356312752\nstep8270, loss: 0.023623423650860786\nstep8271, loss: 0.02969946712255478\nstep8272, loss: 0.03252321109175682\nstep8273, loss: 0.02859937585890293\nstep8274, loss: 0.024587087333202362\nstep8275, loss: 0.03243038430809975\nstep8276, loss: 0.028401942923665047\nstep8277, loss: 0.02690783701837063\nstep8278, loss: 0.029789455235004425\nstep8279, loss: 0.02298000268638134\nstep8280, loss: 0.030546972528100014\nstep8281, loss: 0.035364944487810135\nstep8282, loss: 0.034693680703639984\nstep8283, loss: 0.028553737327456474\nstep8284, loss: 0.029282530769705772\nstep8285, loss: 0.031809113919734955\nstep8286, loss: 0.025507841259241104\nstep8287, loss: 0.02867148630321026\nstep8288, loss: 0.031902045011520386\nstep8289, loss: 0.031907059252262115\nstep8290, loss: 0.0238951463252306\nstep8291, loss: 0.021893363445997238\nstep8292, loss: 0.025728881359100342\nstep8293, loss: 0.02675905078649521\nstep8294, loss: 0.0506536141037941\nstep8295, loss: 0.028707141056656837\nstep8296, loss: 0.023194044828414917\nstep8297, loss: 0.03360680118203163\nstep8298, loss: 0.030121229588985443\nstep8299, loss: 0.03323211893439293\nstep8300, loss: 0.03497014939785004\nstep8301, loss: 0.03482791408896446\nstep8302, loss: 0.02888939529657364\nstep8303, loss: 0.03280818834900856\nstep8304, loss: 0.03927937150001526\nstep8305, loss: 0.03288786858320236\nstep8306, loss: 0.033698517829179764\nstep8307, loss: 0.038003090769052505\nstep8308, loss: 0.03273358941078186\nstep8309, loss: 0.03513813018798828\nstep8310, loss: 0.03186420723795891\nstep8311, loss: 0.029593709856271744\nstep8312, loss: 0.025679651647806168\nstep8313, loss: 0.028947053477168083\nstep8314, loss: 0.02710993029177189\nstep8315, loss: 0.027977772057056427\nstep8316, loss: 0.03056020848453045\nstep8317, loss: 0.03117140755057335\nstep8318, loss: 0.02859397977590561\nstep8319, loss: 0.02953418530523777\nstep8320, loss: 0.02659955807030201\nstep8321, loss: 0.03496516868472099\nstep8322, loss: 0.025896599516272545\nstep8323, loss: 0.024073554202914238\nstep8324, loss: 0.035401102155447006\nstep8325, loss: 0.035943612456321716\nstep8326, loss: 0.02845551259815693\nstep8327, loss: 0.03235974535346031\nstep8328, loss: 0.045604489743709564\nstep8329, loss: 0.04072412848472595\nstep8330, loss: 0.03628600388765335\nstep8331, loss: 0.048177510499954224\nstep8332, loss: 0.03767194598913193\nstep8333, loss: 0.03697887435555458\nstep8334, loss: 0.030519109219312668\nstep8335, loss: 0.03468376770615578\nstep8336, loss: 0.01953786611557007\nstep8337, loss: 0.03102802485227585\nstep8338, loss: 0.030743606388568878\nstep8339, loss: 0.02617226354777813\nstep8340, loss: 0.02909865975379944\nstep8341, loss: 0.032068803906440735\nstep8342, loss: 0.02285030670464039\nstep8343, loss: 0.026903552934527397\nstep8344, loss: 0.0293413158506155\nstep8345, loss: 0.025011196732521057\nstep8346, loss: 0.03248476982116699\nstep8347, loss: 0.027328606694936752\nstep8348, loss: 0.027685627341270447\nstep8349, loss: 0.024982741102576256\nstep8350, loss: 0.030299510806798935\nstep8351, loss: 0.03762080892920494\nstep8352, loss: 0.022142093628644943\nstep8353, loss: 0.028670888394117355\nstep8354, loss: 0.024349596351385117\nstep8355, loss: 0.025363892316818237\nstep8356, loss: 0.020614856854081154\nstep8357, loss: 0.02481597289443016\nstep8358, loss: 0.02144734375178814\nstep8359, loss: 0.024006355553865433\nstep8360, loss: 0.019606268033385277\nstep8361, loss: 0.016512878239154816\nstep8362, loss: 0.02722511626780033\nstep8363, loss: 0.0226996261626482\nstep8364, loss: 0.023613283410668373\nstep8365, loss: 0.02038329280912876\nstep8366, loss: 0.023753516376018524\nstep8367, loss: 0.02422887645661831\nstep8368, loss: 0.02006525918841362\nstep8369, loss: 0.02193393185734749\nstep8370, loss: 0.022973529994487762\nstep8371, loss: 0.024434665217995644\nstep8372, loss: 0.020970609039068222\nstep8373, loss: 0.01804768107831478\nstep8374, loss: 0.021220142021775246\nstep8375, loss: 0.023030677810311317\nstep8376, loss: 0.10445043444633484\nstep8377, loss: 0.02313949540257454\nstep8378, loss: 0.02186877653002739\nstep8379, loss: 0.03477207198739052\nstep8380, loss: 0.04369761794805527\nstep8381, loss: 0.03593556210398674\nstep8382, loss: 0.0351925790309906\nstep8383, loss: 0.029456201940774918\nstep8384, loss: 0.020948655903339386\nstep8385, loss: 0.020502407103776932\nstep8386, loss: 0.02529289945960045\nstep8387, loss: 0.025022316724061966\nstep8388, loss: 0.024486873298883438\nstep8389, loss: 0.026676394045352936\nstep8390, loss: 0.027889078482985497\nstep8391, loss: 0.02794068492949009\nstep8392, loss: 0.026532430201768875\nstep8393, loss: 0.02144511416554451\nstep8394, loss: 0.038447242230176926\nstep8395, loss: 0.03422752022743225\nstep8396, loss: 0.023376334458589554\nstep8397, loss: 0.021244078874588013\nstep8398, loss: 0.02043563686311245\nstep8399, loss: 0.024047646671533585\nstep8400, loss: 0.02692662738263607\nstep8401, loss: 0.03446322679519653\nstep8402, loss: 0.019971420988440514\nstep8403, loss: 0.024692149832844734\nstep8404, loss: 0.020044760778546333\nstep8405, loss: 0.01880318485200405\nstep8406, loss: 0.02432148903608322\nstep8407, loss: 0.024085354059934616\nstep8408, loss: 0.018999915570020676\nstep8409, loss: 0.018190905451774597\nstep8410, loss: 0.02983451820909977\nstep8411, loss: 0.12094412744045258\nstep8412, loss: 0.023018846288323402\nstep8413, loss: 0.027362020686268806\nstep8414, loss: 0.020707473158836365\nstep8415, loss: 0.019675973802804947\nstep8416, loss: 0.024182669818401337\nstep8417, loss: 0.023496918380260468\nstep8418, loss: 0.014277753420174122\nstep8419, loss: 0.02285275235772133\nstep8420, loss: 0.02001381106674671\nstep8421, loss: 0.01926555298268795\nstep8422, loss: 0.022597555071115494\nstep8423, loss: 0.024666206911206245\nstep8424, loss: 0.018865296617150307\nstep8425, loss: 0.021426033228635788\nstep8426, loss: 0.022054672241210938\nstep8427, loss: 0.019287047907710075\nstep8428, loss: 0.021142082288861275\nstep8429, loss: 0.018568728119134903\nstep8430, loss: 0.018346035853028297\nstep8431, loss: 0.01860298588871956\nstep8432, loss: 0.0187661275267601\nstep8433, loss: 0.025057002902030945\nstep8434, loss: 0.017007755115628242\nstep8435, loss: 0.027348071336746216\nstep8436, loss: 0.026703841984272003\nstep8437, loss: 0.0220811665058136\nstep8438, loss: 0.014957315288484097\nstep8439, loss: 0.01526487898081541\nstep8440, loss: 0.015603551641106606\nstep8441, loss: 0.017769964411854744\nstep8442, loss: 0.019074028357863426\nstep8443, loss: 0.014541706070303917\nstep8444, loss: 0.019756456837058067\nstep8445, loss: 0.016781670972704887\nstep8446, loss: 0.017773782834410667\nstep8447, loss: 0.016113702207803726\nstep8448, loss: 0.017852667719125748\nstep8449, loss: 0.01753563992679119\nstep8450, loss: 0.015302876010537148\nstep8451, loss: 0.0186739731580019\nstep8452, loss: 0.01967598870396614\nstep8453, loss: 0.017894698306918144\nstep8454, loss: 0.01697465218603611\nstep8455, loss: 0.013137911446392536\nstep8456, loss: 0.01765165850520134\nstep8457, loss: 0.015860935673117638\nstep8458, loss: 0.03335503488779068\nstep8459, loss: 0.01829650066792965\nstep8460, loss: 0.015703335404396057\nstep8461, loss: 0.01955461874604225\nstep8462, loss: 0.025536097586154938\nstep8463, loss: 0.02408476360142231\nstep8464, loss: 0.024699503555893898\nstep8465, loss: 0.0303817056119442\nstep8466, loss: 0.012999541126191616\nstep8467, loss: 0.014191409572958946\nstep8468, loss: 0.018663303926587105\nstep8469, loss: 0.016695600003004074\nstep8470, loss: 0.01659979857504368\nstep8471, loss: 0.016799835488200188\nstep8472, loss: 0.02059905230998993\nstep8473, loss: 0.01938493363559246\nstep8474, loss: 0.019667990505695343\nstep8475, loss: 0.01598030887544155\nstep8476, loss: 0.030331358313560486\nstep8477, loss: 0.0381813645362854\nstep8478, loss: 0.017642034217715263\nstep8479, loss: 0.01453582476824522\nstep8480, loss: 0.016266603022813797\nstep8481, loss: 0.022830499336123466\nstep8482, loss: 0.019726457074284554\nstep8483, loss: 0.019394751638174057\nstep8484, loss: 0.016850564628839493\nstep8485, loss: 0.024888550862669945\nstep8486, loss: 0.016718026250600815\nstep8487, loss: 0.016940925270318985\nstep8488, loss: 0.01822136528789997\nstep8489, loss: 0.018359627574682236\nstep8490, loss: 0.014886184595525265\nstep8491, loss: 0.013606807217001915\nstep8492, loss: 0.020665975287556648\nstep8493, loss: 0.038203541189432144\nstep8494, loss: 0.01627334952354431\nstep8495, loss: 0.017839105799794197\nstep8496, loss: 0.015672732144594193\nstep8497, loss: 0.01437755860388279\nstep8498, loss: 0.015141498297452927\nstep8499, loss: 0.015442194417119026\nstep8500, loss: 0.00980396568775177\nstep8501, loss: 0.016424549743533134\nstep8502, loss: 0.01467271987348795\nstep8503, loss: 0.013829211704432964\nstep8504, loss: 0.017534330487251282\nstep8505, loss: 0.017170825973153114\nstep8506, loss: 0.014201960526406765\nstep8507, loss: 0.01580321602523327\nstep8508, loss: 0.020963085815310478\nstep8509, loss: 0.014809862710535526\nstep8510, loss: 0.014468410052359104\nstep8511, loss: 0.015386980958282948\nstep8512, loss: 0.025731343775987625\nstep8513, loss: 0.016721084713935852\nstep8514, loss: 0.014677987433969975\nstep8515, loss: 0.016201963648200035\nstep8516, loss: 0.015535318292677402\nstep8517, loss: 0.025853341445326805\nstep8518, loss: 0.016200171783566475\nstep8519, loss: 0.01967226155102253\nstep8520, loss: 0.013236255384981632\nstep8521, loss: 0.013177626766264439\nstep8522, loss: 0.01518652681261301\nstep8523, loss: 0.015466432087123394\nstep8524, loss: 0.014896503649652004\nstep8525, loss: 0.01225702092051506\nstep8526, loss: 0.01367469783872366\nstep8527, loss: 0.01204682420939207\nstep8528, loss: 0.014098688960075378\nstep8529, loss: 0.012877311557531357\nstep8530, loss: 0.01533703226596117\nstep8531, loss: 0.011913968250155449\nstep8532, loss: 0.011769432574510574\nstep8533, loss: 0.012727195397019386\nstep8534, loss: 0.014963964931666851\nstep8535, loss: 0.014969808980822563\nstep8536, loss: 0.01302639115601778\nstep8537, loss: 0.010990360751748085\nstep8538, loss: 0.014761758968234062\nstep8539, loss: 0.012295398861169815\nstep8540, loss: 0.01568451151251793\nstep8541, loss: 0.01413217093795538\nstep8542, loss: 0.011624832637608051\nstep8543, loss: 0.019270608201622963\nstep8544, loss: 0.017436426132917404\nstep8545, loss: 0.016123175621032715\nstep8546, loss: 0.017572324723005295\nstep8547, loss: 0.021432626992464066\nstep8548, loss: 0.012772937305271626\nstep8549, loss: 0.013561161234974861\nstep8550, loss: 0.016296515241265297\nstep8551, loss: 0.013614458963274956\nstep8552, loss: 0.012552950531244278\nstep8553, loss: 0.016143810003995895\nstep8554, loss: 0.017842667177319527\nstep8555, loss: 0.01584205962717533\nstep8556, loss: 0.015462166629731655\nstep8557, loss: 0.01509796641767025\nstep8558, loss: 0.016290780156850815\nstep8559, loss: 0.02854086644947529\nstep8560, loss: 0.016432106494903564\nstep8561, loss: 0.011833424679934978\nstep8562, loss: 0.013282490894198418\nstep8563, loss: 0.013422883115708828\nstep8564, loss: 0.01609107293188572\nstep8565, loss: 0.01267304364591837\nstep8566, loss: 0.013281697407364845\nstep8567, loss: 0.015018376521766186\nstep8568, loss: 0.011750830337405205\nstep8569, loss: 0.013236235827207565\nstep8570, loss: 0.015262169763445854\nstep8571, loss: 0.014157196506857872\nstep8572, loss: 0.012769870460033417\nstep8573, loss: 0.010402226820588112\nstep8574, loss: 0.017885740846395493\nstep8575, loss: 0.09414273500442505\nstep8576, loss: 0.013588261790573597\nstep8577, loss: 0.015195555053651333\nstep8578, loss: 0.015928586944937706\nstep8579, loss: 0.01593950390815735\nstep8580, loss: 0.015288769267499447\nstep8581, loss: 0.014155244454741478\nstep8582, loss: 0.008040950633585453\nstep8583, loss: 0.013800105080008507\nstep8584, loss: 0.012149413116276264\nstep8585, loss: 0.011703773401677608\nstep8586, loss: 0.014385580085217953\nstep8587, loss: 0.013647438958287239\nstep8588, loss: 0.012838035821914673\nstep8589, loss: 0.009996836073696613\nstep8590, loss: 0.012263820506632328\nstep8591, loss: 0.010405532084405422\nstep8592, loss: 0.01098492369055748\nstep8593, loss: 0.010907760821282864\nstep8594, loss: 0.022547947242856026\nstep8595, loss: 0.011557010002434254\nstep8596, loss: 0.013539809733629227\nstep8597, loss: 0.01175997219979763\nstep8598, loss: 0.010994896292686462\nstep8599, loss: 0.01639804244041443\nstep8600, loss: 0.012762472033500671\nstep8601, loss: 0.01564541831612587\nstep8602, loss: 0.010907907038927078\nstep8603, loss: 0.01055961288511753\nstep8604, loss: 0.011754008010029793\nstep8605, loss: 0.012616895139217377\nstep8606, loss: 0.013099104166030884\nstep8607, loss: 0.009768432937562466\nstep8608, loss: 0.01061401329934597\nstep8609, loss: 0.011206521652638912\nstep8610, loss: 0.012571945786476135\nstep8611, loss: 0.011831832118332386\nstep8612, loss: 0.01285462360829115\nstep8613, loss: 0.010711723007261753\nstep8614, loss: 0.009967878460884094\nstep8615, loss: 0.010183448903262615\nstep8616, loss: 0.012096469290554523\nstep8617, loss: 0.013469068333506584\nstep8618, loss: 0.011652359738945961\nstep8619, loss: 0.008600120432674885\nstep8620, loss: 0.011727480217814445\nstep8621, loss: 0.010493866167962551\nstep8622, loss: 0.012026135809719563\nstep8623, loss: 0.011659497395157814\nstep8624, loss: 0.00951553974300623\nstep8625, loss: 0.0200309157371521\nstep8626, loss: 0.011062605306506157\nstep8627, loss: 0.013526947237551212\nstep8628, loss: 0.014242496341466904\nstep8629, loss: 0.013413939625024796\nstep8630, loss: 0.010604997165501118\nstep8631, loss: 0.011692906729876995\nstep8632, loss: 0.012442649342119694\nstep8633, loss: 0.011305470950901508\nstep8634, loss: 0.012545495294034481\nstep8635, loss: 0.011421268805861473\nstep8636, loss: 0.01768946647644043\nstep8637, loss: 0.014016075059771538\nstep8638, loss: 0.01845543272793293\nstep8639, loss: 0.01100948080420494\nstep8640, loss: 0.018094129860401154\nstep8641, loss: 0.03509194403886795\nstep8642, loss: 0.01611403562128544\nstep8643, loss: 0.010644045658409595\nstep8644, loss: 0.012669499032199383\nstep8645, loss: 0.011505688540637493\nstep8646, loss: 0.015886589884757996\nstep8647, loss: 0.012138436548411846\nstep8648, loss: 0.013148842379450798\nstep8649, loss: 0.013435468077659607\nstep8650, loss: 0.010769079439342022\nstep8651, loss: 0.018580779433250427\nstep8652, loss: 0.01169506274163723\nstep8653, loss: 0.012537374161183834\nstep8654, loss: 0.011239705607295036\nstep8655, loss: 0.008871966041624546\nstep8656, loss: 0.014453497715294361\nstep8657, loss: 0.0499708354473114\nstep8658, loss: 0.011850771494209766\nstep8659, loss: 0.011856840923428535\nstep8660, loss: 0.011881976388394833\nstep8661, loss: 0.010520088486373425\nstep8662, loss: 0.01085608173161745\nstep8663, loss: 0.011554200202226639\nstep8664, loss: 0.00629653362557292\nstep8665, loss: 0.011788093484938145\nstep8666, loss: 0.010728321969509125\nstep8667, loss: 0.01028355211019516\nstep8668, loss: 0.0135482307523489\nstep8669, loss: 0.012578301131725311\nstep8670, loss: 0.009471520781517029\nstep8671, loss: 0.012506905011832714\nstep8672, loss: 0.01330789178609848\nstep8673, loss: 0.008239708840847015\nstep8674, loss: 0.010384160093963146\nstep8675, loss: 0.010458444245159626\nstep8676, loss: 0.011857103556394577\nstep8677, loss: 0.010592571459710598\nstep8678, loss: 0.012392384000122547\nstep8679, loss: 0.019372176378965378\nstep8680, loss: 0.015128747560083866\nstep8681, loss: 0.013065342791378498\nstep8682, loss: 0.011591167189180851\nstep8683, loss: 0.016300756484270096\nstep8684, loss: 0.010069074109196663\nstep8685, loss: 0.010242927819490433\nstep8686, loss: 0.010202393867075443\nstep8687, loss: 0.012278812006115913\nstep8688, loss: 0.011012506671249866\nstep8689, loss: 0.009008153341710567\nstep8690, loss: 0.009165260009467602\nstep8691, loss: 0.010436704382300377\nstep8692, loss: 0.011947395280003548\nstep8693, loss: 0.010095594450831413\nstep8694, loss: 0.014842087402939796\nstep8695, loss: 0.009541470557451248\nstep8696, loss: 0.011364147067070007\nstep8697, loss: 0.011373915709555149\nstep8698, loss: 0.0113527188077569\nstep8699, loss: 0.013398155570030212\nstep8700, loss: 0.01043904758989811\nstep8701, loss: 0.007956333458423615\nstep8702, loss: 0.011003866791725159\nstep8703, loss: 0.009559121914207935\nstep8704, loss: 0.012401914224028587\nstep8705, loss: 0.011122788302600384\nstep8706, loss: 0.008474649861454964\nstep8707, loss: 0.013747550547122955\nstep8708, loss: 0.015422860160470009\nstep8709, loss: 0.013384423218667507\nstep8710, loss: 0.012722049839794636\nstep8711, loss: 0.01395311951637268\nstep8712, loss: 0.010225169360637665\nstep8713, loss: 0.009488810785114765\nstep8714, loss: 0.012069781310856342\nstep8715, loss: 0.010630867443978786\nstep8716, loss: 0.009764757938683033\nstep8717, loss: 0.0111760962754488\nstep8718, loss: 0.01564859412610531\nstep8719, loss: 0.012292972765862942\nstep8720, loss: 0.014313912950456142\nstep8721, loss: 0.013063331134617329\nstep8722, loss: 0.014273072592914104\nstep8723, loss: 0.021751927211880684\nstep8724, loss: 0.014906495809555054\nstep8725, loss: 0.010791302658617496\nstep8726, loss: 0.011318568140268326\nstep8727, loss: 0.01137504167854786\nstep8728, loss: 0.014030090533196926\nstep8729, loss: 0.010411384515464306\nstep8730, loss: 0.011688191443681717\nstep8731, loss: 0.01148303784430027\nstep8732, loss: 0.009346045553684235\nstep8733, loss: 0.014692932367324829\nstep8734, loss: 0.009940125048160553\nstep8735, loss: 0.011445682495832443\nstep8736, loss: 0.009979547001421452\nstep8737, loss: 0.007559274788945913\nstep8738, loss: 0.013388904742896557\nstep8739, loss: 0.058122724294662476\nstep8740, loss: 0.01078857108950615\nstep8741, loss: 0.011159980669617653\nstep8742, loss: 0.011339008808135986\nstep8743, loss: 0.009261034429073334\nstep8744, loss: 0.010299501940608025\nstep8745, loss: 0.010960386134684086\nstep8746, loss: 0.006411297246813774\nstep8747, loss: 0.01234579086303711\nstep8748, loss: 0.010800203308463097\nstep8749, loss: 0.011518334038555622\nstep8750, loss: 0.015756934881210327\nstep8751, loss: 0.012100276537239552\nstep8752, loss: 0.010372224263846874\nstep8753, loss: 0.00962899625301361\nstep8754, loss: 0.011217447929084301\nstep8755, loss: 0.007897504605352879\nstep8756, loss: 0.011567789129912853\nstep8757, loss: 0.010202568024396896\nstep8758, loss: 0.015239976346492767\nstep8759, loss: 0.010881598107516766\nstep8760, loss: 0.010914266109466553\nstep8761, loss: 0.011244210414588451\nstep8762, loss: 0.009306317195296288\nstep8763, loss: 0.011488369666039944\nstep8764, loss: 0.010412863455712795\nstep8765, loss: 0.013207286596298218\nstep8766, loss: 0.009891778230667114\nstep8767, loss: 0.010083025321364403\nstep8768, loss: 0.009183487854897976\nstep8769, loss: 0.010725843720138073\nstep8770, loss: 0.011437504552304745\nstep8771, loss: 0.007449320983141661\nstep8772, loss: 0.007728724740445614\nstep8773, loss: 0.010339489206671715\nstep8774, loss: 0.010900941677391529\nstep8775, loss: 0.00857016071677208\nstep8776, loss: 0.01198363583534956\nstep8777, loss: 0.009303583763539791\nstep8778, loss: 0.008797316811978817\nstep8779, loss: 0.008695153519511223\nstep8780, loss: 0.010446649044752121\nstep8781, loss: 0.012434608303010464\nstep8782, loss: 0.009074727073311806\nstep8783, loss: 0.00708955992013216\nstep8784, loss: 0.010068973526358604\nstep8785, loss: 0.008030840195715427\nstep8786, loss: 0.011320912279188633\nstep8787, loss: 0.010706078261137009\nstep8788, loss: 0.007566477172076702\nstep8789, loss: 0.011332210153341293\nstep8790, loss: 0.012990228831768036\nstep8791, loss: 0.012041077949106693\nstep8792, loss: 0.011431031860411167\nstep8793, loss: 0.012380221858620644\nstep8794, loss: 0.009971137158572674\nstep8795, loss: 0.009012915194034576\nstep8796, loss: 0.012850090861320496\nstep8797, loss: 0.008797445334494114\nstep8798, loss: 0.009528589434921741\nstep8799, loss: 0.014635102823376656\nstep8800, loss: 0.015057655982673168\nstep8801, loss: 0.011955448426306248\nstep8802, loss: 0.014043725095689297\nstep8803, loss: 0.009553441777825356\nstep8804, loss: 0.015104859136044979\nstep8805, loss: 0.025191863998770714\nstep8806, loss: 0.013829626142978668\nstep8807, loss: 0.008972948417067528\nstep8808, loss: 0.010135848075151443\nstep8809, loss: 0.010769428685307503\nstep8810, loss: 0.013967854902148247\nstep8811, loss: 0.010392389260232449\nstep8812, loss: 0.011453678831458092\nstep8813, loss: 0.012316562235355377\nstep8814, loss: 0.010442483238875866\nstep8815, loss: 0.026664523407816887\nstep8816, loss: 0.010205937549471855\nstep8817, loss: 0.011798880994319916\nstep8818, loss: 0.010493466630578041\nstep8819, loss: 0.007427793461829424\nstep8820, loss: 0.011374572291970253\nstep8821, loss: 0.01110848132520914\nstep8822, loss: 0.011128220707178116\nstep8823, loss: 0.010599413886666298\nstep8824, loss: 0.0099397674202919\nstep8825, loss: 0.008192332461476326\nstep8826, loss: 0.010831332765519619\nstep8827, loss: 0.010387739166617393\nstep8828, loss: 0.004957268014550209\nstep8829, loss: 0.010713791474699974\nstep8830, loss: 0.008987086825072765\nstep8831, loss: 0.009715829975903034\nstep8832, loss: 0.012147686444222927\nstep8833, loss: 0.011503663845360279\nstep8834, loss: 0.008358447812497616\nstep8835, loss: 0.010775644332170486\nstep8836, loss: 0.012378931976854801\nstep8837, loss: 0.006872409023344517\nstep8838, loss: 0.008882062509655952\nstep8839, loss: 0.009025661274790764\nstep8840, loss: 0.013876453973352909\nstep8841, loss: 0.015456967055797577\nstep8842, loss: 0.01211533322930336\nstep8843, loss: 0.010239426977932453\nstep8844, loss: 0.009557734243571758\nstep8845, loss: 0.010634579695761204\nstep8846, loss: 0.010602764785289764\nstep8847, loss: 0.013073844835162163\nstep8848, loss: 0.009565573185682297\nstep8849, loss: 0.009325340390205383\nstep8850, loss: 0.009165903553366661\nstep8851, loss: 0.010955334641039371\nstep8852, loss: 0.008756311610341072\nstep8853, loss: 0.007845684885978699\nstep8854, loss: 0.008130195550620556\nstep8855, loss: 0.009097795933485031\nstep8856, loss: 0.010136413387954235\nstep8857, loss: 0.008566281758248806\nstep8858, loss: 0.010884211398661137\nstep8859, loss: 0.006611988414078951\nstep8860, loss: 0.010102828033268452\nstep8861, loss: 0.009879391640424728\nstep8862, loss: 0.009867210872471333\nstep8863, loss: 0.011202243156731129\nstep8864, loss: 0.008118475787341595\nstep8865, loss: 0.006666363682597876\nstep8866, loss: 0.010332176461815834\nstep8867, loss: 0.00817691907286644\nstep8868, loss: 0.010192078538239002\nstep8869, loss: 0.009895759634673595\nstep8870, loss: 0.007031240034848452\nstep8871, loss: 0.01025217305868864\nstep8872, loss: 0.010594411753118038\nstep8873, loss: 0.011414969339966774\nstep8874, loss: 0.010740564204752445\nstep8875, loss: 0.011799553409218788\nstep8876, loss: 0.008183865807950497\nstep8877, loss: 0.009274279698729515\nstep8878, loss: 0.01336410641670227\nstep8879, loss: 0.007649325765669346\nstep8880, loss: 0.012446681968867779\nstep8881, loss: 0.014082151465117931\nstep8882, loss: 0.02301432006061077\nstep8883, loss: 0.017385870218276978\nstep8884, loss: 0.015467815101146698\nstep8885, loss: 0.010806448757648468\nstep8886, loss: 0.014065219089388847\nstep8887, loss: 0.026980120688676834\nstep8888, loss: 0.013004623353481293\nstep8889, loss: 0.008341657929122448\nstep8890, loss: 0.009733469225466251\nstep8891, loss: 0.009246322326362133\nstep8892, loss: 0.013315919786691666\nstep8893, loss: 0.009607052430510521\nstep8894, loss: 0.009969917125999928\nstep8895, loss: 0.012289181351661682\nstep8896, loss: 0.010598204098641872\nstep8897, loss: 0.009977568872272968\nstep8898, loss: 0.009439707733690739\nstep8899, loss: 0.010956114158034325\nstep8900, loss: 0.009959590621292591\nstep8901, loss: 0.00666260439902544\nstep8902, loss: 0.011793289333581924\nstep8903, loss: 0.009768175892531872\nstep8904, loss: 0.010189952328801155\nstep8905, loss: 0.010031640529632568\nstep8906, loss: 0.009987428784370422\nstep8907, loss: 0.00767008401453495\nstep8908, loss: 0.008838933892548084\nstep8909, loss: 0.010386914946138859\nstep8910, loss: 0.004806464072316885\nstep8911, loss: 0.010025650262832642\nstep8912, loss: 0.008417575620114803\nstep8913, loss: 0.008409261703491211\nstep8914, loss: 0.010931736789643764\nstep8915, loss: 0.010306302458047867\nstep8916, loss: 0.00869755633175373\nstep8917, loss: 0.017962878569960594\nstep8918, loss: 0.020610759034752846\nstep8919, loss: 0.00673092994838953\nstep8920, loss: 0.020923417061567307\nstep8921, loss: 0.007892971858382225\nstep8922, loss: 0.011628776788711548\nstep8923, loss: 0.012272147461771965\nstep8924, loss: 0.01056382991373539\nstep8925, loss: 0.010851937346160412\nstep8926, loss: 0.008984353393316269\nstep8927, loss: 0.01027990598231554\nstep8928, loss: 0.015934109687805176\nstep8929, loss: 0.019376050680875778\nstep8930, loss: 0.013376704417169094\nstep8931, loss: 0.010590309277176857\nstep8932, loss: 0.009244422428309917\nstep8933, loss: 0.010486005805432796\nstep8934, loss: 0.010737825185060501\nstep8935, loss: 0.007474301382899284\nstep8936, loss: 0.008404401130974293\nstep8937, loss: 0.009159546345472336\nstep8938, loss: 0.00939068291336298\nstep8939, loss: 0.008211525157094002\nstep8940, loss: 0.010594721883535385\nstep8941, loss: 0.008589309640228748\nstep8942, loss: 0.00950354803353548\nstep8943, loss: 0.007507961709052324\nstep8944, loss: 0.009507925249636173\nstep8945, loss: 0.01159338653087616\nstep8946, loss: 0.008044981397688389\nstep8947, loss: 0.006734258029609919\nstep8948, loss: 0.010172427631914616\nstep8949, loss: 0.007968077436089516\nstep8950, loss: 0.00888825673609972\nstep8951, loss: 0.010125510394573212\nstep8952, loss: 0.006603611633181572\nstep8953, loss: 0.009658567607402802\nstep8954, loss: 0.009300293400883675\nstep8955, loss: 0.010767946019768715\nstep8956, loss: 0.011407851241528988\nstep8957, loss: 0.011065353639423847\nstep8958, loss: 0.006860225461423397\nstep8959, loss: 0.008214270696043968\nstep8960, loss: 0.00955064408481121\nstep8961, loss: 0.006996870506554842\nstep8962, loss: 0.00792887806892395\nstep8963, loss: 0.01039794459939003\nstep8964, loss: 0.01417400874197483\nstep8965, loss: 0.01037123054265976\nstep8966, loss: 0.01584172621369362\nstep8967, loss: 0.013587711378932\nstep8968, loss: 0.01361162681132555\nstep8969, loss: 0.013502410613000393\nstep8970, loss: 0.012475857511162758\nstep8971, loss: 0.007907725870609283\nstep8972, loss: 0.008804317563772202\nstep8973, loss: 0.008812223561108112\nstep8974, loss: 0.012742278166115284\nstep8975, loss: 0.008772889152169228\nstep8976, loss: 0.009649550542235374\nstep8977, loss: 0.012707955203950405\nstep8978, loss: 0.009472331032156944\nstep8979, loss: 0.011564607731997967\nstep8980, loss: 0.01007839385420084\nstep8981, loss: 0.01093698013573885\nstep8982, loss: 0.009768988005816936\nstep8983, loss: 0.005807363893836737\nstep8984, loss: 0.012789440341293812\nstep8985, loss: 0.008961055427789688\nstep8986, loss: 0.009725869633257389\nstep8987, loss: 0.010248198173940182\nstep8988, loss: 0.009272009134292603\nstep8989, loss: 0.007359721232205629\nstep8990, loss: 0.008764787577092648\nstep8991, loss: 0.010500886477530003\nstep8992, loss: 0.004150730092078447\nstep8993, loss: 0.010720574297010899\nstep8994, loss: 0.007657656911760569\nstep8995, loss: 0.008473849855363369\nstep8996, loss: 0.01006544753909111\nstep8997, loss: 0.009997576475143433\nstep8998, loss: 0.007446879055351019\nstep8999, loss: 0.007371402345597744\nstep9000, loss: 0.009787164628505707\nstep9001, loss: 0.0059740012511610985\nstep9002, loss: 0.009528792463243008\nstep9003, loss: 0.009420054033398628\nstep9004, loss: 0.015048837289214134\nstep9005, loss: 0.01507608499377966\nstep9006, loss: 0.008742832578718662\nstep9007, loss: 0.009029911831021309\nstep9008, loss: 0.007712621241807938\nstep9009, loss: 0.009721104055643082\nstep9010, loss: 0.010382253676652908\nstep9011, loss: 0.01291932724416256\nstep9012, loss: 0.009915304370224476\nstep9013, loss: 0.00917787291109562\nstep9014, loss: 0.009370570071041584\nstep9015, loss: 0.010983385145664215\nstep9016, loss: 0.008316169492900372\nstep9017, loss: 0.006988946348428726\nstep9018, loss: 0.008119585923850536\nstep9019, loss: 0.008030736818909645\nstep9020, loss: 0.007918721064925194\nstep9021, loss: 0.008428637869656086\nstep9022, loss: 0.010557481087744236\nstep9023, loss: 0.006637066137045622\nstep9024, loss: 0.008736452087759972\nstep9025, loss: 0.009115352295339108\nstep9026, loss: 0.009854675270617008\nstep9027, loss: 0.011998182162642479\nstep9028, loss: 0.009310713969171047\nstep9029, loss: 0.006994015071541071\nstep9030, loss: 0.01030146423727274\nstep9031, loss: 0.008204188197851181\nstep9032, loss: 0.009440808556973934\nstep9033, loss: 0.0092514269053936\nstep9034, loss: 0.0063032410107553005\nstep9035, loss: 0.013825277797877789\nstep9036, loss: 0.011234482750296593\nstep9037, loss: 0.011217133142054081\nstep9038, loss: 0.01143946684896946\nstep9039, loss: 0.011261028237640858\nstep9040, loss: 0.006234652828425169\nstep9041, loss: 0.007678964175283909\nstep9042, loss: 0.010307679884135723\nstep9043, loss: 0.007174947299063206\nstep9044, loss: 0.007676755543798208\nstep9045, loss: 0.01139514334499836\nstep9046, loss: 0.012245816178619862\nstep9047, loss: 0.01242148969322443\nstep9048, loss: 0.014971582219004631\nstep9049, loss: 0.016104629263281822\nstep9050, loss: 0.019572576507925987\nstep9051, loss: 0.02268454059958458\nstep9052, loss: 0.011629818938672543\nstep9053, loss: 0.007658618967980146\nstep9054, loss: 0.008809769526124\nstep9055, loss: 0.008509409613907337\nstep9056, loss: 0.01249659899622202\nstep9057, loss: 0.008593600243330002\nstep9058, loss: 0.009167610667645931\nstep9059, loss: 0.012245718389749527\nstep9060, loss: 0.008996863849461079\nstep9061, loss: 0.009910371154546738\nstep9062, loss: 0.010628287680447102\nstep9063, loss: 0.011713407002389431\nstep9064, loss: 0.010419786907732487\nstep9065, loss: 0.006063252687454224\nstep9066, loss: 0.014482637867331505\nstep9067, loss: 0.009446668438613415\nstep9068, loss: 0.00982281006872654\nstep9069, loss: 0.010938771069049835\nstep9070, loss: 0.011403688229620457\nstep9071, loss: 0.00829110387712717\nstep9072, loss: 0.009607149288058281\nstep9073, loss: 0.012063190340995789\nstep9074, loss: 0.004512499552220106\nstep9075, loss: 0.011042656376957893\nstep9076, loss: 0.007548959460109472\nstep9077, loss: 0.008743391372263432\nstep9078, loss: 0.010798481293022633\nstep9079, loss: 0.01036795973777771\nstep9080, loss: 0.008189411833882332\nstep9081, loss: 0.012875162996351719\nstep9082, loss: 0.017057884484529495\nstep9083, loss: 0.006818063091486692\nstep9084, loss: 0.017357077449560165\nstep9085, loss: 0.011749978177249432\nstep9086, loss: 0.03471272811293602\nstep9087, loss: 0.03045659139752388\nstep9088, loss: 0.015544557943940163\nstep9089, loss: 0.011432102881371975\nstep9090, loss: 0.010756436735391617\nstep9091, loss: 0.009962973184883595\nstep9092, loss: 0.01141811441630125\nstep9093, loss: 0.013227967545390129\nstep9094, loss: 0.01052386499941349\nstep9095, loss: 0.010605850256979465\nstep9096, loss: 0.010416819714009762\nstep9097, loss: 0.011303799226880074\nstep9098, loss: 0.011216476559638977\nstep9099, loss: 0.007829595357179642\nstep9100, loss: 0.008475231006741524\nstep9101, loss: 0.008542257361114025\nstep9102, loss: 0.010185079649090767\nstep9103, loss: 0.008249420672655106\nstep9104, loss: 0.011372358538210392\nstep9105, loss: 0.008557543158531189\nstep9106, loss: 0.008190968073904514\nstep9107, loss: 0.0076364753767848015\nstep9108, loss: 0.01100847590714693\nstep9109, loss: 0.012852894142270088\nstep9110, loss: 0.010987925343215466\nstep9111, loss: 0.007786449044942856\nstep9112, loss: 0.010869456455111504\nstep9113, loss: 0.009142572060227394\nstep9114, loss: 0.008921405300498009\nstep9115, loss: 0.01016897615045309\nstep9116, loss: 0.008627156727015972\nstep9117, loss: 0.010169395245611668\nstep9118, loss: 0.012690390460193157\nstep9119, loss: 0.011486601084470749\nstep9120, loss: 0.012685834430158138\nstep9121, loss: 0.013167986646294594\nstep9122, loss: 0.009567255154252052\nstep9123, loss: 0.008857905864715576\nstep9124, loss: 0.01015745010226965\nstep9125, loss: 0.007846142165362835\nstep9126, loss: 0.00888668280094862\nstep9127, loss: 0.012055923230946064\nstep9128, loss: 0.017062263563275337\nstep9129, loss: 0.010064296424388885\nstep9130, loss: 0.013889032416045666\nstep9131, loss: 0.009081033058464527\nstep9132, loss: 0.020411737263202667\nstep9133, loss: 0.014859309419989586\nstep9134, loss: 0.011910701170563698\nstep9135, loss: 0.006913678254932165\nstep9136, loss: 0.008253085426986217\nstep9137, loss: 0.007732177618891001\nstep9138, loss: 0.01229725033044815\nstep9139, loss: 0.006953507196158171\nstep9140, loss: 0.008204626850783825\nstep9141, loss: 0.010770536959171295\nstep9142, loss: 0.008510381914675236\nstep9143, loss: 0.010523016564548016\nstep9144, loss: 0.00945117138326168\nstep9145, loss: 0.011009331792593002\nstep9146, loss: 0.009112254716455936\nstep9147, loss: 0.0054073636420071125\nstep9148, loss: 0.013122085481882095\nstep9149, loss: 0.008251357823610306\nstep9150, loss: 0.009922238066792488\nstep9151, loss: 0.009991640225052834\nstep9152, loss: 0.011188866570591927\nstep9153, loss: 0.007848887704312801\nstep9154, loss: 0.008958956226706505\nstep9155, loss: 0.011210450902581215\nstep9156, loss: 0.0037915431894361973\nstep9157, loss: 0.01095114927738905\nstep9158, loss: 0.007529890164732933\nstep9159, loss: 0.008894950151443481\nstep9160, loss: 0.010557572357356548\nstep9161, loss: 0.010169731453061104\nstep9162, loss: 0.007895066402852535\nstep9163, loss: 0.007518125232309103\nstep9164, loss: 0.010424564592540264\nstep9165, loss: 0.007200272288173437\nstep9166, loss: 0.007201660890132189\nstep9167, loss: 0.008697396144270897\nstep9168, loss: 0.021101636812090874\nstep9169, loss: 0.0135478675365448\nstep9170, loss: 0.014295943081378937\nstep9171, loss: 0.01024905126541853\nstep9172, loss: 0.008146856911480427\nstep9173, loss: 0.008738570846617222\nstep9174, loss: 0.010533906519412994\nstep9175, loss: 0.01285096351057291\nstep9176, loss: 0.010722637176513672\nstep9177, loss: 0.008090490475296974\nstep9178, loss: 0.008844945579767227\nstep9179, loss: 0.010999532416462898\nstep9180, loss: 0.007842241786420345\nstep9181, loss: 0.0070527601055800915\nstep9182, loss: 0.0077230557799339294\nstep9183, loss: 0.007568875327706337\nstep9184, loss: 0.007598805706948042\nstep9185, loss: 0.008497240953147411\nstep9186, loss: 0.009862234815955162\nstep9187, loss: 0.005867252591997385\nstep9188, loss: 0.007483345922082663\nstep9189, loss: 0.009038680233061314\nstep9190, loss: 0.010231134481728077\nstep9191, loss: 0.011154566891491413\nstep9192, loss: 0.00970769114792347\nstep9193, loss: 0.0064209625124931335\nstep9194, loss: 0.009280141443014145\nstep9195, loss: 0.00724003417417407\nstep9196, loss: 0.008166649378836155\nstep9197, loss: 0.008669587783515453\nstep9198, loss: 0.0062063755467534065\nstep9199, loss: 0.008314628154039383\nstep9200, loss: 0.009383905678987503\nstep9201, loss: 0.010026906616985798\nstep9202, loss: 0.01245688647031784\nstep9203, loss: 0.011214888654649258\nstep9204, loss: 0.006377855781465769\nstep9205, loss: 0.007654571905732155\nstep9206, loss: 0.009314841590821743\nstep9207, loss: 0.007843280211091042\nstep9208, loss: 0.007146197371184826\nstep9209, loss: 0.008350974880158901\nstep9210, loss: 0.01974352076649666\nstep9211, loss: 0.008463814854621887\nstep9212, loss: 0.010386176407337189\nstep9213, loss: 0.008829700760543346\nstep9214, loss: 0.012585936114192009\nstep9215, loss: 0.013598542660474777\nstep9216, loss: 0.011475482024252415\nstep9217, loss: 0.006470522377640009\nstep9218, loss: 0.007990304380655289\nstep9219, loss: 0.007545490749180317\nstep9220, loss: 0.01180283259600401\nstep9221, loss: 0.007281337399035692\nstep9222, loss: 0.0065616280771791935\nstep9223, loss: 0.00899836141616106\nstep9224, loss: 0.007223087828606367\nstep9225, loss: 0.007656683214008808\nstep9226, loss: 0.008276297710835934\nstep9227, loss: 0.01050789188593626\nstep9228, loss: 0.00783048290759325\nstep9229, loss: 0.004397362004965544\nstep9230, loss: 0.01063514593988657\nstep9231, loss: 0.007195994257926941\nstep9232, loss: 0.008217674680054188\nstep9233, loss: 0.00858281459659338\nstep9234, loss: 0.009870137088000774\nstep9235, loss: 0.00694706104695797\nstep9236, loss: 0.007623005658388138\nstep9237, loss: 0.009603702463209629\nstep9238, loss: 0.003631132422015071\nstep9239, loss: 0.010228419676423073\nstep9240, loss: 0.006881978362798691\nstep9241, loss: 0.00852605514228344\nstep9242, loss: 0.009882960468530655\nstep9243, loss: 0.009568268433213234\nstep9244, loss: 0.007328798063099384\nstep9245, loss: 0.0067509934306144714\nstep9246, loss: 0.008635465987026691\nstep9247, loss: 0.007752820383757353\nstep9248, loss: 0.008344979025423527\nstep9249, loss: 0.007271880749613047\nstep9250, loss: 0.009898453950881958\nstep9251, loss: 0.010780799202620983\nstep9252, loss: 0.009360031224787235\nstep9253, loss: 0.0072419787757098675\nstep9254, loss: 0.011058070696890354\nstep9255, loss: 0.008021041750907898\nstep9256, loss: 0.00917668454349041\nstep9257, loss: 0.012385757640004158\nstep9258, loss: 0.007630749139934778\nstep9259, loss: 0.007002212107181549\nstep9260, loss: 0.007408957928419113\nstep9261, loss: 0.009310174733400345\nstep9262, loss: 0.008289448916912079\nstep9263, loss: 0.0053056636825203896\nstep9264, loss: 0.006712386384606361\nstep9265, loss: 0.006538043729960918\nstep9266, loss: 0.007420327048748732\nstep9267, loss: 0.0063111972995102406\nstep9268, loss: 0.008498955518007278\nstep9269, loss: 0.006500644143670797\nstep9270, loss: 0.00721459835767746\nstep9271, loss: 0.006101209204643965\nstep9272, loss: 0.009018974378705025\nstep9273, loss: 0.010873910039663315\nstep9274, loss: 0.008373213931918144\nstep9275, loss: 0.006267308257520199\nstep9276, loss: 0.008387792855501175\nstep9277, loss: 0.0067171333357691765\nstep9278, loss: 0.00783572532236576\nstep9279, loss: 0.008763950318098068\nstep9280, loss: 0.005617411341518164\nstep9281, loss: 0.008439060300588608\nstep9282, loss: 0.007574834395200014\nstep9283, loss: 0.009399425238370895\nstep9284, loss: 0.012235522270202637\nstep9285, loss: 0.008993341587483883\nstep9286, loss: 0.005059066694229841\nstep9287, loss: 0.007191941607743502\nstep9288, loss: 0.008620736189186573\nstep9289, loss: 0.009081924334168434\nstep9290, loss: 0.006702905520796776\nstep9291, loss: 0.007554347161203623\nstep9292, loss: 0.036707282066345215\nstep9293, loss: 0.013462711125612259\nstep9294, loss: 0.011746910400688648\nstep9295, loss: 0.006759571377187967\nstep9296, loss: 0.009056796319782734\nstep9297, loss: 0.018561914563179016\nstep9298, loss: 0.01059908140450716\nstep9299, loss: 0.006152350455522537\nstep9300, loss: 0.008268410339951515\nstep9301, loss: 0.00774406036362052\nstep9302, loss: 0.012745720334351063\nstep9303, loss: 0.0074674710631370544\nstep9304, loss: 0.007285216823220253\nstep9305, loss: 0.010114351287484169\nstep9306, loss: 0.008232684805989265\nstep9307, loss: 0.009993291459977627\nstep9308, loss: 0.009487616829574108\nstep9309, loss: 0.010274776257574558\nstep9310, loss: 0.008157283067703247\nstep9311, loss: 0.004508334212005138\nstep9312, loss: 0.01167207956314087\nstep9313, loss: 0.007509513758122921\nstep9314, loss: 0.008330879732966423\nstep9315, loss: 0.007958547212183475\nstep9316, loss: 0.008981828577816486\nstep9317, loss: 0.00718722864985466\nstep9318, loss: 0.007625951431691647\nstep9319, loss: 0.008459567092359066\nstep9320, loss: 0.0031421741005033255\nstep9321, loss: 0.01007374469190836\nstep9322, loss: 0.006757721770554781\nstep9323, loss: 0.007539667189121246\nstep9324, loss: 0.010341488756239414\nstep9325, loss: 0.009098119102418423\nstep9326, loss: 0.0070280288346111774\nstep9327, loss: 0.006406639236956835\nstep9328, loss: 0.008474932052195072\nstep9329, loss: 0.006348858121782541\nstep9330, loss: 0.006020667497068644\nstep9331, loss: 0.007089863996952772\nstep9332, loss: 0.008737072348594666\nstep9333, loss: 0.011690336279571056\nstep9334, loss: 0.008468438871204853\nstep9335, loss: 0.00864249374717474\nstep9336, loss: 0.008130897767841816\nstep9337, loss: 0.007975654676556587\nstep9338, loss: 0.009110361337661743\nstep9339, loss: 0.013020915910601616\nstep9340, loss: 0.007230532821267843\nstep9341, loss: 0.005924115423113108\nstep9342, loss: 0.007355159148573875\nstep9343, loss: 0.009907824918627739\nstep9344, loss: 0.006595026236027479\nstep9345, loss: 0.005218135192990303\nstep9346, loss: 0.0064394669607281685\nstep9347, loss: 0.006736884359270334\nstep9348, loss: 0.007832305505871773\nstep9349, loss: 0.007046980317682028\nstep9350, loss: 0.008246931247413158\nstep9351, loss: 0.005163286812603474\nstep9352, loss: 0.008059029467403889\nstep9353, loss: 0.008176195435225964\nstep9354, loss: 0.009600934572517872\nstep9355, loss: 0.009970545768737793\nstep9356, loss: 0.007345333695411682\nstep9357, loss: 0.005824431777000427\nstep9358, loss: 0.00834200531244278\nstep9359, loss: 0.006002409383654594\nstep9360, loss: 0.008775856345891953\nstep9361, loss: 0.012731130234897137\nstep9362, loss: 0.00591968884691596\nstep9363, loss: 0.007746533490717411\nstep9364, loss: 0.0071301450952887535\nstep9365, loss: 0.008696289733052254\nstep9366, loss: 0.011722216382622719\nstep9367, loss: 0.010156954638659954\nstep9368, loss: 0.004369993694126606\nstep9369, loss: 0.006336212158203125\nstep9370, loss: 0.00869052391499281\nstep9371, loss: 0.007498715538531542\nstep9372, loss: 0.010210739448666573\nstep9373, loss: 0.020562831312417984\nstep9374, loss: 0.03318493813276291\nstep9375, loss: 0.031556084752082825\nstep9376, loss: 0.011795246973633766\nstep9377, loss: 0.012534625828266144\nstep9378, loss: 0.008789937943220139\nstep9379, loss: 0.011123185977339745\nstep9380, loss: 0.010279040783643723\nstep9381, loss: 0.006484884303063154\nstep9382, loss: 0.011473284102976322\nstep9383, loss: 0.00713600916787982\nstep9384, loss: 0.01180929597467184\nstep9385, loss: 0.007545157335698605\nstep9386, loss: 0.007391638588160276\nstep9387, loss: 0.010697122663259506\nstep9388, loss: 0.008213608525693417\nstep9389, loss: 0.007778255268931389\nstep9390, loss: 0.010742725804448128\nstep9391, loss: 0.012172125279903412\nstep9392, loss: 0.010470532812178135\nstep9393, loss: 0.005615059286355972\nstep9394, loss: 0.011479300446808338\nstep9395, loss: 0.007373151835054159\nstep9396, loss: 0.008265453390777111\nstep9397, loss: 0.008164353668689728\nstep9398, loss: 0.008771686814725399\nstep9399, loss: 0.0063896034844219685\nstep9400, loss: 0.0071977777406573296\nstep9401, loss: 0.00886271707713604\nstep9402, loss: 0.0032018849160522223\nstep9403, loss: 0.009626504965126514\nstep9404, loss: 0.006568321492522955\nstep9405, loss: 0.010017800144851208\nstep9406, loss: 0.010927485302090645\nstep9407, loss: 0.00896232109516859\nstep9408, loss: 0.00738914217799902\nstep9409, loss: 0.00723517220467329\nstep9410, loss: 0.007544448133558035\nstep9411, loss: 0.0067915902473032475\nstep9412, loss: 0.007929766550660133\nstep9413, loss: 0.006749882362782955\nstep9414, loss: 0.007990476675331593\nstep9415, loss: 0.010598139837384224\nstep9416, loss: 0.010430729947984219\nstep9417, loss: 0.00819841306656599\nstep9418, loss: 0.007799815386533737\nstep9419, loss: 0.007679122034460306\nstep9420, loss: 0.00931627955287695\nstep9421, loss: 0.012281406670808792\nstep9422, loss: 0.007129192817956209\nstep9423, loss: 0.005923872347921133\nstep9424, loss: 0.007152428850531578\nstep9425, loss: 0.00930982455611229\nstep9426, loss: 0.008934837765991688\nstep9427, loss: 0.005041847936809063\nstep9428, loss: 0.006544481962919235\nstep9429, loss: 0.009377934969961643\nstep9430, loss: 0.010453161783516407\nstep9431, loss: 0.007710498291999102\nstep9432, loss: 0.009142555296421051\nstep9433, loss: 0.006802882067859173\nstep9434, loss: 0.007251125760376453\nstep9435, loss: 0.005929781123995781\nstep9436, loss: 0.010678395628929138\nstep9437, loss: 0.011163563467562199\nstep9438, loss: 0.008017566055059433\nstep9439, loss: 0.005963745526969433\nstep9440, loss: 0.008372783660888672\nstep9441, loss: 0.006466936785727739\nstep9442, loss: 0.016245491802692413\nstep9443, loss: 0.008229403756558895\nstep9444, loss: 0.006699710618704557\nstep9445, loss: 0.007485439535230398\nstep9446, loss: 0.007086919620633125\nstep9447, loss: 0.009331012144684792\nstep9448, loss: 0.0116270761936903\nstep9449, loss: 0.009722715243697166\nstep9450, loss: 0.0040808008052408695\nstep9451, loss: 0.006971722934395075\nstep9452, loss: 0.008923388086259365\nstep9453, loss: 0.008487040176987648\nstep9454, loss: 0.009719276800751686\nstep9455, loss: 0.015293286181986332\nstep9456, loss: 0.030169103294610977\nstep9457, loss: 0.016507722437381744\nstep9458, loss: 0.012732570059597492\nstep9459, loss: 0.01176957506686449\nstep9460, loss: 0.008373122662305832\nstep9461, loss: 0.009661045856773853\nstep9462, loss: 0.010860024951398373\nstep9463, loss: 0.007184911053627729\nstep9464, loss: 0.021625235676765442\nstep9465, loss: 0.006836910732090473\nstep9466, loss: 0.011235483922064304\nstep9467, loss: 0.0069653792306780815\nstep9468, loss: 0.008755823597311974\nstep9469, loss: 0.009100549854338169\nstep9470, loss: 0.007976687513291836\nstep9471, loss: 0.009765339083969593\nstep9472, loss: 0.00908175390213728\nstep9473, loss: 0.010107139125466347\nstep9474, loss: 0.008293920196592808\nstep9475, loss: 0.005484720692038536\nstep9476, loss: 0.011022932827472687\nstep9477, loss: 0.006998288445174694\nstep9478, loss: 0.008734076283872128\nstep9479, loss: 0.007894990034401417\nstep9480, loss: 0.008782871067523956\nstep9481, loss: 0.006439628545194864\nstep9482, loss: 0.007368056103587151\nstep9483, loss: 0.00933089479804039\nstep9484, loss: 0.0029296011198312044\nstep9485, loss: 0.009325304999947548\nstep9486, loss: 0.006743471138179302\nstep9487, loss: 0.010046911425888538\nstep9488, loss: 0.012130302377045155\nstep9489, loss: 0.009348628111183643\nstep9490, loss: 0.006844324059784412\nstep9491, loss: 0.015817806124687195\nstep9492, loss: 0.007303067948669195\nstep9493, loss: 0.006034510675817728\nstep9494, loss: 0.005993472412228584\nstep9495, loss: 0.006670616567134857\nstep9496, loss: 0.00751635292544961\nstep9497, loss: 0.009616468101739883\nstep9498, loss: 0.009151265025138855\nstep9499, loss: 0.0077316113747656345\nstep9500, loss: 0.006586734671145678\nstep9501, loss: 0.0077413455583155155\nstep9502, loss: 0.01212413888424635\nstep9503, loss: 0.014166017062962055\nstep9504, loss: 0.007480951491743326\nstep9505, loss: 0.0058463760651648045\nstep9506, loss: 0.00806579552590847\nstep9507, loss: 0.00967641081660986\nstep9508, loss: 0.008951854892075062\nstep9509, loss: 0.004942659754306078\nstep9510, loss: 0.007419217843562365\nstep9511, loss: 0.009564445354044437\nstep9512, loss: 0.0077033150009810925\nstep9513, loss: 0.010140404105186462\nstep9514, loss: 0.008451308123767376\nstep9515, loss: 0.005916214548051357\nstep9516, loss: 0.00925602950155735\nstep9517, loss: 0.008207891136407852\nstep9518, loss: 0.010696910321712494\nstep9519, loss: 0.011087884195148945\nstep9520, loss: 0.009349237196147442\nstep9521, loss: 0.00578494044020772\nstep9522, loss: 0.008802616968750954\nstep9523, loss: 0.007473841775208712\nstep9524, loss: 0.00793677382171154\nstep9525, loss: 0.008249666541814804\nstep9526, loss: 0.005472654942423105\nstep9527, loss: 0.007162506692111492\nstep9528, loss: 0.0078501608222723\nstep9529, loss: 0.008757022209465504\nstep9530, loss: 0.011146512813866138\nstep9531, loss: 0.010399953462183475\nstep9532, loss: 0.004130068700760603\nstep9533, loss: 0.01022624596953392\nstep9534, loss: 0.00853562168776989\nstep9535, loss: 0.007021875120699406\nstep9536, loss: 0.006785928271710873\nstep9537, loss: 0.008069532923400402\nstep9538, loss: 0.014085575938224792\nstep9539, loss: 0.017785945907235146\nstep9540, loss: 0.011058500036597252\nstep9541, loss: 0.016244949772953987\nstep9542, loss: 0.008367874659597874\nstep9543, loss: 0.008875377476215363\nstep9544, loss: 0.010334180667996407\nstep9545, loss: 0.008476202376186848\nstep9546, loss: 0.008178669959306717\nstep9547, loss: 0.0064402297139167786\nstep9548, loss: 0.010950208641588688\nstep9549, loss: 0.0067780097015202045\nstep9550, loss: 0.009239544160664082\nstep9551, loss: 0.008551452308893204\nstep9552, loss: 0.010592900216579437\nstep9553, loss: 0.008642517030239105\nstep9554, loss: 0.01143646240234375\nstep9555, loss: 0.009295796975493431\nstep9556, loss: 0.007520768325775862\nstep9557, loss: 0.0076827979646623135\nstep9558, loss: 0.01019361149519682\nstep9559, loss: 0.006759598385542631\nstep9560, loss: 0.008885504677891731\nstep9561, loss: 0.007895178161561489\nstep9562, loss: 0.008589093573391438\nstep9563, loss: 0.006801440846174955\nstep9564, loss: 0.007155468687415123\nstep9565, loss: 0.008681891486048698\nstep9566, loss: 0.0028065829537808895\nstep9567, loss: 0.008847355842590332\nstep9568, loss: 0.007597023621201515\nstep9569, loss: 0.007001376245170832\nstep9570, loss: 0.011569864116609097\nstep9571, loss: 0.010935652069747448\nstep9572, loss: 0.006977922283113003\nstep9573, loss: 0.01834983006119728\nstep9574, loss: 0.007511455100029707\nstep9575, loss: 0.006247990764677525\nstep9576, loss: 0.007403870113193989\nstep9577, loss: 0.00597630999982357\nstep9578, loss: 0.007424462120980024\nstep9579, loss: 0.010005642659962177\nstep9580, loss: 0.00787158403545618\nstep9581, loss: 0.006591924466192722\nstep9582, loss: 0.006507333368062973\nstep9583, loss: 0.00893405918031931\nstep9584, loss: 0.00891567301005125\nstep9585, loss: 0.014273704029619694\nstep9586, loss: 0.013264812529087067\nstep9587, loss: 0.006981516722589731\nstep9588, loss: 0.010261009447276592\nstep9589, loss: 0.009999813511967659\nstep9590, loss: 0.014232923276722431\nstep9591, loss: 0.005284242797642946\nstep9592, loss: 0.009033446200191975\nstep9593, loss: 0.009427091106772423\nstep9594, loss: 0.007421619724482298\nstep9595, loss: 0.005883163306862116\nstep9596, loss: 0.01099485345184803\nstep9597, loss: 0.006384422071278095\nstep9598, loss: 0.007867483422160149\nstep9599, loss: 0.006179888732731342\n>  Welcome home.\nBehold, these are the tribunes of the people,\nThe tongues o' the common mouth: I do despise them;\n> ANUS:\nWhy then should I be consul? By yond clouds,\nLet me deserve so ill as you, and make me\n\n> Twere well\nWe let the people know't.\n\nMENENIUS:\nWhat, what? his choler?\n\nCOR\n>  fed\nThe ruin of the state.\n\nBRUTUS:\nWhy, shall the people give\nOne that speaks thus their voice?\n\n> \nMangles true judgment and bereaves the state\nOf that integrity which should become't,\nNot having the power to do the good it would\nsaving models\nModel saved to /kaggle/working/gpt2_model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b71bce05b6c43c4b490cca109b8acf7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cb07ef1598b419da463594e7b4bd5db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c8ffd2572904d2ab9769acb4a355345"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"102404408a8e4179934c88f3e4fcce3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a702c06db9a6453286662227c23845c5"}},"metadata":{}},{"name":"stdout","text":"Tokenizer saved to /kaggle/working/gpt2_model\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import shutil\nshutil.make_archive('output_files', 'zip', '/kaggle/working/gpt2_model')","metadata":{"id":"5DCBI8YPvF1x","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T06:37:52.970403Z","iopub.execute_input":"2025-01-24T06:37:52.970779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}