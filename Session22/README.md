**Phi-2 Fine‚ÄëTuning & qLoRA Compression Demo**

This repository demonstrates the conceptual process of fine‚Äëtuning the Phi‚Äë2 (un‚ÄëSFTed) language model using the GRPO Trainer from Hugging Face, compressing the resulting model with qLoRA, and deploying a live inference demo on Hugging Face Spaces.

---

## üöÄ Project Overview

1. **Fine‚ÄëTune Phi‚Äë2 with GRPO**
   - GRPO (Guided Reinforcement Policy Optimization) is used to fine‚Äëtune the Phi‚Äë2 model using preference-based training strategies.

2. **Compress with qLoRA**
   - After training, the model is compressed using qLoRA, enabling efficient deployment and inference.

3. **Deploy on Hugging Face Spaces**
   - The compressed model is wrapped in a Gradio application and deployed as a live demo on Hugging Face Spaces. (Running on CPU)

4. **Repository & Demo Links**
   - **Spaces Demo**: [https://huggingface.co/spaces/kishkath/phi2-grpo-qlora]

---

## üì¶ Repository Structure (Conceptual)

- **data/**: Contains the dataset used for fine‚Äëtuning.
- **grpo_training/**: Contains fine‚Äëtuning logic using the GRPO Trainer.
- **quantization/**: Details the qLoRA compression process.
- **app/**: Hosts the Gradio-based web demo for the model.
- **README.md**: Provides theoretical overview and project documentation.

---

## üéì Fine‚ÄëTuning with GRPO Trainer

### What is GRPO?

GRPO (Guided Reinforcement Policy Optimization) is a method that merges reinforcement learning and supervised fine-tuning with the goal of improving language models based on preference or reward signals. GRPO extends PPO (Proximal Policy Optimization) by incorporating guidance from a reward model or curated feedback. This helps the language model to learn more human-aligned responses without straying too far from its original capabilities.

**Key Features:**
- **Guided Rewards:** Enables fine‚Äëtuning based on scalar rewards, human feedback, or synthetic signals.
- **Stability:** Employs mechanisms similar to PPO‚Äôs trust region to prevent performance degradation.
- **Hybrid Learning:** Balances reinforcement learning and supervised objectives.

---

## üîç GRPO vs. DPO vs. PPO

| Aspect               | GRPO                                | DPO (Direct Preference Optimization) | PPO (Proximal Policy Optimization)      |
|----------------------|-------------------------------------|---------------------------------------|-----------------------------------------|
| **Optimization Goal** | Maximize reward with guidance       | Maximize preference likelihood        | Maximize expected reward                 |
| **Feedback Type**    | Scalar rewards or ranked outputs    | Pairwise preferences only             | Scalar rewards                           |
| **Learning Method**  | Hybrid: supervised + policy updates | Supervised (no RL)                    | Reinforcement learning with clipping     |
| **Policy Stability** | Maintains stability via constraints | No explicit trust region              | Trust-region-like objective              |
| **Sample Efficiency**| Moderate to high                    | High                                  | Moderate                                 |
| **Complexity**       | Moderate                            | Low                                   | Moderate                                 |
| **Use Case**         | Aligning models with detailed reward signals | Pairwise preference alignment         | General RL with language models         |

---

## üßä qLoRA Compression

qLoRA (quantized Low-Rank Adaptation) is an optimization technique for compressing large language models by combining quantization and parameter-efficient fine-tuning.

**Benefits:**
- **Reduced Memory Usage:** Achieves 4‚Äëbit precision while maintaining accuracy.
- **Faster Inference:** Lower memory consumption translates to faster predictions.
- **Efficient Deployment:** Ideal for deploying models on resource-constrained environments like Hugging Face Spaces.

qLoRA is particularly useful when deploying fine‚Äëtuned models at scale or in interactive demos where inference speed and memory efficiency are crucial.

---

## üåê Deployment on Hugging Face Spaces

The qLoRA‚Äëcompressed model is integrated into a Gradio app that allows users to interact with the model live via Hugging Face Spaces. The app provides a simple user interface where users can input prompts and observe model responses in real time.

---

## üìñ Recalculated Responses Demo

The base model response:


The finetuned (GRPO) model response:


This showcases the effectiveness of GRPO training combined with qLoRA compression.

---

## üîó Links

- **Hugging Face Space:** [https://huggingface.co/spaces/<your-username>/phi2-grpo-qLoRA-demo](https://huggingface.co/spaces/kishkath/phi2-grpo-qlora)

---

## üìú License

This project is licensed under the Educational Purposes.
